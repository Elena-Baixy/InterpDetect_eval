{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ed2027",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65fd8ae",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Repository root: /net/scratch2/smallyan/othello-world_eval\n",
      "Evaluation directory: /net/scratch2/smallyan/othello-world_eval/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Set up environment for generalization testing\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Add repo paths\n",
    "REPO_ROOT = Path(\"/net/scratch2/smallyan/othello-world_eval\")\n",
    "sys.path.insert(0, str(REPO_ROOT / \"mechanistic_interpretability\"))\n",
    "sys.path.insert(0, str(REPO_ROOT / \"data\"))\n",
    "\n",
    "# Set environment for HuggingFace\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['HF_HUB_CACHE'] = '/tmp/hf_cache'\n",
    "os.makedirs('/tmp/hf_cache', exist_ok=True)\n",
    "\n",
    "# Create evaluation directory\n",
    "eval_dir = REPO_ROOT / \"evaluation\"\n",
    "eval_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Evaluation directory: {eval_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da403d5",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for Othello-World\n",
    "\n",
    "This notebook evaluates whether the findings in the Othello-World repository generalize beyond the original experimental setting.\n",
    "\n",
    "## Evaluation Checklist:\n",
    "- **GT1**: Model Generalization - Do findings transfer to a new model?\n",
    "- **GT2**: Data Generalization - Do findings hold on new data instances?\n",
    "- **GT3**: Method/Specificity Generalizability - Can the method be applied to another similar task?\n",
    "\n",
    "## Key Findings from Original Work:\n",
    "1. Nonlinear probes can decode board state with ~1.7% error (synthetic model)\n",
    "2. Interventional experiments show causal role of representation\n",
    "3. Specific neurons detect specific board configurations (e.g., L5N1393)\n",
    "4. Linear probes are less effective (~20% error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85586a9c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"transformer_lens==1.2.1\", \"-q\"], capture_output=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"einops\", \"-q\"], capture_output=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/neelnanda-io/neel-plotly.git\", \"-q\"], capture_output=True)\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as tl_utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Import utility functions from the repo\n",
    "from mech_interp_othello_utils import (\n",
    "    OthelloBoardState, \n",
    "    to_string, to_int, \n",
    "    int_to_label, string_to_label,\n",
    "    stoi_indices\n",
    ")\n",
    "\n",
    "# Disable gradients for inference\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e5a07e",
   "metadata": {
    "execution_status": "running"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/torch/cuda/__init__.py:155: UserWarning: \n",
      "NVIDIA H200 NVL with CUDA capability sm_90 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86.\n",
      "If you want to use the NVIDIA H200 NVL GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "# Define model configuration (same for both synthetic and championship models)\n",
    "model_config = HookedTransformerConfig(\n",
    "    n_layers=8,\n",
    "    d_model=512,\n",
    "    d_head=64,\n",
    "    n_heads=8,\n",
    "    d_mlp=2048,\n",
    "    d_vocab=61,\n",
    "    n_ctx=59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "\n",
    "# Load the SYNTHETIC model (used in original study for neuron analysis)\n",
    "synthetic_model = HookedTransformer(model_config)\n",
    "synthetic_path = hf_hub_download(\n",
    "    repo_id=\"NeelNanda/Othello-GPT-Transformer-Lens\", \n",
    "    filename=\"synthetic_model.pth\",\n",
    "    cache_dir='/tmp/hf_cache'\n",
    ")\n",
    "synthetic_state_dict = torch.load(synthetic_path, map_location='cuda', weights_only=False)\n",
    "synthetic_model.load_state_dict(synthetic_state_dict)\n",
    "synthetic_model = synthetic_model.cuda()\n",
    "print(f\"Synthetic model loaded: {sum(p.numel() for p in synthetic_model.parameters()):,} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-25-00-24_GeneralizationEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
