{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d876a6f4",
   "metadata": {},
   "source": [
    "# Consistency Evaluation - Self Matching Analysis\n",
    "\n",
    "This notebook performs a consistency evaluation of the InterpDetect project, checking:\n",
    "1. **CS1**: Whether conclusions match the original recorded results\n",
    "2. **CS2**: Whether implementation follows the plan\n",
    "\n",
    "## Project Overview\n",
    "The InterpDetect project implements a mechanistic interpretability-based hallucination detection method for RAG systems using:\n",
    "- External Context Score (ECS) - measures attention to external context\n",
    "- Parametric Knowledge Score (PKS) - measures FFN contribution via Jensen-Shannon divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5623a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import torch\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "repo_path = '/net/scratch2/smallyan/InterpDetect_eval'\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40922241",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0fd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "folder_path = os.path.join(repo_path, \"datasets/train\")\n",
    "examples = []\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        examples.extend(data)\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples from {len(json_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c6313",
   "metadata": {},
   "source": [
    "## CS1: Results vs Conclusions Analysis\n",
    "\n",
    "### Claim 1: ECS Correlation Analysis\n",
    "**Plan states**: \"All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ECS and PKS data by hallucination label\n",
    "ecs_truthful = collections.defaultdict(list)\n",
    "ecs_hallucinated = collections.defaultdict(list)\n",
    "pks_truthful = collections.defaultdict(list)\n",
    "pks_hallucinated = collections.defaultdict(list)\n",
    "\n",
    "for example in examples:\n",
    "    for score in example['scores']:\n",
    "        if score['hallucination_label']==0:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_truthful[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_truthful[k].append(v)\n",
    "        else:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_hallucinated[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_hallucinated[k].append(v)\n",
    "\n",
    "print(f\"Number of attention heads: {len(ecs_truthful)}\")\n",
    "print(f\"Number of FFN layers: {len(pks_truthful)}\")\n",
    "print(f\"Truthful spans: {len(list(ecs_truthful.values())[0])}\")\n",
    "print(f\"Hallucinated spans: {len(list(ecs_hallucinated.values())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a67c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECS vs Hallucination correlation\n",
    "def pearson_corr(attention_scores, hallucination_labels, inverse=False):\n",
    "    scores = np.array(attention_scores, dtype=float)\n",
    "    labels = np.array(hallucination_labels, dtype=int)\n",
    "    \n",
    "    if inverse:\n",
    "        inverse_labels = 1 - labels\n",
    "        r, p_value = pearsonr(scores, inverse_labels)\n",
    "    else:\n",
    "        r, p_value = pearsonr(scores, labels)\n",
    "    \n",
    "    return r, p_value\n",
    "\n",
    "# Aggregate ECS data for correlation\n",
    "ecs_lst = collections.defaultdict(list) \n",
    "ecs_label_lst = collections.defaultdict(list) \n",
    "for k, v in ecs_truthful.items():\n",
    "    for a in v:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(0)\n",
    "    for a in ecs_hallucinated[k]:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(1)\n",
    "\n",
    "# Compute ECS correlations (ECS vs Hallucination directly)\n",
    "ecs_pcc_direct = {}\n",
    "for k, v in ecs_lst.items():\n",
    "    r, p_val = pearson_corr(v, ecs_label_lst[k], inverse=False)\n",
    "    ecs_pcc_direct[k] = r\n",
    "\n",
    "# Count positive and negative correlations\n",
    "positive_corr = sum(1 for v in ecs_pcc_direct.values() if v > 0)\n",
    "negative_corr = sum(1 for v in ecs_pcc_direct.values() if v < 0)\n",
    "\n",
    "print(f\"ECS Correlation Analysis (ECS vs Hallucination Label):\")\n",
    "print(f\"  Positive correlations: {positive_corr}\")\n",
    "print(f\"  Negative correlations: {negative_corr}\")\n",
    "print(f\"  Total heads: {len(ecs_pcc_direct)}\")\n",
    "print(f\"\\nClaim: 'All attention heads exhibit negative correlations'\")\n",
    "print(f\"Result: {'MATCHES' if negative_corr == len(ecs_pcc_direct) else 'DOES NOT MATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77790b19",
   "metadata": {},
   "source": [
    "### Claim 2: PKS Correlation Analysis\n",
    "**Plan states**: \"Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PKS vs Hallucination correlation\n",
    "pks_lst = collections.defaultdict(list) \n",
    "pks_label_lst = collections.defaultdict(list) \n",
    "for k, v in pks_truthful.items():\n",
    "    for a in v:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(0)\n",
    "    for a in pks_hallucinated[k]:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(1)\n",
    "\n",
    "# Compute PKS correlations\n",
    "pks_pcc = {}\n",
    "for k, v in pks_lst.items():\n",
    "    r, p_val = pearson_corr(v, pks_label_lst[k], inverse=False)\n",
    "    pks_pcc[k] = r\n",
    "\n",
    "# Sort by layer number\n",
    "sorted_pks = sorted(pks_pcc.items(), key=lambda x: int(x[0].split('_')[1]))\n",
    "\n",
    "# Compare early vs later layers\n",
    "early_layers = [v for k, v in sorted_pks[:14]]  # layers 0-13\n",
    "later_layers = [v for k, v in sorted_pks[14:]]  # layers 14-27\n",
    "\n",
    "print(f\"PKS Correlation Analysis:\")\n",
    "print(f\"  Mean correlation - Early layers (0-13): {np.mean(early_layers):.4f}\")\n",
    "print(f\"  Mean correlation - Later layers (14-27): {np.mean(later_layers):.4f}\")\n",
    "print(f\"\\nClaim: 'Later-layer FFNs exhibit higher PKS correlation with hallucinations'\")\n",
    "print(f\"Result: {'MATCHES' if np.mean(later_layers) > np.mean(early_layers) else 'DOES NOT MATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884be1c",
   "metadata": {},
   "source": [
    "### Claim 3: Classifier Performance\n",
    "**Plan states**: \"SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classifier evaluation\n",
    "ATTENTION_COLS = list(examples[0]['scores'][0]['prompt_attention_score'].keys())\n",
    "PARAMETER_COLS = list(examples[0]['scores'][0]['parameter_knowledge_scores'].keys())\n",
    "\n",
    "data_dict = {\n",
    "    \"identifier\": [],\n",
    "    **{col: [] for col in ATTENTION_COLS},\n",
    "    **{col: [] for col in PARAMETER_COLS},\n",
    "    \"hallucination_label\": []\n",
    "}\n",
    "\n",
    "for i, resp in enumerate(examples):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Balance and split\n",
    "min_count = df['hallucination_label'].value_counts().min()\n",
    "df_balanced = df.groupby('hallucination_label', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_count, random_state=42), include_groups=False\n",
    ").reset_index(drop=True)\n",
    "df_balanced['hallucination_label'] = df.groupby('hallucination_label', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_count, random_state=42)\n",
    ")['hallucination_label'].values\n",
    "\n",
    "train, val = train_test_split(df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['hallucination_label'])\n",
    "features = [col for col in df_balanced.columns if col not in ['identifier', 'hallucination_label']]\n",
    "\n",
    "X_train, y_train = train[features], train[\"hallucination_label\"]\n",
    "X_val, y_val = val[features], val[\"hallucination_label\"]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples, Validation set: {len(X_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pre-trained models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "models_path = os.path.join(repo_path, \"trained_models\")\n",
    "model_names = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
    "model_results = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model_file = os.path.join(models_path, f\"model_{name}_3000.pickle\")\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    model_results[name] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    \n",
    "print(\"Classifier Performance Comparison:\")\n",
    "print(\"=\"*50)\n",
    "for name in model_names:\n",
    "    print(f\"{name}: F1 = {model_results[name]['f1']*100:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nClaim: 'SVC achieved highest validation F1 (76.60%)'\")\n",
    "print(f\"Actual highest: {max(model_results.items(), key=lambda x: x[1]['f1'])[0]} with {max(model_results.values(), key=lambda x: x['f1'])['f1']*100:.2f}%\")\n",
    "print(f\"Result: {'MATCHES' if max(model_results.items(), key=lambda x: x[1]['f1'])[0] == 'SVC' else 'DOES NOT MATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5ee75",
   "metadata": {},
   "source": [
    "### Claim 4 & 5: Detection Performance\n",
    "**Plan states**: \n",
    "- Self-Evaluation: \"Method achieved F1=74.68%\"\n",
    "- Proxy-Based: \"Method achieved F1=75.36%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVC model for response-level evaluation\n",
    "svc_model_path = os.path.join(models_path, \"model_SVC_3000.pickle\")\n",
    "with open(svc_model_path, \"rb\") as f:\n",
    "    svc_model = pickle.load(f)\n",
    "\n",
    "# Self-Evaluation (Qwen test data)\n",
    "test_qwen_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")\n",
    "with open(test_qwen_path, \"r\") as f:\n",
    "    test_qwen = json.load(f)\n",
    "\n",
    "test_qwen_dict = {\"identifier\": [], **{col: [] for col in ATTENTION_COLS}, \n",
    "                  **{col: [] for col in PARAMETER_COLS}, \"hallucination_label\": []}\n",
    "for i, resp in enumerate(test_qwen):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_qwen_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_qwen_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            test_qwen_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_qwen_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_qwen_df = pd.DataFrame(test_qwen_dict)\n",
    "test_qwen_df['pred'] = svc_model.predict(test_qwen_df[features])\n",
    "test_qwen_df[\"response_id\"] = test_qwen_df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
    "agg_qwen = test_qwen_df.groupby(\"response_id\").agg({\"pred\": \"max\", \"hallucination_label\": \"max\"}).reset_index()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(agg_qwen[\"hallucination_label\"], agg_qwen[\"pred\"]).ravel()\n",
    "f1_self = 2 * (tp/(tp+fp)) * (tp/(tp+fn)) / ((tp/(tp+fp)) + (tp/(tp+fn)))\n",
    "print(f\"Self-Evaluation F1: {f1_self*100:.2f}% (Claimed: 74.68%)\")\n",
    "print(f\"Result: {'MATCHES' if abs(f1_self*100 - 74.68) < 0.1 else 'DOES NOT MATCH'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy-Based Evaluation (GPT-4.1-mini test data)\n",
    "test_gpt_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\")\n",
    "with open(test_gpt_path, \"r\") as f:\n",
    "    test_gpt = json.load(f)\n",
    "\n",
    "test_gpt_dict = {\"identifier\": [], **{col: [] for col in ATTENTION_COLS}, \n",
    "                 **{col: [] for col in PARAMETER_COLS}, \"hallucination_label\": []}\n",
    "for i, resp in enumerate(test_gpt):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_gpt_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_gpt_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            test_gpt_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_gpt_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_gpt_df = pd.DataFrame(test_gpt_dict)\n",
    "test_gpt_df['pred'] = svc_model.predict(test_gpt_df[features])\n",
    "test_gpt_df[\"response_id\"] = test_gpt_df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
    "agg_gpt = test_gpt_df.groupby(\"response_id\").agg({\"pred\": \"max\", \"hallucination_label\": \"max\"}).reset_index()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(agg_gpt[\"hallucination_label\"], agg_gpt[\"pred\"]).ravel()\n",
    "f1_proxy = 2 * (tp/(tp+fp)) * (tp/(tp+fn)) / ((tp/(tp+fp)) + (tp/(tp+fn)))\n",
    "print(f\"Proxy-Based Evaluation F1: {f1_proxy*100:.2f}% (Claimed: 75.36%)\")\n",
    "print(f\"Result: {'MATCHES' if abs(f1_proxy*100 - 75.36) < 0.1 else 'DOES NOT MATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640b193",
   "metadata": {},
   "source": [
    "## CS2: Plan vs Implementation Analysis\n",
    "\n",
    "Verifying that all methodology steps from the plan are implemented in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read implementation files\n",
    "scripts_path = os.path.join(repo_path, 'scripts')\n",
    "with open(os.path.join(scripts_path, 'compute_scores.py'), 'r') as f:\n",
    "    compute_scores_content = f.read()\n",
    "with open(os.path.join(scripts_path, 'classifier.py'), 'r') as f:\n",
    "    classifier_content = f.read()\n",
    "with open(os.path.join(scripts_path, 'predict.py'), 'r') as f:\n",
    "    predict_content = f.read()\n",
    "\n",
    "# Verify each plan step\n",
    "cs2_checks = {\n",
    "    \"Step1_ECS\": {\n",
    "        \"attention_weights\": \"outputs.attentions\" in compute_scores_content,\n",
    "        \"cosine_similarity\": \"calculate_sentence_similarity\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step2_PKS\": {\n",
    "        \"jensen_shannon\": \"calculate_dist_2d\" in compute_scores_content,\n",
    "        \"kl_divergence\": \"F.kl_div\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step3_TransformerLens\": {\n",
    "        \"hooked_transformer\": \"HookedTransformer\" in compute_scores_content,\n",
    "        \"run_with_cache\": \"run_with_cache\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step4_Classifiers\": {\n",
    "        \"LR\": \"LogisticRegression\" in classifier_content,\n",
    "        \"SVC\": \"SVC\" in classifier_content,\n",
    "        \"RF\": \"RandomForestClassifier\" in classifier_content,\n",
    "        \"XGB\": \"XGBClassifier\" in classifier_content,\n",
    "    },\n",
    "    \"Step5_Evaluation\": {\n",
    "        \"response_level\": \"response_id\" in predict_content,\n",
    "        \"test_data_qwen\": os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")),\n",
    "        \"test_data_gpt\": os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\")),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"CS2: Plan vs Implementation Verification\")\n",
    "print(\"=\"*50)\n",
    "all_pass = True\n",
    "for step, checks in cs2_checks.items():\n",
    "    step_pass = all(checks.values())\n",
    "    all_pass = all_pass and step_pass\n",
    "    print(f\"\\n{step}: {'PASS' if step_pass else 'FAIL'}\")\n",
    "    for check, result in checks.items():\n",
    "        print(f\"  {'✓' if result else '✗'} {check}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CS2 Overall: {'PASS' if all_pass else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42284812",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### CS1: Results vs Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final CS1 Summary\n",
    "cs1_results = {\n",
    "    \"ECS_Correlation\": {\n",
    "        \"claim\": \"All attention heads exhibit negative correlations\",\n",
    "        \"verified\": negative_corr == len(ecs_pcc_direct),\n",
    "        \"details\": f\"All {len(ecs_pcc_direct)} heads show negative correlation\"\n",
    "    },\n",
    "    \"PKS_Correlation\": {\n",
    "        \"claim\": \"Later-layer FFNs have higher positive correlation\",\n",
    "        \"verified\": np.mean(later_layers) > np.mean(early_layers),\n",
    "        \"details\": f\"Early: {np.mean(early_layers):.4f}, Later: {np.mean(later_layers):.4f}\"\n",
    "    },\n",
    "    \"Classifier_Selection\": {\n",
    "        \"claim\": \"SVC achieved highest validation F1 (76.60%)\",\n",
    "        \"verified\": max(model_results.items(), key=lambda x: x[1]['f1'])[0] == 'SVC',\n",
    "        \"details\": f\"Best: {max(model_results.items(), key=lambda x: x[1]['f1'])[0]} with {max(model_results.values(), key=lambda x: x['f1'])['f1']*100:.2f}%\"\n",
    "    },\n",
    "    \"Self_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=74.68%\",\n",
    "        \"verified\": abs(f1_self*100 - 74.68) < 0.1,\n",
    "        \"details\": f\"Actual: {f1_self*100:.2f}%\"\n",
    "    },\n",
    "    \"Proxy_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=75.36%\",\n",
    "        \"verified\": abs(f1_proxy*100 - 75.36) < 0.1,\n",
    "        \"details\": f\"Actual: {f1_proxy*100:.2f}%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"CS1: Results vs Conclusions\")\n",
    "print(\"=\"*60)\n",
    "for key, result in cs1_results.items():\n",
    "    status = \"PASS\" if result['verified'] else \"FAIL\"\n",
    "    print(f\"\\n{key}: {status}\")\n",
    "    print(f\"  Claim: {result['claim']}\")\n",
    "    print(f\"  Details: {result['details']}\")\n",
    "\n",
    "cs1_pass = all(r['verified'] for r in cs1_results.values())\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CS1 Overall: {'PASS' if cs1_pass else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfaab13",
   "metadata": {},
   "source": [
    "### Binary Checklist Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d42e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BINARY CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCS1 (Results vs Conclusions): {'PASS' if cs1_pass else 'FAIL'}\")\n",
    "print(f\"CS2 (Plan vs Implementation): {'PASS' if all_pass else 'FAIL'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not cs1_pass:\n",
    "    print(\"\\nCS1 FAIL Reason:\")\n",
    "    for key, result in cs1_results.items():\n",
    "        if not result['verified']:\n",
    "            print(f\"  - {key}: {result['details']}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
