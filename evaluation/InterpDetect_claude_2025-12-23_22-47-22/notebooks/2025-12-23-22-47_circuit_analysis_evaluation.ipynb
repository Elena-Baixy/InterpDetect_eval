{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b82bf3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3e020",
   "metadata": {},
   "source": [
    "# Code Evaluation: InterpDetect Circuit Analysis\n",
    "\n",
    "This notebook provides a strict, deterministic evaluation of the code implementing the circuit analysis in the repository:\n",
    "`/net/scratch2/smallyan/InterpDetect_eval`\n",
    "\n",
    "## Evaluation Criteria\n",
    "1. **Runnable (Y/N)** - Block executes without error\n",
    "2. **Correct-Implementation (Y/N)** - Logic implements described computation correctly\n",
    "3. **Redundant (Y/N)** - Block duplicates another block's computation\n",
    "4. **Irrelevant (Y/N)** - Block does not contribute to project goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d78458",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43954a",
   "metadata": {},
   "source": [
    "## Step 1: Identify Core Analysis Files\n",
    "\n",
    "Based on the CodeWalkthrough.md, the core analysis scripts are:\n",
    "\n",
    "### Main Analysis Pipeline (Part 2: Training & Prediction)\n",
    "1. **scripts/compute_scores.py** - Computes PKS (Parameter Knowledge Score) and ECS (External Context Score)\n",
    "2. **scripts/classifier.py** - Trains classifiers (LR, SVC, RandomForest, XGBoost)\n",
    "3. **scripts/predict.py** - Makes predictions using trained models\n",
    "\n",
    "### Preprocessing Pipeline (Part 1)\n",
    "4. **scripts/preprocess/preprocess.py** - Adds prompt and prompt_spans to raw data\n",
    "5. **scripts/preprocess/generate_response_hf.py** - Generates responses using HuggingFace models\n",
    "6. **scripts/preprocess/generate_response_gpt.py** - Generates responses using GPT models  \n",
    "7. **scripts/preprocess/generate_labels.py** - Generates hallucination labels\n",
    "8. **scripts/preprocess/filter.py** - Filters datasets based on confidence\n",
    "9. **scripts/preprocess/helper.py** - Utility functions\n",
    "\n",
    "### Baselines (Part 3)\n",
    "10. **scripts/baseline/run_hf.py** - HuggingFace baseline\n",
    "11. **scripts/baseline/run_ragas.py** - RAGAS baseline\n",
    "12. **scripts/baseline/run_gpt.py** - GPT baseline\n",
    "13. **scripts/baseline/run_groq.py** - Groq baseline\n",
    "14. **scripts/baseline/run_refchecker.py** - RefChecker baseline\n",
    "15. **scripts/baseline/run_trulens.py** - TruLens baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42f686b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: /net/scratch2/smallyan/InterpDetect_eval/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "import os\n",
    "\n",
    "output_dir = \"/net/scratch2/smallyan/InterpDetect_eval/evaluation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09b7f9",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Core Analysis Code\n",
    "\n",
    "### 2.1 Evaluate compute_scores.py\n",
    "\n",
    "This script computes PKS (Parameter Knowledge Score) and ECS (External Context Score) using TransformerLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad03446",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports for compute_scores.py successful\n"
     ]
    }
   ],
   "source": [
    "# First, let's test the imports and helper functions from compute_scores.py\n",
    "import sys\n",
    "sys.path.insert(0, '/net/scratch2/smallyan/InterpDetect_eval/scripts')\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer\n",
    "    from transformer_lens import HookedTransformer\n",
    "    import json\n",
    "    from torch.nn import functional as F\n",
    "    from typing import Dict, List, Tuple\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import pointbiserialr\n",
    "    print(\"✓ All imports for compute_scores.py successful\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddba7a7a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading examples from /net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/datasets/test/test1176_w_labels_filtered.jsonl...\n",
      "Loaded 256 examples\n",
      "✓ load_examples function works correctly\n",
      "  First example keys: dict_keys(['id', 'question', 'documents', 'documents_sentences', 'prompt', 'prompt_spans', 'num_tokens', 'response', 'response_spans', 'labels', 'hallucinated_llama-4-maverick-17b-128e-instruct', 'hallucinated_gpt-oss-120b', 'labels_llama', 'labels_gpt'])\n"
     ]
    }
   ],
   "source": [
    "# Test helper functions from compute_scores.py\n",
    "\n",
    "# Test load_examples function\n",
    "def load_examples(file_path):\n",
    "    \"\"\"Load examples from JSONL file\"\"\"\n",
    "    print(f\"Loading examples from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        examples = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                examples.append(data)\n",
    "        \n",
    "        print(f\"Loaded {len(examples)} examples\")\n",
    "        return examples\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading examples: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with actual data\n",
    "test_file = \"/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/datasets/test/test1176_w_labels_filtered.jsonl\"\n",
    "if os.path.exists(test_file):\n",
    "    examples = load_examples(test_file)\n",
    "    if examples and len(examples) > 0:\n",
    "        print(f\"✓ load_examples function works correctly\")\n",
    "        print(f\"  First example keys: {examples[0].keys()}\")\n",
    "    else:\n",
    "        print(f\"✗ load_examples returned empty or None\")\n",
    "else:\n",
    "    print(f\"Test file not found: {test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272f6b7a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ calculate_dist_2d function works correctly\n",
      "  JS divergence score: 2.8917\n"
     ]
    }
   ],
   "source": [
    "# Test the JS divergence calculation function\n",
    "def calculate_dist_2d(sep_vocabulary_dist, sep_attention_dist):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence between distributions\"\"\"\n",
    "    # Calculate softmax\n",
    "    softmax_mature_layer = F.softmax(sep_vocabulary_dist, dim=-1)\n",
    "    softmax_anchor_layer = F.softmax(sep_attention_dist, dim=-1)\n",
    "\n",
    "    # Calculate the average distribution M\n",
    "    M = 0.5 * (softmax_mature_layer + softmax_anchor_layer)\n",
    "\n",
    "    # Calculate log-softmax for the KL divergence\n",
    "    log_softmax_mature_layer = F.log_softmax(sep_vocabulary_dist, dim=-1)\n",
    "    log_softmax_anchor_layer = F.log_softmax(sep_attention_dist, dim=-1)\n",
    "\n",
    "    # Calculate the KL divergences and then the JS divergences\n",
    "    kl1 = F.kl_div(log_softmax_mature_layer, M, reduction='none').sum(dim=-1)\n",
    "    kl2 = F.kl_div(log_softmax_anchor_layer, M, reduction='none').sum(dim=-1)\n",
    "    js_divs = 0.5 * (kl1 + kl2)\n",
    "\n",
    "    scores = js_divs.cpu().tolist()\n",
    "    return sum(scores)\n",
    "\n",
    "# Test with random tensors\n",
    "torch.manual_seed(42)\n",
    "dist1 = torch.randn(10, 100)\n",
    "dist2 = torch.randn(10, 100)\n",
    "\n",
    "try:\n",
    "    js_score = calculate_dist_2d(dist1, dist2)\n",
    "    print(f\"✓ calculate_dist_2d function works correctly\")\n",
    "    print(f\"  JS divergence score: {js_score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ calculate_dist_2d error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c971a9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ add_special_template error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--Qwen--Qwen3-0.6B'\n"
     ]
    }
   ],
   "source": [
    "# Test add_special_template function\n",
    "def add_special_template(tokenizer, prompt):\n",
    "    \"\"\"Add special template to prompt\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Load tokenizer and test\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "    test_prompt = \"What is 2+2?\"\n",
    "    templated = add_special_template(tokenizer, test_prompt)\n",
    "    print(f\"✓ add_special_template function works correctly\")\n",
    "    print(f\"  Template length: {len(templated)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ add_special_template error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d2eb86",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ add_special_template error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--Qwen--Qwen3-0.6B'\n"
     ]
    }
   ],
   "source": [
    "# Set cache to scratch to avoid quota issues\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/net/scratch2/smallyan/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/net/scratch2/smallyan/hf_cache\"\n",
    "os.makedirs(\"/net/scratch2/smallyan/hf_cache\", exist_ok=True)\n",
    "\n",
    "# Retry loading tokenizer\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "    test_prompt = \"What is 2+2?\"\n",
    "    templated = add_special_template(tokenizer, test_prompt)\n",
    "    print(f\"✓ add_special_template function works correctly\")\n",
    "    print(f\"  Template length: {len(templated)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ add_special_template error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfc3ccc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87edaf1fedad4ff3a8c055c0871a6a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df82a49dbc3d482a9bb852fcb6d58fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c647c6c59f44895b43291bf769f7e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055e4531fb254eb08f6837b45fd588c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ add_special_template function works correctly\n",
      "  Template length: 120 characters\n"
     ]
    }
   ],
   "source": [
    "# Use a different approach - force local cache\n",
    "import os\n",
    "\n",
    "# Clear any previous cache config\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/net/scratch2/smallyan/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/net/scratch2/smallyan/hf_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/net/scratch2/smallyan/hf_cache\"\n",
    "\n",
    "# Create cache dir\n",
    "cache_dir = \"/net/scratch2/smallyan/hf_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Reload transformers\n",
    "import importlib\n",
    "import transformers\n",
    "importlib.reload(transformers)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", cache_dir=cache_dir)\n",
    "    test_prompt = \"What is 2+2?\"\n",
    "    templated = add_special_template(tokenizer, test_prompt)\n",
    "    print(f\"✓ add_special_template function works correctly\")\n",
    "    print(f\"  Template length: {len(templated)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ add_special_template error: {e}\")\n",
    "    print(\"Note: This appears to be a disk quota issue, not a code issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d92d4e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ is_hallucination_span function works correctly\n",
      "  Test result (should be True): True\n",
      "  Test result (should be False): False\n"
     ]
    }
   ],
   "source": [
    "# Test helper functions for span calculations\n",
    "def is_hallucination_span(r_span, hallucination_spans):\n",
    "    \"\"\"Check if a span contains hallucination\"\"\"\n",
    "    for token_id in range(r_span[0], r_span[1]):\n",
    "        for span in hallucination_spans:\n",
    "            if token_id >= span[0] and token_id <= span[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Test\n",
    "test_r_span = [10, 20]\n",
    "test_hallucination_spans = [[15, 25], [30, 40]]\n",
    "result = is_hallucination_span(test_r_span, test_hallucination_spans)\n",
    "print(f\"✓ is_hallucination_span function works correctly\")\n",
    "print(f\"  Test result (should be True): {result}\")\n",
    "\n",
    "# Test with non-overlapping\n",
    "test_r_span2 = [50, 60]\n",
    "result2 = is_hallucination_span(test_r_span2, test_hallucination_spans)\n",
    "print(f\"  Test result (should be False): {result2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b41a476",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model setup on device: cuda\n",
      "Setting up models: qwen3-0.6b, Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting up models: qwen3-0.6b not found. Valid official model names (excl aliases): ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', 'stanford-crfm/alias-gpt2-small-x21', 'stanford-crfm/battlestar-gpt2-small-x49', 'stanford-crfm/caprica-gpt2-small-x81', 'stanford-crfm/darkmatter-gpt2-small-x343', 'stanford-crfm/expanse-gpt2-small-x777', 'stanford-crfm/arwen-gpt2-medium-x21', 'stanford-crfm/beren-gpt2-medium-x49', 'stanford-crfm/celebrimbor-gpt2-medium-x81', 'stanford-crfm/durin-gpt2-medium-x343', 'stanford-crfm/eowyn-gpt2-medium-x777', 'EleutherAI/pythia-14m', 'EleutherAI/pythia-31m', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b', 'EleutherAI/pythia-12b', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-1b-deduped', 'EleutherAI/pythia-1.4b-deduped', 'EleutherAI/pythia-2.8b-deduped', 'EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-12b-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-160m-v0', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-1b-v0', 'EleutherAI/pythia-1.4b-v0', 'EleutherAI/pythia-2.8b-v0', 'EleutherAI/pythia-6.9b-v0', 'EleutherAI/pythia-12b-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/pythia-1b-deduped-v0', 'EleutherAI/pythia-1.4b-deduped-v0', 'EleutherAI/pythia-2.8b-deduped-v0', 'EleutherAI/pythia-6.9b-deduped-v0', 'EleutherAI/pythia-12b-deduped-v0', 'EleutherAI/pythia-160m-seed1', 'EleutherAI/pythia-160m-seed2', 'EleutherAI/pythia-160m-seed3', 'NeelNanda/SoLU_1L_v9_old', 'NeelNanda/SoLU_2L_v10_old', 'NeelNanda/SoLU_4L_v11_old', 'NeelNanda/SoLU_6L_v13_old', 'NeelNanda/SoLU_8L_v21_old', 'NeelNanda/SoLU_10L_v22_old', 'NeelNanda/SoLU_12L_v23_old', 'NeelNanda/SoLU_1L512W_C4_Code', 'NeelNanda/SoLU_2L512W_C4_Code', 'NeelNanda/SoLU_3L512W_C4_Code', 'NeelNanda/SoLU_4L512W_C4_Code', 'NeelNanda/SoLU_6L768W_C4_Code', 'NeelNanda/SoLU_8L1024W_C4_Code', 'NeelNanda/SoLU_10L1280W_C4_Code', 'NeelNanda/SoLU_12L1536W_C4_Code', 'NeelNanda/GELU_1L512W_C4_Code', 'NeelNanda/GELU_2L512W_C4_Code', 'NeelNanda/GELU_3L512W_C4_Code', 'NeelNanda/GELU_4L512W_C4_Code', 'NeelNanda/Attn_Only_1L512W_C4_Code', 'NeelNanda/Attn_Only_2L512W_C4_Code', 'NeelNanda/Attn_Only_3L512W_C4_Code', 'NeelNanda/Attn_Only_4L512W_C4_Code', 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr', 'NeelNanda/SoLU_1L512W_Wiki_Finetune', 'NeelNanda/SoLU_4L512W_Wiki_Finetune', 'ArthurConmy/redwood_attn_2l', 'llama-7b-hf', 'llama-13b-hf', 'llama-30b-hf', 'llama-65b-hf', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', 'codellama/CodeLlama-7b-hf', 'codellama/CodeLlama-7b-Python-hf', 'codellama/CodeLlama-7b-Instruct-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3-70B', 'meta-llama/Meta-Llama-3-70B-Instruct', 'meta-llama/Llama-3.2-1B', 'meta-llama/Llama-3.2-3B', 'meta-llama/Llama-3.2-1B-Instruct', 'meta-llama/Llama-3.2-3B-Instruct', 'meta-llama/Llama-3.1-70B', 'meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.1-8B-Instruct', 'meta-llama/Llama-3.1-70B-Instruct', 'Baidicoot/Othello-GPT-Transformer-Lens', 'bert-base-cased', 'roneneldan/TinyStories-1M', 'roneneldan/TinyStories-3M', 'roneneldan/TinyStories-8M', 'roneneldan/TinyStories-28M', 'roneneldan/TinyStories-33M', 'roneneldan/TinyStories-Instruct-1M', 'roneneldan/TinyStories-Instruct-3M', 'roneneldan/TinyStories-Instruct-8M', 'roneneldan/TinyStories-Instruct-28M', 'roneneldan/TinyStories-Instruct-33M', 'roneneldan/TinyStories-1Layer-21M', 'roneneldan/TinyStories-2Layers-33M', 'roneneldan/TinyStories-Instuct-1Layer-21M', 'roneneldan/TinyStories-Instruct-2Layers-33M', 'stabilityai/stablelm-base-alpha-3b', 'stabilityai/stablelm-base-alpha-7b', 'stabilityai/stablelm-tuned-alpha-3b', 'stabilityai/stablelm-tuned-alpha-7b', 'mistralai/Mistral-7B-v0.1', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mistral-Nemo-Base-2407', 'mistralai/Mixtral-8x7B-v0.1', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'bigscience/bloom-560m', 'bigscience/bloom-1b1', 'bigscience/bloom-1b7', 'bigscience/bloom-3b', 'bigscience/bloom-7b1', 'bigcode/santacoder', 'Qwen/Qwen-1_8B', 'Qwen/Qwen-7B', 'Qwen/Qwen-14B', 'Qwen/Qwen-1_8B-Chat', 'Qwen/Qwen-7B-Chat', 'Qwen/Qwen-14B-Chat', 'Qwen/Qwen1.5-0.5B', 'Qwen/Qwen1.5-0.5B-Chat', 'Qwen/Qwen1.5-1.8B', 'Qwen/Qwen1.5-1.8B-Chat', 'Qwen/Qwen1.5-4B', 'Qwen/Qwen1.5-4B-Chat', 'Qwen/Qwen1.5-7B', 'Qwen/Qwen1.5-7B-Chat', 'Qwen/Qwen1.5-14B', 'Qwen/Qwen1.5-14B-Chat', 'Qwen/Qwen2-0.5B', 'Qwen/Qwen2-0.5B-Instruct', 'Qwen/Qwen2-1.5B', 'Qwen/Qwen2-1.5B-Instruct', 'Qwen/Qwen2-7B', 'Qwen/Qwen2-7B-Instruct', 'microsoft/phi-1', 'microsoft/phi-1_5', 'microsoft/phi-2', 'microsoft/Phi-3-mini-4k-instruct', 'google/gemma-2b', 'google/gemma-7b', 'google/gemma-2b-it', 'google/gemma-7b-it', 'google/gemma-2-2b', 'google/gemma-2-2b-it', 'google/gemma-2-9b', 'google/gemma-2-9b-it', 'google/gemma-2-27b', 'google/gemma-2-27b-it', '01-ai/Yi-6B', '01-ai/Yi-34B', '01-ai/Yi-6B-Chat', '01-ai/Yi-34B-Chat', 'google-t5/t5-small', 'google-t5/t5-base', 'google-t5/t5-large', 'ai-forever/mGPT']\n",
      "✗ setup_models failed\n"
     ]
    }
   ],
   "source": [
    "# Test the full model setup with TransformerLens\n",
    "def setup_models(model_name, hf_model_name, device=\"cuda\", cache_dir=None):\n",
    "    \"\"\"Setup tokenizer, model, and sentence transformer\"\"\"\n",
    "    print(f\"Setting up models: {model_name}, {hf_model_name}\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        from transformer_lens import HookedTransformer\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_model_name, cache_dir=cache_dir)\n",
    "        \n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            model_name,\n",
    "            device=\"cpu\",\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        model.to(device)\n",
    "        \n",
    "        bge_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\", cache_folder=cache_dir).to(device)\n",
    "        \n",
    "        return tokenizer, model, bge_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up models: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Test setup (use a smaller test to save time)\n",
    "cache_dir = \"/net/scratch2/smallyan/hf_cache\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Testing model setup on device: {device}\")\n",
    "tokenizer, model, bge_model = setup_models(\"qwen3-0.6b\", \"Qwen/Qwen3-0.6B\", device, cache_dir)\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"✓ setup_models function works correctly\")\n",
    "    print(f\"  Model config: n_layers={model.cfg.n_layers}, n_heads={model.cfg.n_heads}\")\n",
    "else:\n",
    "    print(\"✗ setup_models failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c3013c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Qwen models in TransformerLens:\n",
      "  - Qwen/Qwen2-0.5B\n",
      "  - Qwen/Qwen2-1.5B\n",
      "  - Qwen/Qwen1.5-0.5B\n",
      "\n",
      "NOTE: The code uses 'qwen3-0.6b' but TransformerLens doesn't support Qwen3 models.\n",
      "This is a Correct-Implementation issue - the model name is invalid for TransformerLens.\n",
      "\n",
      "Testing with Qwen/Qwen2-0.5B instead...\n"
     ]
    }
   ],
   "source": [
    "# The code has an issue - \"qwen3-0.6b\" is not a valid TransformerLens model name\n",
    "# This is a CORRECT-IMPLEMENTATION issue - let's document this\n",
    "\n",
    "# Check if Qwen2 models are available in TransformerLens\n",
    "valid_qwen_models = [m for m in ['Qwen/Qwen2-0.5B', 'Qwen/Qwen2-1.5B', 'Qwen/Qwen1.5-0.5B'] ]\n",
    "print(\"Valid Qwen models in TransformerLens:\")\n",
    "for m in valid_qwen_models:\n",
    "    print(f\"  - {m}\")\n",
    "    \n",
    "print(\"\\nNOTE: The code uses 'qwen3-0.6b' but TransformerLens doesn't support Qwen3 models.\")\n",
    "print(\"This is a Correct-Implementation issue - the model name is invalid for TransformerLens.\")\n",
    "\n",
    "# Let's test with a valid model instead to verify the rest of the code works\n",
    "print(\"\\nTesting with Qwen/Qwen2-0.5B instead...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14513ded",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2-0.5B with TransformerLens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9307e030aed4a15ac9494d64b377ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--Qwen--Qwen2-0.5B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--Qwen--Qwen2-0.5B'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--Qwen--Qwen2-0.5B'\n"
     ]
    }
   ],
   "source": [
    "# Test with a valid model that is available in TransformerLens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "cache_dir = \"/net/scratch2/smallyan/hf_cache\"\n",
    "\n",
    "try:\n",
    "    # Use a smaller model for testing - Qwen2-0.5B \n",
    "    print(\"Loading Qwen/Qwen2-0.5B with TransformerLens...\")\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        \"Qwen/Qwen2-0.5B\",\n",
    "        device=\"cpu\",\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    model = model.to(\"cuda\")\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"  Model config: n_layers={model.cfg.n_layers}, n_heads={model.cfg.n_heads}, n_ctx={model.cfg.n_ctx}\")\n",
    "    \n",
    "    # Load BGE model\n",
    "    print(\"\\nLoading BGE model...\")\n",
    "    bge_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\", cache_folder=cache_dir).to(\"cuda\")\n",
    "    print(f\"✓ BGE model loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7c10e19",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directories set to: /net/scratch2/smallyan/hf_cache\n",
      "Available disk space check:\n",
      "Filesystem                                  Size  Used Avail Use% Mounted on\n",
      "cluster-storage4.ds.uchicago.edu:/scratch2  171T   92T   80T  54% /net/scratch2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The cache issue is external - let's force all cache to our scratch directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Force HuggingFace to use only our cache directory\n",
    "cache_dir = \"/net/scratch2/smallyan/hf_cache\"\n",
    "\n",
    "# Set all possible cache environment variables BEFORE importing libraries\n",
    "os.environ[\"HF_HOME\"] = cache_dir\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "os.environ[\"XDG_CACHE_HOME\"] = cache_dir\n",
    "\n",
    "# For transformer_lens specifically\n",
    "os.environ[\"TRANSFORMER_LENS_CACHE\"] = cache_dir\n",
    "\n",
    "print(f\"Cache directories set to: {cache_dir}\")\n",
    "print(\"Available disk space check:\")\n",
    "import subprocess\n",
    "result = subprocess.run(['df', '-h', '/net/scratch2/'], capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895048ea",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16278a5cab340ae9f2fa7352dbe0a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model with explicit cache_dir...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9283d8f3dbf645399008af92967bb740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54f0b8819924b17b28911772392007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ac827607504907badd4ce5a1432226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126693dca0b744799fce9fcb978aa6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8a20cbe6db4e7ea5a05b1a636d4152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f83183956894553b64b2f52bf7a9f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-0.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "  Model config: n_layers=24, n_heads=14\n"
     ]
    }
   ],
   "source": [
    "# Restart Python kernel state for transformers\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_remove = [k for k in sys.modules.keys() if 'huggingface' in k or 'transformers' in k or 'transformer_lens' in k]\n",
    "for m in modules_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "# Now import fresh\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "cache_dir = \"/net/scratch2/smallyan/hf_cache\"\n",
    "\n",
    "try:\n",
    "    # Force cache directory in the from_pretrained call\n",
    "    print(\"Attempting to load model with explicit cache_dir...\")\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        \"Qwen/Qwen2-0.5B\",\n",
    "        device=\"cpu\",\n",
    "        torch_dtype=torch.float16,\n",
    "        hf_model = None,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    model = model.to(\"cuda\")\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"  Model config: n_layers={model.cfg.n_layers}, n_heads={model.cfg.n_heads}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model loading error: {e}\")\n",
    "    print(\"\\nNote: This is an environment issue (disk quota on shared cache), not a code logic issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3aeffd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f468ff79ea774e63ba1735568899c635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85735f4b5364367aec6438539bab3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05010b7ad2d94785a71a2c927a4bb32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb6d46741e74abfa3ac9be229139631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190346e13aaa45529256285f12d559b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457078290f5041529998ce6b2954d230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276191747aa54c49816375eee57df9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe178bc8749410d923f3126cb8ce22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec9fa9409244410a05d13d86b9e37c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749151c728fa4e469b369efb9bed86ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a745c25ed8c4ceea4ac1ed426483768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BGE model loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ calculate_sentence_similarity works correctly\n",
      "  Similarity score: 0.9214\n"
     ]
    }
   ],
   "source": [
    "# Load BGE model for sentence similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\", cache_folder=cache_dir).to(\"cuda\")\n",
    "print(\"✓ BGE model loaded successfully\")\n",
    "\n",
    "# Test sentence similarity function\n",
    "def calculate_sentence_similarity(bge_model, r_text, p_text):\n",
    "    \"\"\"Calculate sentence similarity using BGE model\"\"\"\n",
    "    part_embedding = bge_model.encode([r_text], normalize_embeddings=True)\n",
    "    q_embeddings = bge_model.encode([p_text], normalize_embeddings=True)\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    scores_named = np.matmul(q_embeddings, part_embedding.T).flatten()\n",
    "    return float(scores_named[0])\n",
    "\n",
    "# Test\n",
    "sim = calculate_sentence_similarity(bge_model, \"The revenue was $100M.\", \"Company reported revenue of $100 million.\")\n",
    "print(f\"✓ calculate_sentence_similarity works correctly\")\n",
    "print(f\"  Similarity score: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18199137",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Test with a simple input\u001b[39;00m\n\u001b[1;32m     27\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m test_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test the MockOutputs class and process_example function\n",
    "class MockOutputs:\n",
    "    \"\"\"Mock outputs class for transformer lens compatibility\"\"\"\n",
    "    def __init__(self, cache, model_cfg):\n",
    "        self.cache = cache\n",
    "        self.model_cfg = model_cfg\n",
    "\n",
    "    @property\n",
    "    def attentions(self):\n",
    "        attentions = []\n",
    "        for layer in range(self.model_cfg.n_layers):\n",
    "            attn_pattern = self.cache[f\"blocks.{layer}.attn.hook_pattern\"]\n",
    "            attentions.append(attn_pattern)\n",
    "        return tuple(attentions)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key == \"hidden_states\":\n",
    "            hidden_states = []\n",
    "            for layer in range(self.model_cfg.n_layers):\n",
    "                hidden_state = self.cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "                hidden_states.append(hidden_state)\n",
    "            return tuple(hidden_states)\n",
    "        else:\n",
    "            raise KeyError(f\"Key {key} not found\")\n",
    "\n",
    "# Test with a simple input\n",
    "test_text = \"Hello, how are you?\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logits, cache = model.run_with_cache(test_ids, return_type=\"logits\")\n",
    "outputs = MockOutputs(cache, model.cfg)\n",
    "\n",
    "print(\"✓ MockOutputs class works correctly\")\n",
    "print(f\"  Attentions shape (layer 0): {outputs.attentions[0].shape}\")\n",
    "print(f\"  Hidden states shape (layer 0): {outputs['hidden_states'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46b3171",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MockOutputs class works correctly\n",
      "  Attentions shape (layer 0): torch.Size([1, 14, 6, 6])\n",
      "  Hidden states shape (layer 0): torch.Size([1, 6, 896])\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\", cache_dir=cache_dir)\n",
    "\n",
    "# Test with a simple input\n",
    "test_text = \"Hello, how are you?\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logits, cache = model.run_with_cache(test_ids, return_type=\"logits\")\n",
    "outputs = MockOutputs(cache, model.cfg)\n",
    "\n",
    "print(\"✓ MockOutputs class works correctly\")\n",
    "print(f\"  Attentions shape (layer 0): {outputs.attentions[0].shape}\")\n",
    "print(f\"  Hidden states shape (layer 0): {outputs['hidden_states'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c86de1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Define helper functions needed for process_example\n",
    "def calculate_hallucination_spans(response, text, response_rag, tokenizer, prefix_len):\n",
    "    \"\"\"Calculate hallucination spans\"\"\"\n",
    "    hallucination_span = []\n",
    "    for item in response:\n",
    "        start_id = item['start']\n",
    "        end_id = item['end']\n",
    "        start_text = text + response_rag[:start_id]\n",
    "        end_text = text + response_rag[:end_id]\n",
    "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
    "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
    "        start_id = start_text_id.shape[-1]\n",
    "        end_id = end_text_id.shape[-1]\n",
    "        hallucination_span.append([start_id, end_id])\n",
    "    return hallucination_span\n",
    "\n",
    "def calculate_respond_spans(raw_response_spans, text, response_rag, tokenizer):\n",
    "    \"\"\"Calculate response spans\"\"\"\n",
    "    respond_spans = []\n",
    "    for item in raw_response_spans:\n",
    "        start_id = item[0]\n",
    "        end_id = item[1]\n",
    "        start_text = text + response_rag[:start_id]\n",
    "        end_text = text + response_rag[:end_id]\n",
    "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
    "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
    "        start_id = start_text_id.shape[-1]\n",
    "        end_id = end_text_id.shape[-1]\n",
    "        respond_spans.append([start_id, end_id])\n",
    "    return respond_spans\n",
    "\n",
    "def calculate_prompt_spans(raw_prompt_spans, prompt, tokenizer):\n",
    "    \"\"\"Calculate prompt spans\"\"\"\n",
    "    prompt_spans = []\n",
    "    for item in raw_prompt_spans:\n",
    "        start_id = item[0]\n",
    "        end_id = item[1]\n",
    "        start_text = prompt[:start_id]\n",
    "        end_text = prompt[:end_id]\n",
    "        added_start_text = add_special_template(tokenizer, start_text)\n",
    "        added_end_text = add_special_template(tokenizer, end_text)\n",
    "        start_text_id = tokenizer(added_start_text, return_tensors=\"pt\").input_ids.shape[-1] - 4\n",
    "        end_text_id = tokenizer(added_end_text, return_tensors=\"pt\").input_ids.shape[-1] - 4\n",
    "        prompt_spans.append([start_text_id, end_text_id])\n",
    "    return prompt_spans\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782a62d1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ process_example function defined\n"
     ]
    }
   ],
   "source": [
    "# Define the full process_example function\n",
    "def process_example(example, tokenizer, model, bge_model, device, max_ctx, iter_step=1):\n",
    "    \"\"\"Process a single example to compute scores\"\"\"\n",
    "    response_rag = example['response']\n",
    "    prompt = example['prompt']\n",
    "    original_prompt_spans = example['prompt_spans']\n",
    "    original_response_spans = example['response_spans']\n",
    "\n",
    "    text = add_special_template(tokenizer, prompt)\n",
    "\n",
    "    prompt_ids = tokenizer([text], return_tensors=\"pt\").input_ids\n",
    "    response_ids = tokenizer([response_rag], return_tensors=\"pt\").input_ids\n",
    "    input_ids = torch.cat([prompt_ids, response_ids[:, 1:]], dim=1)\n",
    "\n",
    "    if input_ids.shape[-1] > max_ctx:\n",
    "        overflow = input_ids.shape[-1] - max_ctx\n",
    "        input_ids = input_ids[:, overflow:]\n",
    "        prompt_kept = max(prompt_ids.shape[-1] - overflow, 0)\n",
    "    else:\n",
    "        prompt_kept = prompt_ids.shape[-1]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    prefix_len = prompt_kept\n",
    "\n",
    "    if \"labels\" in example.keys():\n",
    "        hallucination_spans = calculate_hallucination_spans(example['labels'], text, response_rag, tokenizer, prefix_len)\n",
    "    else:\n",
    "        hallucination_spans = []\n",
    "\n",
    "    prompt_spans = calculate_prompt_spans(example['prompt_spans'], prompt, tokenizer)\n",
    "    respond_spans = calculate_respond_spans(example['response_spans'], text, response_rag, tokenizer)\n",
    "\n",
    "    # Run model with cache to get all intermediate activations\n",
    "    logits, cache = model.run_with_cache(\n",
    "        input_ids,\n",
    "        return_type=\"logits\"\n",
    "    )\n",
    "\n",
    "    outputs = MockOutputs(cache, model.cfg)\n",
    "\n",
    "    # skip tokens without hallucination\n",
    "    hidden_states = outputs[\"hidden_states\"]\n",
    "    last_hidden_states = hidden_states[-1][0, :, :]\n",
    "    del hidden_states\n",
    "\n",
    "    span_score_dict = []\n",
    "    for r_id, r_span in enumerate(respond_spans):\n",
    "        layer_head_span = {}\n",
    "        parameter_knowledge_dict = {}\n",
    "        for attentions_layer_id in range(0, model.cfg.n_layers, iter_step):\n",
    "            for head_id in range(model.cfg.n_heads):\n",
    "                layer_head = (attentions_layer_id, head_id)\n",
    "                p_span_score_dict = []\n",
    "                for p_span in prompt_spans:\n",
    "                    attention_score = outputs.attentions[attentions_layer_id][0, head_id, :, :]\n",
    "                    p_span_score_dict.append([p_span, torch.sum(attention_score[r_span[0]:r_span[1], p_span[0]:p_span[1]]).cpu().item()])\n",
    "                \n",
    "                # Get the span with maximum score\n",
    "                if len(p_span_score_dict) > 0:\n",
    "                    p_id = max(range(len(p_span_score_dict)), key=lambda i: p_span_score_dict[i][1])\n",
    "                    prompt_span_text = prompt[original_prompt_spans[p_id][0]:original_prompt_spans[p_id][1]]\n",
    "                    respond_span_text = response_rag[original_response_spans[r_id][0]:original_response_spans[r_id][1]]\n",
    "                    layer_head_span[str(layer_head)] = calculate_sentence_similarity(bge_model, prompt_span_text, respond_span_text)\n",
    "\n",
    "            x_mid = cache[f\"blocks.{attentions_layer_id}.hook_resid_mid\"][0, r_span[0]:r_span[1], :]\n",
    "            x_post = cache[f\"blocks.{attentions_layer_id}.hook_resid_post\"][0, r_span[0]:r_span[1], :]\n",
    "\n",
    "            score = calculate_dist_2d(\n",
    "                x_mid @ model.W_U,\n",
    "                x_post @ model.W_U\n",
    "            )\n",
    "            parameter_knowledge_dict[f\"layer_{attentions_layer_id}\"] = score\n",
    "\n",
    "        span_score_dict.append({\n",
    "            \"prompt_attention_score\": layer_head_span,\n",
    "            \"r_span\": r_span,\n",
    "            \"hallucination_label\": 1 if is_hallucination_span(r_span, hallucination_spans) else 0,\n",
    "            \"parameter_knowledge_scores\": parameter_knowledge_dict\n",
    "        })\n",
    "\n",
    "    example[\"scores\"] = span_score_dict\n",
    "    return example\n",
    "\n",
    "print(\"✓ process_example function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcc5c3a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading examples from /net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/datasets/test/test1176_w_labels_filtered.jsonl...\n",
      "Loaded 256 examples\n",
      "Testing with example ID: finqa_6345\n",
      "  Prompt length: 1819 chars\n",
      "  Response length: 655 chars\n",
      "  Prompt spans: 8\n",
      "  Response spans: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ process_example works correctly!\n",
      "  Number of span scores: 5\n",
      "  Sample span score keys: dict_keys(['prompt_attention_score', 'r_span', 'hallucination_label', 'parameter_knowledge_scores'])\n",
      "  Number of attention scores: 84\n",
      "  Number of PKS scores: 6\n"
     ]
    }
   ],
   "source": [
    "# Test with a real example from the dataset\n",
    "# Load test examples\n",
    "test_file = \"/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/datasets/test/test1176_w_labels_filtered.jsonl\"\n",
    "examples = load_examples(test_file)\n",
    "\n",
    "# Take first example\n",
    "example = examples[0]\n",
    "print(f\"Testing with example ID: {example.get('id', 'N/A')}\")\n",
    "print(f\"  Prompt length: {len(example['prompt'])} chars\")\n",
    "print(f\"  Response length: {len(example['response'])} chars\")\n",
    "print(f\"  Prompt spans: {len(example['prompt_spans'])}\")\n",
    "print(f\"  Response spans: {len(example['response_spans'])}\")\n",
    "\n",
    "# Process the example\n",
    "max_ctx = model.cfg.n_ctx\n",
    "device = \"cuda\"\n",
    "\n",
    "try:\n",
    "    result = process_example(example, tokenizer, model, bge_model, device, max_ctx, iter_step=4)  # Use larger step for faster test\n",
    "    print(f\"\\n✓ process_example works correctly!\")\n",
    "    print(f\"  Number of span scores: {len(result['scores'])}\")\n",
    "    if result['scores']:\n",
    "        print(f\"  Sample span score keys: {result['scores'][0].keys()}\")\n",
    "        print(f\"  Number of attention scores: {len(result['scores'][0]['prompt_attention_score'])}\")\n",
    "        print(f\"  Number of PKS scores: {len(result['scores'][0]['parameter_knowledge_scores'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ process_example error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db0a3f",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate classifier.py\n",
    "\n",
    "This script trains classifiers (LR, SVC, RandomForest, XGBoost) on the computed scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fde558b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports for classifier.py successful\n"
     ]
    }
   ],
   "source": [
    "# Test imports for classifier.py\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pickle\n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "    print(\"✓ All imports for classifier.py successful\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee7755a2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /net/scratch2/smallyan/InterpDetect_eval/datasets/train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1800 examples from 18 files\n",
      "✓ load_data function works correctly\n",
      "  First example keys: dict_keys(['id', 'question', 'documents', 'documents_sentences', 'prompt', 'prompt_spans', 'num_tokens', 'response', 'response_spans', 'labels', 'hallucinated_llama-4-maverick-17b-128e-instruct', 'hallucinated_gpt-oss-120b', 'labels_llama', 'labels_gpt', 'scores'])\n"
     ]
    }
   ],
   "source": [
    "# Test load_data function from classifier.py\n",
    "def load_data(folder_path):\n",
    "    \"\"\"Load data from JSON files in the specified folder\"\"\"\n",
    "    print(f\"Loading data from {folder_path}...\")\n",
    "    \n",
    "    try:\n",
    "        response = []\n",
    "        json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"No JSON files found in {folder_path}\")\n",
    "            return None\n",
    "        \n",
    "        for file_path in json_files:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                response.extend(data)\n",
    "        \n",
    "        print(f\"Loaded {len(response)} examples from {len(json_files)} files\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with actual training data\n",
    "train_folder = \"/net/scratch2/smallyan/InterpDetect_eval/datasets/train\"\n",
    "response = load_data(train_folder)\n",
    "\n",
    "if response:\n",
    "    print(f\"✓ load_data function works correctly\")\n",
    "    print(f\"  First example keys: {response[0].keys()}\")\n",
    "else:\n",
    "    print(\"✗ load_data failed or no data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99d44497",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 7799 samples\n",
      "Class distribution: {0: 4406, 1: 3393}\n",
      "After balancing: {0: 3393, 1: 3393}\n",
      "\n",
      "✓ preprocess_data function works correctly\n",
      "  DataFrame shape: (6786, 478)\n",
      "  Number of attention columns: 448\n",
      "  Number of parameter columns: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3041301/2688690020.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min_count, random_state=random_state))\n"
     ]
    }
   ],
   "source": [
    "# Test preprocess_data function from classifier.py\n",
    "def preprocess_data(response, balance_classes=True, random_state=42):\n",
    "    \"\"\"Preprocess the loaded data into a DataFrame\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    if not response:\n",
    "        print(\"No data to preprocess\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Get column names from first example\n",
    "    ATTENTION_COLS = response[0]['scores'][0]['prompt_attention_score'].keys()\n",
    "    PARAMETER_COLS = response[0]['scores'][0]['parameter_knowledge_scores'].keys()\n",
    "    \n",
    "    data_dict = {\n",
    "        \"identifier\": [],\n",
    "        **{col: [] for col in ATTENTION_COLS},\n",
    "        **{col: [] for col in PARAMETER_COLS},\n",
    "        \"hallucination_label\": []\n",
    "    }\n",
    "    \n",
    "    for i, resp in enumerate(response):\n",
    "        for j in range(len(resp[\"scores\"])):\n",
    "            data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "            for col in ATTENTION_COLS:\n",
    "                data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "            \n",
    "            for col in PARAMETER_COLS:\n",
    "                data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "            data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "    \n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    print(f\"Created DataFrame with {len(df)} samples\")\n",
    "    print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Balance classes if requested\n",
    "    if balance_classes:\n",
    "        min_count = df['hallucination_label'].value_counts().min()\n",
    "        df = (\n",
    "            df.groupby('hallucination_label', group_keys=False)\n",
    "              .apply(lambda x: x.sample(min_count, random_state=random_state))\n",
    "        )\n",
    "        print(f\"After balancing: {df['hallucination_label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df, list(ATTENTION_COLS), list(PARAMETER_COLS)\n",
    "\n",
    "# Test\n",
    "df, attention_cols, parameter_cols = preprocess_data(response, balance_classes=True)\n",
    "if df is not None:\n",
    "    print(f\"\\n✓ preprocess_data function works correctly\")\n",
    "    print(f\"  DataFrame shape: {df.shape}\")\n",
    "    print(f\"  Number of attention columns: {len(attention_cols)}\")\n",
    "    print(f\"  Number of parameter columns: {len(parameter_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a32fae0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and validation sets...\n",
      "Train set: 6107 samples\n",
      "Validation set: 679 samples\n",
      "Number of features: 476\n",
      "\n",
      "✓ split_data function works correctly\n"
     ]
    }
   ],
   "source": [
    "# Test split_data and train_models functions\n",
    "def split_data(df, test_size=0.1, random_state=42):\n",
    "    \"\"\"Split data into train and validation sets\"\"\"\n",
    "    print(\"Splitting data into train and validation sets...\")\n",
    "    \n",
    "    train, val = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df['hallucination_label'])\n",
    "    \n",
    "    features = [col for col in df.columns if col not in ['identifier', 'hallucination_label']]\n",
    "    \n",
    "    X_train = train[features]\n",
    "    y_train = train[\"hallucination_label\"]\n",
    "    X_val = val[features]\n",
    "    y_val = val[\"hallucination_label\"]\n",
    "    \n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Number of features: {len(features)}\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, features\n",
    "\n",
    "# Test\n",
    "X_train, X_val, y_train, y_val, features = split_data(df)\n",
    "print(f\"\\n✓ split_data function works correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1e98a6a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ create_preprocessor function works correctly\n",
      "  Pipeline steps: [('scaler', StandardScaler())]\n"
     ]
    }
   ],
   "source": [
    "# Test create_preprocessor and train_models\n",
    "def create_preprocessor(use_feature_selection=False):\n",
    "    \"\"\"Create preprocessing pipeline\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if use_feature_selection:\n",
    "        try:\n",
    "            from feature_engine.selection import DropConstantFeatures, SmartCorrelatedSelection, DropDuplicateFeatures\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            \n",
    "            drop_const = DropConstantFeatures(tol=0.95, missing_values='ignore')\n",
    "            drop_dup = DropDuplicateFeatures()\n",
    "            drop_corr = SmartCorrelatedSelection(\n",
    "                method='pearson', \n",
    "                threshold=0.90,\n",
    "                selection_method='model_performance',\n",
    "                estimator=RandomForestClassifier(max_depth=5, random_state=42)\n",
    "            )\n",
    "            \n",
    "            preprocessor = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('drop_constant', drop_const),\n",
    "                ('drop_duplicates', drop_dup),\n",
    "                ('smart_corr_selection', drop_corr),\n",
    "            ])\n",
    "        except ImportError:\n",
    "            print(\"feature_engine not available, using simple preprocessing\")\n",
    "            preprocessor = Pipeline([('scaler', scaler)])\n",
    "    else:\n",
    "        preprocessor = Pipeline([\n",
    "            ('scaler', scaler),\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = create_preprocessor(use_feature_selection=False)\n",
    "print(f\"✓ create_preprocessor function works correctly\")\n",
    "print(f\"  Pipeline steps: {preprocessor.steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96f8c1ae",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "Training LR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "      Algorithm   Train_p     Val_p   Train_r     Val_r   Train_f     Val_f\n",
      "0            LR  0.794437  0.737500  0.766863  0.696165  0.780407  0.716237\n",
      "1  RandomForest  0.794307  0.763754  0.767518  0.696165  0.780683  0.728395\n",
      "\n",
      "✓ train_models function works correctly\n",
      "  Trained models: ['LR', 'RandomForest']\n"
     ]
    }
   ],
   "source": [
    "# Test train_models function\n",
    "def train_models(X_train, X_val, y_train, y_val, preprocessor, models_to_train=None):\n",
    "    \"\"\"Train multiple models and compare their performance\"\"\"\n",
    "    print(\"Training models...\")\n",
    "    \n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # Define models to train\n",
    "    if models_to_train is None:\n",
    "        models_to_train = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
    "    \n",
    "    models = []\n",
    "    if \"LR\" in models_to_train:\n",
    "        models.append((\"LR\", LogisticRegression()))\n",
    "    if \"SVC\" in models_to_train:\n",
    "        models.append(('SVC', SVC()))\n",
    "    if \"RandomForest\" in models_to_train:\n",
    "        models.append(('RandomForest', RandomForestClassifier(max_depth=5)))\n",
    "    if \"XGBoost\" in models_to_train:\n",
    "        models.append(('XGBoost', XGBClassifier(max_depth=5)))\n",
    "    \n",
    "    # Initialize lists for results\n",
    "    names = []\n",
    "    train_ps = []\n",
    "    train_rs = []\n",
    "    train_fs = []\n",
    "    val_ps = []\n",
    "    val_rs = []\n",
    "    val_fs = []\n",
    "    clfs = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for name, model_obj in models:\n",
    "        print(f\"Training {name}...\")\n",
    "        names.append(name)\n",
    "        clf = make_pipeline(preprocessor, model_obj)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp, tr, tf, _ = precision_recall_fscore_support(y_train, clf.predict(X_train), average='binary')\n",
    "        train_ps.append(tp)\n",
    "        train_rs.append(tr)\n",
    "        train_fs.append(tf)\n",
    "        \n",
    "        vp, vr, vf, _ = precision_recall_fscore_support(y_val, clf.predict(X_val), average='binary')\n",
    "        val_ps.append(vp)\n",
    "        val_rs.append(vr)\n",
    "        val_fs.append(vf)\n",
    "        \n",
    "        clfs[name] = clf\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    model_comparison = pd.DataFrame({\n",
    "        'Algorithm': names,\n",
    "        'Train_p': train_ps,\n",
    "        'Val_p': val_ps,\n",
    "        'Train_r': train_rs,\n",
    "        'Val_r': val_rs,\n",
    "        'Train_f': train_fs,\n",
    "        'Val_f': val_fs,\n",
    "    })\n",
    "    \n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(model_comparison)\n",
    "    \n",
    "    return clfs, model_comparison\n",
    "\n",
    "# Test training with just LR for speed\n",
    "clfs, model_comparison = train_models(X_train, X_val, y_train, y_val, preprocessor, models_to_train=[\"LR\", \"RandomForest\"])\n",
    "print(f\"\\n✓ train_models function works correctly\")\n",
    "print(f\"  Trained models: {list(clfs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926ed0f9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models to /net/scratch2/smallyan/InterpDetect_eval/evaluation/temp_models...\n",
      "Saved LR model to /net/scratch2/smallyan/InterpDetect_eval/evaluation/temp_models/model_LR_test.pickle\n",
      "Saved RandomForest model to /net/scratch2/smallyan/InterpDetect_eval/evaluation/temp_models/model_RandomForest_test.pickle\n",
      "\n",
      "✓ save_models function works correctly\n"
     ]
    }
   ],
   "source": [
    "# Test save_models function\n",
    "def save_models(clfs, output_dir):\n",
    "    \"\"\"Save trained models\"\"\"\n",
    "    print(f\"Saving models to {output_dir}...\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for name, clf in clfs.items():\n",
    "        model_path = os.path.join(output_dir, f\"model_{name}_test.pickle\")\n",
    "        with open(model_path, \"wb\") as fout:\n",
    "            pickle.dump(clf, fout)\n",
    "        print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "# Test (save to temp directory)\n",
    "temp_output = \"/net/scratch2/smallyan/InterpDetect_eval/evaluation/temp_models\"\n",
    "save_models(clfs, temp_output)\n",
    "print(f\"\\n✓ save_models function works correctly\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28379f8b",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate predict.py\n",
    "\n",
    "This script makes predictions using trained models and evaluates at span and response level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f33c2a64",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports for predict.py successful\n"
     ]
    }
   ],
   "source": [
    "# Test imports for predict.py\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pickle\n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "    print(\"✓ All imports for predict.py successful\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a46a1ed",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /net/scratch2/smallyan/InterpDetect_eval/datasets/test/test_w_chunk_score_qwen06b.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256 examples\n",
      "✓ load_data function works correctly\n",
      "  First example keys: dict_keys(['id', 'question', 'documents', 'documents_sentences', 'prompt', 'prompt_spans', 'num_tokens', 'response', 'response_spans', 'labels', 'hallucinated_llama-4-maverick-17b-128e-instruct', 'hallucinated_gpt-oss-120b', 'labels_llama', 'labels_gpt', 'scores'])\n"
     ]
    }
   ],
   "source": [
    "# Test load_data function from predict.py\n",
    "def load_data_predict(data_path):\n",
    "    \"\"\"Load data from JSON file\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(data_path, \"r\") as f:\n",
    "            response = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(response)} examples\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with test data\n",
    "test_data_path = \"/net/scratch2/smallyan/InterpDetect_eval/datasets/test/test_w_chunk_score_qwen06b.json\"\n",
    "response = load_data_predict(test_data_path)\n",
    "\n",
    "if response:\n",
    "    print(f\"✓ load_data function works correctly\")\n",
    "    print(f\"  First example keys: {response[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41ed8199",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Created DataFrame with 975 samples\n",
      "Class distribution: {0: 699, 1: 276}\n",
      "\n",
      "✓ preprocess_data function works correctly\n"
     ]
    }
   ],
   "source": [
    "# Test preprocess_data for predict.py\n",
    "def preprocess_data_predict(response):\n",
    "    \"\"\"Preprocess the loaded data into a DataFrame\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    if not response:\n",
    "        print(\"No data to preprocess\")\n",
    "        return None\n",
    "    \n",
    "    # Get column names from first example\n",
    "    ATTENTION_COLS = response[0]['scores'][0]['prompt_attention_score'].keys()\n",
    "    PARAMETER_COLS = response[0]['scores'][0]['parameter_knowledge_scores'].keys()\n",
    "    \n",
    "    data_dict = {\n",
    "        \"identifier\": [],\n",
    "        **{col: [] for col in ATTENTION_COLS},\n",
    "        **{col: [] for col in PARAMETER_COLS},\n",
    "        \"hallucination_label\": []\n",
    "    }\n",
    "    \n",
    "    for i, resp in enumerate(response):\n",
    "        for j in range(len(resp[\"scores\"])):\n",
    "            data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "            for col in ATTENTION_COLS:\n",
    "                data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "            \n",
    "            for col in PARAMETER_COLS:\n",
    "                data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "            data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "    \n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    print(f\"Created DataFrame with {len(df)} samples\")\n",
    "    print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_test = preprocess_data_predict(response)\n",
    "print(f\"\\n✓ preprocess_data function works correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3623995a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /net/scratch2/smallyan/InterpDetect_eval/trained_models/model_SVC_3000.pickle...\n",
      "Model loaded successfully\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed for 975 samples\n",
      "\n",
      "✓ make_predictions function works correctly\n",
      "  Prediction value counts: {0: 595, 1: 380}\n"
     ]
    }
   ],
   "source": [
    "# Test load_model and make_predictions\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load trained model from pickle file\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        print(\"Model loaded successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def make_predictions(df, model):\n",
    "    \"\"\"Make predictions using the loaded model\"\"\"\n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    features = [col for col in df.columns if col not in ['identifier', 'hallucination_label']]\n",
    "    y_pred = model.predict(df[features])\n",
    "    df['pred'] = y_pred\n",
    "    \n",
    "    print(f\"Predictions completed for {len(df)} samples\")\n",
    "    return df\n",
    "\n",
    "# Load a pre-trained model\n",
    "model_path = \"/net/scratch2/smallyan/InterpDetect_eval/trained_models/model_SVC_3000.pickle\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "if model:\n",
    "    df_test = make_predictions(df_test, model)\n",
    "    print(f\"\\n✓ make_predictions function works correctly\")\n",
    "    print(f\"  Prediction value counts: {df_test['pred'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90db0103",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Span-level Evaluation ===\n",
      "TP: 213, TN: 532, FP: 167, FN: 63\n",
      "Precision: 0.561\n",
      "Recall: 0.772\n",
      "F1 Score: 0.649\n",
      "\n",
      "✓ evaluate_span_level function works correctly\n"
     ]
    }
   ],
   "source": [
    "# Test evaluate_span_level function\n",
    "def evaluate_span_level(df):\n",
    "    \"\"\"Evaluate predictions at span level\"\"\"\n",
    "    print(\"\\n=== Span-level Evaluation ===\")\n",
    "    \n",
    "    # Confusion matrix: tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = confusion_matrix(df[\"hallucination_label\"], df[\"pred\"]).ravel()\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = precision_score(df[\"hallucination_label\"], df[\"pred\"])\n",
    "    recall = recall_score(df[\"hallucination_label\"], df[\"pred\"])\n",
    "    f1 = f1_score(df[\"hallucination_label\"], df[\"pred\"])\n",
    "    \n",
    "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "        'precision': precision, 'recall': recall, 'f1': f1\n",
    "    }\n",
    "\n",
    "span_results = evaluate_span_level(df_test)\n",
    "print(f\"\\n✓ evaluate_span_level function works correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91d0f811",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Response-level Evaluation ===\n",
      "TP: 115, TN: 63, FP: 65, FN: 13\n",
      "Precision: 0.6389\n",
      "Recall: 0.8984\n",
      "F1 Score: 0.7468\n",
      "\n",
      "✓ evaluate_response_level function works correctly\n"
     ]
    }
   ],
   "source": [
    "# Test evaluate_response_level function\n",
    "def evaluate_response_level(df):\n",
    "    \"\"\"Evaluate predictions at response level\"\"\"\n",
    "    print(\"\\n=== Response-level Evaluation ===\")\n",
    "    \n",
    "    # Extract response_id from identifier (everything before \"_item_\")\n",
    "    df[\"response_id\"] = df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
    "    \n",
    "    # Group by response_id, aggregate with OR (max works for binary 0/1)\n",
    "    agg_df = df.groupby(\"response_id\").agg({\n",
    "        \"pred\": \"max\",\n",
    "        \"hallucination_label\": \"max\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Confusion matrix: tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = confusion_matrix(agg_df[\"hallucination_label\"], agg_df[\"pred\"]).ravel()\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = precision_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
    "    recall = recall_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
    "    f1 = f1_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
    "    \n",
    "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "        'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'agg_df': agg_df\n",
    "    }\n",
    "\n",
    "response_results = evaluate_response_level(df_test)\n",
    "print(f\"\\n✓ evaluate_response_level function works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341bcdc",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate Preprocessing Scripts\n",
    "\n",
    "Testing preprocess.py, generate_labels.py, filter.py, helper.py, and response generation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67149635",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ clean_text function works correctly\n",
      "  Original: 'hello  .  this is a test . . . another sentence'\n",
      "  Cleaned: 'Hello. This is a test. Another sentence'\n"
     ]
    }
   ],
   "source": [
    "# Test helper.py functions\n",
    "import sys\n",
    "sys.path.insert(0, '/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess')\n",
    "\n",
    "# Test clean_text function\n",
    "import re\n",
    "import nltk\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces before punctuation\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    # Collapse multiple periods (e.g., \". . .\" => \".\")\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    # Fix spacing after punctuation\n",
    "    text = re.sub(r'([.,!?;:])(?=\\w)', r'\\1 ', text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Capitalize first letter of each sentence\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip().capitalize() for s in sentences if s.strip()]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# Test\n",
    "test_text = \"hello  .  this is a test . . . another sentence\"\n",
    "cleaned = clean_text(test_text)\n",
    "print(f\"✓ clean_text function works correctly\")\n",
    "print(f\"  Original: '{test_text}'\")\n",
    "print(f\"  Cleaned: '{cleaned}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffc172b7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ get_sentence_spans function works correctly\n",
      "  Text: 'This is the first sentence. This is the second sentence. And this is the third.'\n",
      "  Spans: [(0, 27), (28, 56), (57, 79)]\n"
     ]
    }
   ],
   "source": [
    "# Test get_sentence_spans function from helper.py\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_sentence_spans(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for sentence in sentences:\n",
    "        start = text.find(sentence, start)\n",
    "        end = start + len(sentence)\n",
    "        spans.append((start, end))\n",
    "        start = end\n",
    "    return spans\n",
    "\n",
    "# Test\n",
    "test_text = \"This is the first sentence. This is the second sentence. And this is the third.\"\n",
    "spans = get_sentence_spans(test_text)\n",
    "print(f\"✓ get_sentence_spans function works correctly\")\n",
    "print(f\"  Text: '{test_text}'\")\n",
    "print(f\"  Spans: {spans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "130c9dbc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ add_prompt_spans function works correctly\n",
      "  Generated prompt length: 250 chars\n",
      "  Number of spans: 7\n"
     ]
    }
   ],
   "source": [
    "# Test preprocess.py - add_prompt_spans function\n",
    "def add_prompt_spans(df):\n",
    "    \"\"\"Build prompt and compute spans for the dataset\"\"\"\n",
    "    part1 = \"Given the context, please answer the question based on the provided information from the context. Include any reasoning with the answer\\n\"\n",
    "    part2 = \"\\nContext:\"\n",
    "    part3 = \"\\nQuestion:\"\n",
    "    part4 = \"\\nAnswer:\"\n",
    "\n",
    "    prompt_texts = []\n",
    "    prompt_spans = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        docs = list(row[\"documents\"])  # assume list of document strings\n",
    "        \n",
    "        # prefix\n",
    "        prompt = \"\"\n",
    "        spans = []\n",
    "        l1 = len(part1)\n",
    "        prompt+=part1\n",
    "        spans.append([0, l1-1])\n",
    "        \n",
    "        # context\n",
    "        l2 = len(part2)\n",
    "        prompt+=part2\n",
    "        spans.append([l1, l1+l2-1])\n",
    "        cur = l1+l2\n",
    "        for doc in docs:\n",
    "            doc = clean_text(doc)\n",
    "            prompt+=doc\n",
    "            spans.append([cur, cur+len(doc)-1])\n",
    "            cur = cur+len(doc)\n",
    "\n",
    "        # question\n",
    "        l3 = len(part3)\n",
    "        prompt+=part3\n",
    "        spans.append([cur, cur+l3-1])\n",
    "        cur = cur+l3\n",
    "        prompt+=question\n",
    "        spans.append([cur, cur+len(question)-1])\n",
    "        cur = cur+len(question)\n",
    "        \n",
    "        # answer\n",
    "        l4 = len(part4)\n",
    "        prompt+=part4\n",
    "        spans.append([cur, cur+l4-1])\n",
    "\n",
    "        # append\n",
    "        prompt_texts.append(prompt)\n",
    "        prompt_spans.append(spans)\n",
    "\n",
    "    return prompt_texts, prompt_spans\n",
    "\n",
    "# Test with sample data\n",
    "test_df = pd.DataFrame({\n",
    "    'question': ['What is the revenue?'],\n",
    "    'documents': [['Company A reported revenue of $100M in 2023.', 'The growth rate was 5%.']]\n",
    "})\n",
    "\n",
    "prompts, spans = add_prompt_spans(test_df)\n",
    "print(f\"✓ add_prompt_spans function works correctly\")\n",
    "print(f\"  Generated prompt length: {len(prompts[0])} chars\")\n",
    "print(f\"  Number of spans: {len(spans[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13dbaeed",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding binary labels for LLM judge evaluations...\n",
      "✓ add_labels_llm function works correctly\n",
      "  Labels llama: [0, 1]\n",
      "  Labels gpt: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test filter.py - add_labels_llm function\n",
    "import textwrap\n",
    "\n",
    "def add_labels_llm(df, llama_column, gpt_column):\n",
    "    \"\"\"Add binary labels for LLM judge evaluations\"\"\"\n",
    "    print(\"Adding binary labels for LLM judge evaluations...\")\n",
    "    \n",
    "    labels_llama = []\n",
    "    labels_gpt = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            # Process Llama labels\n",
    "            if \"Yes\" in row[llama_column]:\n",
    "                labels_llama.append(0)\n",
    "            elif \"No\" in row[llama_column]:\n",
    "                labels_llama.append(1)\n",
    "            else:\n",
    "                labels_llama.append(-1)  # Error indicator\n",
    "\n",
    "            # Process GPT labels\n",
    "            if \"Yes\" in row[gpt_column]:\n",
    "                labels_gpt.append(0)\n",
    "            elif \"No\" in row[gpt_column]:\n",
    "                labels_gpt.append(1)\n",
    "            else:\n",
    "                labels_gpt.append(-1)  # Error indicator\n",
    "                \n",
    "        except Exception as e:\n",
    "            labels_llama.append(-1)\n",
    "            labels_gpt.append(-1)\n",
    "\n",
    "    df['labels_llama'] = labels_llama\n",
    "    df['labels_gpt'] = labels_gpt\n",
    "    return df\n",
    "\n",
    "# Test with sample data\n",
    "test_df = pd.DataFrame({\n",
    "    'hallucinated_llama': ['Yes, the response is correct.', 'No, there are errors.'],\n",
    "    'hallucinated_gpt': ['Yes', 'No, incorrect.']\n",
    "})\n",
    "\n",
    "result_df = add_labels_llm(test_df, 'hallucinated_llama', 'hallucinated_gpt')\n",
    "print(f\"✓ add_labels_llm function works correctly\")\n",
    "print(f\"  Labels llama: {result_df['labels_llama'].tolist()}\")\n",
    "print(f\"  Labels gpt: {result_df['labels_gpt'].tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-23-22-47_circuit_analysis_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
