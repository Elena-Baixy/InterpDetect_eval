{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5859c68b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a373626d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef1cde3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIR]  .git\n",
      "[FILE] .gitignore\n",
      "[FILE] CodeWalkthrough.md\n",
      "[FILE] LICENSE\n",
      "[DIR]  datasets\n",
      "[FILE] documentation.pdf\n",
      "[FILE] plan.md\n",
      "[FILE] requirements.txt\n",
      "[DIR]  scripts\n",
      "[DIR]  trained_models\n"
     ]
    }
   ],
   "source": [
    "# Explore the repo structure\n",
    "repo_path = '/net/scratch2/smallyan/InterpDetect_eval'\n",
    "\n",
    "# List top-level contents\n",
    "for item in sorted(os.listdir(repo_path)):\n",
    "    full_path = os.path.join(repo_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"[DIR]  {item}\")\n",
    "    else:\n",
    "        print(f\"[FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a9bc17",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Plan\n",
      "## Objective\n",
      "Develop a mechanistic interpretability-based hallucination detection method for Retrieval-Augmented Generation (RAG) systems by computing External Context Scores (ECS) across layers and attention heads and Parametric Knowledge Scores (PKS) across layers (FFN), training regression-based classifiers on these signals, and demonstrating generalization from a small proxy model (Qwen3-0.6b) to larger production models (GPT-4.1-mini).\n",
      "\n",
      "## Hypothesis\n",
      "1. RAG hallucinations correlate with:  later-layer FFN modules disproportionately inject parametric knowledge into the residual stream while attention heads fail to adequately exploit external context.\n",
      "2. External Context Score (ECS) and Parametric Knowledge Score (PKS) are correlated with hallucination occurrence and can serve as predictive features for hallucination detection.\n",
      "3. Mechanistic signals extracted from a small proxy model (0.6b parameters) can generalize to detect hallucinations in responses from larger production-level models.\n",
      "\n",
      "## Methodology\n",
      "1. Compute External Context Score (ECS) per attention head and layer by identifying the most attended context chunk via attention weights, then measuring cosine similarity between response and context embeddings.\n",
      "2. Compute Parametric Knowledge Score (PKS) per FFN layer by measuring Jensen-Shannon divergence between vocabulary distributions before and after the FFN layer in the residual stream.\n",
      "3. Use TransformerLens library on Qwen3-0.6b model to extract internal mechanistic signals (ECS and PKS) at span level across 28 layers and 16 attention heads.\n",
      "4. Train binary classifiers (Logistic Regression, SVC, Random Forest, XGBoost) on standardized and correlation-filtered ECS/PKS features to predict span-level hallucinations, then aggregate to response-level.\n",
      "5. Evaluate both self-evaluation (same model generates responses and computes signals) and proxy-based evaluation (Qwen3-0.6b signals applied to GPT-4.1-mini responses) settings.\n",
      "\n",
      "## Experiments\n",
      "### Correlation Analysis: ECS vs Hallucination\n",
      "- What varied: Comparing ECS values between truthful and hallucinated responses across layers and attention heads\n",
      "- Metric: Pearson Correlation Coefficient between inverse hallucination label and ECS\n",
      "- Main result: All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\n",
      "\n",
      "### Correlation Analysis: PKS vs Hallucination\n",
      "- What varied: Comparing PKS values between truthful and hallucinated responses across FFN layers\n",
      "- Metric: Pearson correlation between hallucination labels and PKS\n",
      "- Main result: Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\n",
      "\n",
      "### Classifier Training and Selection\n",
      "- What varied: Four classifier types: Logistic Regression, SVC, Random Forest, XGBoost trained on 7,799 span-level samples\n",
      "- Metric: Validation F1 score, precision, and recall at span level\n",
      "- Main result: SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\n",
      "\n",
      "### Self-Evaluation Detection\n",
      "- What varied: Comparing proposed method against baselines (LLMs and commercial tools) on Qwen3-0.6b generated responses\n",
      "- Metric: Response-level Precision, Recall, F1\n",
      "- Main result: Method achieved F1=74.68%, outperforming TruLens (67.32%) and llama-3.1-8b-instant (57.53%), comparable to RefChecker (75.86%).\n",
      "\n",
      "### Proxy-Based Evaluation Detection\n",
      "- What varied: Applying Qwen3-0.6b trained classifier to GPT-4.1-mini responses against same baselines\n",
      "- Metric: Response-level Precision, Recall, F1\n",
      "- Main result: Method achieved F1=75.36%, outperforming nearly all models except GPT-5 (76.92%) and RAGAS (76.19%), using only 0.6b parameter signals.\n"
     ]
    }
   ],
   "source": [
    "# Read the plan file\n",
    "with open(os.path.join(repo_path, 'plan.md'), 'r') as f:\n",
    "    plan_content = f.read()\n",
    "print(plan_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c016c7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# InterpDetect\n",
      "\n",
      "**InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation**\n",
      "\n",
      "[![Paper](https://img.shields.io/badge/Paper-OpenReview-blue)](https://openreview.net/pdf?id=TZzBKwHLwF)\n",
      "[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n",
      "\n",
      "A comprehensive framework for detecting and analyzing hallucinations in Retrieval-Augmented Generation (RAG) systems using interpretability techniques and chunk-level analysis on the RAGBench/FinQA dataset.\n",
      "\n",
      "## Overview\n",
      "\n",
      "This project implements a novel approach to hallucination detection by leveraging interpretability methods. The framework consists of three main parts:\n",
      "\n",
      "1. **Preprocessing Pipeline** - Convert raw datasets to labeled data with hallucination spans\n",
      "2. **Training & Prediction** - Train classifiers or use pre-trained models for hallucination detection\n",
      "3. **Baseline Comparisons** - Evaluate against multiple baseline methods (RAGAS, TruLens, RefChecker, GPT-based, etc.)\n",
      "\n",
      "The framework can work with existing labeled datasets or process raw data through the complete pipeline.\n",
      "\n",
      "## Features\n",
      "\n",
      "- **Interpretability Focus**: Uses interpretability techniques to understand model decisions\n",
      "- **Chunk-level Analysis**: Breaks down responses into chunks and analyzes each for hallucination likelihood\n",
      "- **Trained Models**: Pre-trained machine learning models (Logistic Regression, Random Forest, SVC, XGBoost) for hallucination detection\n",
      "- **Multiple Baselines**: Implements various baseline methods including GPT, Groq, HuggingFace models, RAGAS, RefChecker, and TruLens\n",
      "\n",
      "\n",
      "## Project Structure\n",
      "\n",
      "```\n",
      "interpretablity-hallucination-detection/\n",
      "├── datasets/                    # Data files\n",
      "│   ├── OV_copying_score.json   # Overlap copying scores\n",
      "│   ├── test/                   # Chunk-level scores for testing\n",
      "│   └── train/                  # Chunk-level scores for training\n",
      "├── scripts/                    # Python scripts for pipeline execution\n",
      "|   └── baseline/               # Baseline implementations\n",
      "|       |── requirements.txt    # Python dependencies for baselines\n",
      "│       ├── run_gpt.py          # GPT baseline\n",
      "│       ├── run_groq.py         # Groq baseline\n",
      "│       ├── run_hf.py           # HuggingFace baseline\n",
      "│       ├── run_ragas.py        # RAGAS baseline\n",
      "│       ├── run_refchecker.py   # RefChecker baseline\n",
      "│       └── run_trulens.py      # TruLens baseline\n",
      "│   ├── preprocess              # Data preprocessing\n",
      "|       |── datasets            # Preprocessed Train and Test\n",
      "|       |── preprocess.py           # 1. add prompt and prompt_spans to raw data\n",
      "│       ├── generate_response.py    # 2. Response generation (either hf models or gpt)\n",
      "│       ├── generate_labels.py      # 3. Generate Hallucination labels and Add LLM-as-a-Judge\n",
      "│       ├── filter.py               # 4. Run majority voting to filter out low confident prediction\n",
      "|       ├── helper.py               # Utility functions\n",
      "│   ├── compute_scores.py       # Chunk-level score computation\n",
      "│   ├── classifier.py           # Model training \n",
      "│   ├── predict.py              # Model prediction    \n",
      "│   \n",
      "├── trained_models/             # Pre-trained ML models\n",
      "│   ├── model_LR_3000.pickle    # Logistic Regression model\n",
      "│   ├── model_RandomForest_3000.pickle # Random Forest model\n",
      "│   ├── model_SVC_3000.pickle   # Support Vector Classifier\n",
      "│   └── model_XGBoost_3000.pickle # XGBoost model\n",
      "├── requirements.txt            # Python dependencies\n",
      "└── README.md                   # This file\n",
      "```\n",
      "\n",
      "## Installation\n",
      "\n",
      "1. Clone the repository:\n",
      "```bash\n",
      "git clone <repository-url>\n",
      "cd InterpDetect\n",
      "```\n",
      "\n",
      "2. Create a virtual environment (recommended):\n",
      "```bash\n",
      "python -m venv venv\n",
      "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
      "```\n",
      "\n",
      "3. Install required dependencies:\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "4. Set up environment variables (create a `.env` file):\n",
      "```bash\n",
      "# API Keys\n",
      "OPENAI_API_KEY=your_openai_api_key\n",
      "GROQ_API_KEY=your_groq_api_key\n",
      "\n",
      "# Optional: HuggingFace token for private datasets\n",
      "HUGGINGFACE_TOKEN=your_hf_token\n",
      "```\n",
      "\n",
      "## Usage\n",
      "\n",
      "The framework consists of three main parts that can be used independently or together:\n",
      "\n",
      "### Part 1: Preprocessing Pipeline (Optional)\n",
      "\n",
      "**Skip this part if you already have datasets with the required format:**\n",
      "- Required columns: `prompt`, `prompt_spans`, `response`, `response_spans`, `labels` (containing hallucinated spans)\n",
      "\n",
      "If you need to process raw data, see the [Preprocessing README](scripts/preprocess/README.md) for detailed instructions.\n",
      "\n",
      "**Quick preprocessing workflow:**\n",
      "\n",
      "```bash\n",
      "# Step 1: Generate responses using GPT\n",
      "python scripts/preprocess/generate_response_gpt.py \\\n",
      "  --model_name \"gpt-4.1-mini\" \\\n",
      "  --train_samples 3000 \\\n",
      "  --test_samples 1176\n",
      "\n",
      "# Step 2: Generate hallucination labels\n",
      "python scripts/preprocess/generate_labels.py \\\n",
      "  --llm_client \"groq\" \\\n",
      "  --llm_model \"llama-3.1-70b-versatile\"\n",
      "\n",
      "# Step 3: Filter datasets based on confidence\n",
      "python scripts/preprocess/filter.py \\\n",
      "  --use_confidence_threshold \\\n",
      "  --confidence_threshold 0.8\n",
      "```\n",
      "\n",
      "See [scripts/preprocess/README.md](scripts/preprocess/README.md) for complete documentation.\n",
      "\n",
      "---\n",
      "\n",
      "### Part 2: Training & Prediction\n",
      "\n",
      "This part computes interpretability scores (PKS and ECS) and trains/uses classifiers for hallucination detection.\n",
      "\n",
      "#### Option A: Direct Prediction (Using Pre-trained Models)\n",
      "\n",
      "Use this if you want to predict without training:\n",
      "\n",
      "```bash\n",
      "# Step 1: Compute PKS and ECS scores for test data\n",
      "python scripts/compute_scores.py \\\n",
      "    --input_path \"datasets/test/test1176_w_labels_filtered.jsonl\" \\\n",
      "    --output_dir \"datasets/test\" \\\n",
      "    --model_name \"Qwen/Qwen3-0.6B\" \\\n",
      "    --device \"cpu\"\n",
      "\n",
      "# Step 2: Run prediction using a pre-trained model\n",
      "python scripts/predict.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\" \\\n",
      "    --model_path \"trained_models/model_XGBoost_3000.pickle\" \\\n",
      "    --output_dir \"results\" \\\n",
      "    --save_predictions \\\n",
      "    --save_plots\n",
      "```\n",
      "\n",
      "#### Option B: Train Your Own Classifier\n",
      "\n",
      "Use this to train a new classifier on your data:\n",
      "\n",
      "```bash\n",
      "# Step 1: Compute scores for both training and test data\n",
      "python scripts/compute_scores.py \\\n",
      "    --input_path \"datasets/train/train3000_w_labels_filtered.jsonl\" \\\n",
      "    --output_dir \"datasets/train/chunk_scores\" \\\n",
      "    --model_name \"Qwen/Qwen3-0.6B\"\n",
      "\n",
      "python scripts/compute_scores.py \\\n",
      "    --input_path \"datasets/test/test1176_w_labels_filtered.jsonl\" \\\n",
      "    --output_dir \"datasets/test\" \\\n",
      "    --model_name \"Qwen/Qwen3-0.6B\"\n",
      "\n",
      "python scripts/classifier.py \\\n",
      "    --input_dir \"datasets/train/chunk_scores\" \\\n",
      "    --output_dir \"trained_models\" \\\n",
      "    --models \"LogisticRegression\" \"RandomForest\" \"SVC\" \"XGBoost\" \\\n",
      "    --test_size 0.2 \\\n",
      "    --balance_classes\n",
      "\n",
      "# Step 3: Run prediction with your trained model\n",
      "python scripts/predict.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\" \\\n",
      "    --model_path \"trained_models/model_XGBoost_3000.pickle\" \\\n",
      "    --output_dir \"results\" \\\n",
      "    --save_predictions \\\n",
      "    --save_plots\n",
      "```\n",
      "\n",
      "**Available Pre-trained Models:**\n",
      "- `model_LR_3000.pickle` - Logistic Regression\n",
      "- `model_RandomForest_3000.pickle` - Random Forest\n",
      "- `model_SVC_3000.pickle` - Support Vector Classifier\n",
      "- `model_XGBoost_3000.pickle` - XGBoost (recommended)\n",
      "\n",
      "---\n",
      "\n",
      "### Part 3: Baseline Comparisons\n",
      "\n",
      "Run various baseline methods to compare against your approach. Baselines require additional dependencies (see `scripts/baseline/requirements.txt`).\n",
      "\n",
      "**Available Baselines:**\n",
      "\n",
      "```bash\n",
      "# GPT Baseline\n",
      "python scripts/baseline/run_gpt.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\" \\\n",
      "    --models \"gpt-4o-mini\"\n",
      "\n",
      "# Groq Baseline (Llama models)\n",
      "python scripts/baseline/run_groq.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\" \\\n",
      "    --models \"llama3-70b-8192\"\n",
      "\n",
      "# HuggingFace Models Baseline\n",
      "python scripts/baseline/run_hf.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\" \\\n",
      "    --models \"Qwen/Qwen3-0.6B\"\n",
      "\n",
      "# RAGAS Baseline\n",
      "python scripts/baseline/run_ragas.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\"\n",
      "\n",
      "# RefChecker Baseline\n",
      "python scripts/baseline/run_refchecker.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\"\n",
      "\n",
      "# TruLens Baseline\n",
      "python scripts/baseline/run_trulens.py \\\n",
      "    --data_path \"datasets/test/test1176_w_chunk_score.json\"\n",
      "```\n",
      "\n",
      "**Install baseline dependencies:**\n",
      "```bash\n",
      "pip install -r scripts/baseline/requirements.txt\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Data Format\n",
      "\n",
      "### Required Dataset Format\n",
      "\n",
      "For Parts 2 and 3, your dataset must include:\n",
      "\n",
      "**Required columns:**\n",
      "- `prompt`: The input question/prompt\n",
      "- `prompt_spans`: Span information for the prompt\n",
      "- `response`: The model's generated response\n",
      "- `response_spans`: Span information for the response  \n",
      "- `labels`: List of hallucinated spans in the response\n",
      "\n",
      "**Example:**\n",
      "```json\n",
      "{\n",
      "  \"id\": \"finqa_123\",\n",
      "  \"question\": \"What is the revenue?\",\n",
      "  \"documents\": [\"Company revenue was $100M...\"],\n",
      "  \"prompt\": \"Given the context...\",\n",
      "  \"prompt_spans\": [[0, 150]],\n",
      "  \"response\": \"The revenue is $100M\",\n",
      "  \"response_spans\": [[0, 20]],\n",
      "  \"labels\": []\n",
      "}\n",
      "```\n",
      "\n",
      "### Output Files\n",
      "\n",
      "**After compute_scores.py:**\n",
      "- Chunk-level PKS (Parameter Knowledge Score) and ECS (Embedding Cosine Similarity) for each response chunk\n",
      "- JSON format with scores per chunk\n",
      "\n",
      "**After classifier.py:**\n",
      "- Trained model files (`.pickle` format)\n",
      "- Training metrics and plots\n",
      "\n",
      "**After predict.py:**\n",
      "- Predictions with confidence scores\n",
      "- Evaluation metrics (precision, recall, F1-score, AUC-ROC)\n",
      "- Confusion matrix and performance plots\n",
      "\n",
      "## Configuration\n",
      "\n",
      "### Environment Variables\n",
      "Create a `.env` file in the project root:\n",
      "\n",
      "```bash\n",
      "# Required API Keys\n",
      "OPENAI_API_KEY=your_openai_api_key_here\n",
      "GROQ_API_KEY=your_groq_api_key_here\n",
      "\n",
      "# Optional\n",
      "HUGGINGFACE_TOKEN=your_hf_token_here\n",
      "```\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this code or our work in your research, please cite our paper:\n",
      "\n",
      "```bibtex\n",
      "@inproceedings{tan2025interpdetect,\n",
      "  title={InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation},\n",
      "  author={Tan, Likun and Huang, Kuan-Wei and Shi, Joy and Wu, Kevin},\n",
      "  booktitle={OpenReview},\n",
      "  year={2025},\n",
      "  url={https://openreview.net/pdf?id=TZzBKwHLwF}\n",
      "}\n",
      "```\n",
      "\n",
      "## Contributing\n",
      "\n",
      "1. Fork the repository\n",
      "2. Create a feature branch\n",
      "3. Make your changes\n",
      "4. Add tests if applicable\n",
      "5. Update documentation\n",
      "6. Submit a pull request\n",
      "\n",
      "## License\n",
      "\n",
      "This project is licensed under the MIT License - see the LICENSE file for details.\n",
      "\n",
      "## Acknowledgments\n",
      "\n",
      "- Thanks to the open-source community for the various baseline implementations\n",
      "- Contributors to the interpretability research community, especially the TransformerLens team\n",
      "- The RAGBench team for providing the FinQA dataset\n",
      "- Users and testers of this framework\n"
     ]
    }
   ],
   "source": [
    "# Read the Code Walkthrough\n",
    "with open(os.path.join(repo_path, 'CodeWalkthrough.md'), 'r') as f:\n",
    "    walkthrough_content = f.read()\n",
    "print(walkthrough_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18896063",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIR] scripts/\n",
      "  predict.py\n",
      "  .DS_Store\n",
      "  compute_scores.py\n",
      "  classifier.py\n",
      "  [DIR] baseline/\n",
      "    run_refchecker.py\n",
      "    requirements.txt\n",
      "    run_hf.py\n",
      "    run_ragas.py\n",
      "    run_groq.py\n",
      "    run_trulens.py\n",
      "    run_gpt.py\n",
      "  [DIR] plots/\n",
      "    plot_correlation.ipynb\n",
      "  [DIR] preprocess/\n",
      "    generate_response_hf.py\n",
      "    preprocess.py\n",
      "    helper.py\n",
      "    filter.py\n",
      "    README.md\n",
      "    generate_response_gpt.py\n",
      "    generate_labels.py\n",
      "    [DIR] datasets/\n",
      "      [DIR] test/\n",
      "        test1176_w_response_gpt41mini.jsonl\n",
      "        test.jsonl\n",
      "        test1176_w_labels_filtered.jsonl\n",
      "        test1176_w_labels.jsonl\n",
      "        test1176_w_response.jsonl\n",
      "        test1176_w_labels_filtered_gpt41mini.jsonl\n",
      "        test1176_w_labels_gpt41mini.jsonl\n"
     ]
    }
   ],
   "source": [
    "# List the scripts directory\n",
    "scripts_path = os.path.join(repo_path, 'scripts')\n",
    "for root, dirs, files in os.walk(scripts_path):\n",
    "    level = root.replace(scripts_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}[DIR] {os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f'{subindent}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5db751c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIR] datasets/\n",
      "  OV_copying_score.json\n",
      "  [DIR] train/\n",
      "    train3000_w_chunk_score_part8.json\n",
      "    train3000_w_chunk_score_part12.json\n",
      "    train3000_w_chunk_score_part16.json\n",
      "    train3000_w_chunk_score_part0.json\n",
      "    train3000_w_chunk_score_part4.json\n",
      "    train3000_w_chunk_score_part1.json\n",
      "    train3000_w_chunk_score_part5.json\n",
      "    train3000_w_chunk_score_part13.json\n",
      "    train3000_w_chunk_score_part9.json\n",
      "    train3000_w_chunk_score_part17.json\n",
      "    train3000_w_chunk_score_part3.json\n",
      "    train3000_w_chunk_score_part7.json\n",
      "    train3000_w_chunk_score_part11.json\n",
      "    train3000_w_chunk_score_part15.json\n",
      "    train3000_w_chunk_score_part10.json\n",
      "    train3000_w_chunk_score_part14.json\n",
      "    train3000_w_chunk_score_part2.json\n",
      "    train3000_w_chunk_score_part6.json\n",
      "  [DIR] test/\n",
      "    test_w_chunk_score_qwen06b.json\n",
      "    test_w_chunk_score_gpt41mini.json\n"
     ]
    }
   ],
   "source": [
    "# List datasets directory\n",
    "datasets_path = os.path.join(repo_path, 'datasets')\n",
    "for root, dirs, files in os.walk(datasets_path):\n",
    "    level = root.replace(datasets_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}[DIR] {os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f'{subindent}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff6aff6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== compute_scores.py ===\n",
      "# %%\n",
      "#!pip install transformer_lens\n",
      "\n",
      "import torch\n",
      "from transformers import AutoTokenizer\n",
      "from transformer_lens import HookedTransformer\n",
      "import json\n",
      "from torch.nn import functional as F\n",
      "from typing import Dict, List, Tuple\n",
      "import pdb\n",
      "from sentence_transformers import SentenceTransformer\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import argparse\n",
      "import sys\n",
      "import os\n",
      "import gc\n",
      "from tqdm import tqdm\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import pointbiserialr\n",
      "\n",
      "def load_examples(file_path):\n",
      "    \"\"\"Load examples from JSONL file\"\"\"\n",
      "    print(f\"Loading examples from {file_path}...\")\n",
      "    \n",
      "    try:\n",
      "        examples = []\n",
      "        with open(file_path, 'r') as f:\n",
      "            for line in f:\n",
      "                data = json.loads(line)\n",
      "                examples.append(data)\n",
      "        \n",
      "        print(f\"Loaded {len(examples)} examples\")\n",
      "        return examples\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading examples: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def setup_models(model_name, hf_model_name, device=\"cuda\"):\n",
      "    \"\"\"Setup tokenizer, model, and sentence transformer\"\"\"\n",
      "    print(f\"Setting up models: {model_name}, {hf_model_name}\")\n",
      "    \n",
      "    try:\n",
      "        tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
      "        \n",
      "        model = HookedTransformer.from_pretrained(\n",
      "            model_name,\n",
      "            device=\"cpu\",\n",
      "            torch_dtype=torch.float16\n",
      "        )\n",
      "        model.to(device)\n",
      "        \n",
      "        bge_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\").to(device)\n",
      "        \n",
      "        return tokenizer, model, bge_model\n",
      "    except Exception as e:\n",
      "        print(f\"Error setting up models: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def calculate_dist_2d(sep_vocabulary_dist, sep_attention_dist):\n",
      "    \"\"\"Calculate Jensen-Shannon divergence between distributions\"\"\"\n",
      "    # Calculate softmax\n",
      "    softmax_mature_layer = F.softmax(sep_vocabulary_dist, dim=-1)\n",
      "    softmax_anchor_layer = F.softmax(sep_attention_dist, dim=-1)\n",
      "\n",
      "    # Calculate the average distribution M\n",
      "    M = 0.5 * (softmax_mature_layer + softmax_anchor_layer)\n",
      "\n",
      "    # Calculate log-softmax for the KL divergence\n",
      "    log_softmax_mature_layer = F.log_softmax(sep_vocabulary_dist, dim=-1)\n",
      "    log_softmax_anchor_layer = F.log_softmax(sep_attention_dist, dim=-1)\n",
      "\n",
      "    # Calculate the KL divergences and then the JS divergences\n",
      "    kl1 = F.kl_div(log_softmax_mature_layer, M, reduction='none').sum(dim=-1)\n",
      "    kl2 = F.kl_div(log_softmax_anchor_layer, M, reduction='none').sum(dim=-1)\n",
      "    js_divs = 0.5 * (kl1 + kl2)\n",
      "\n",
      "    scores = js_divs.cpu().tolist()\n",
      "    return sum(scores)\n",
      "\n",
      "def add_special_template(tokenizer, prompt):\n",
      "    \"\"\"Add special template to prompt\"\"\"\n",
      "    messages = [\n",
      "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "        {\"role\": \"user\", \"content\": prompt}\n",
      "    ]\n",
      "    text = tokenizer.apply_chat_template(\n",
      "        messages,\n",
      "        tokenize=False,\n",
      "        add_generation_prompt=True,\n",
      "    )\n",
      "    return text\n",
      "\n",
      "def is_hallucination_span(r_span, hallucination_spans):\n",
      "    \"\"\"Check if a span contains hallucination\"\"\"\n",
      "    for token_id in range(r_span[0], r_span[1]):\n",
      "        for span in hallucination_spans:\n",
      "            if token_id >= span[0] and token_id <= span[1]:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "def calculate_hallucination_spans(response, text, response_rag, tokenizer, prefix_len):\n",
      "    \"\"\"Calculate hallucination spans\"\"\"\n",
      "    hallucination_span = []\n",
      "    for item in response:\n",
      "        start_id = item['start']\n",
      "        end_id = item['end']\n",
      "        start_text = text + response_rag[:start_id]\n",
      "        end_text = text + response_rag[:end_id]\n",
      "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
      "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
      "        start_id = start_text_id.shape[-1]\n",
      "        end_id = end_text_id.shape[-1]\n",
      "        hallucination_span.append([start_id, end_id])\n",
      "    return hallucination_span\n",
      "\n",
      "def calculate_respond_spans(raw_response_spans, text, response_rag, tokenizer):\n",
      "    \"\"\"Calculate response spans\"\"\"\n",
      "    respond_spans = []\n",
      "    for item in raw_response_spans:\n",
      "        start_id = item[0]\n",
      "        end_id = item[1]\n",
      "        start_text = text + response_rag[:start_id]\n",
      "        end_text = text + response_rag[:end_id]\n",
      "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
      "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
      "        start_id = start_text_id.shape[-1]\n",
      "        end_id = end_text_id.shape[-1]\n",
      "        respond_spans.append([start_id, end_id])\n",
      "    return respond_spans\n",
      "\n",
      "def calculate_prompt_spans(raw_prompt_spans, prompt, tokenizer):\n",
      "    \"\"\"Calculate prompt spans\"\"\"\n",
      "    prompt_spans = []\n",
      "    for item in raw_prompt_spans:\n",
      "        start_id = item[0]\n",
      "        end_id = item[1]\n",
      "        start_text = prompt[:start_id]\n",
      "        end_text = prompt[:end_id]\n",
      "        added_start_text = add_special_template(tokenizer, start_text)\n",
      "        added_end_text = add_special_template(tokenizer, end_text)\n",
      "        start_text_id = tokenizer(added_start_text, return_tensors=\"pt\").i\n"
     ]
    }
   ],
   "source": [
    "# Read the main scripts to understand implementation\n",
    "# Start with compute_scores.py\n",
    "with open(os.path.join(scripts_path, 'compute_scores.py'), 'r') as f:\n",
    "    compute_scores_content = f.read()\n",
    "print(\"=== compute_scores.py ===\")\n",
    "print(compute_scores_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb5853e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nput_ids.shape[-1] - 4\n",
      "        end_text_id = tokenizer(added_end_text, return_tensors=\"pt\").input_ids.shape[-1] - 4\n",
      "        prompt_spans.append([start_text_id, end_text_id])\n",
      "    return prompt_spans\n",
      "\n",
      "def calculate_sentence_similarity(bge_model, r_text, p_text):\n",
      "    \"\"\"Calculate sentence similarity using BGE model\"\"\"\n",
      "    part_embedding = bge_model.encode([r_text], normalize_embeddings=True)\n",
      "    q_embeddings = bge_model.encode([p_text], normalize_embeddings=True)\n",
      "    \n",
      "    # Calculate similarity score\n",
      "    scores_named = np.matmul(q_embeddings, part_embedding.T).flatten()\n",
      "    return float(scores_named[0])\n",
      "\n",
      "class MockOutputs:\n",
      "    \"\"\"Mock outputs class for transformer lens compatibility\"\"\"\n",
      "    def __init__(self, cache, model_cfg):\n",
      "        self.cache = cache\n",
      "        self.model_cfg = model_cfg\n",
      "\n",
      "    @property\n",
      "    def attentions(self):\n",
      "        # Return attention patterns in the expected format\n",
      "        attentions = []\n",
      "        for layer in range(self.model_cfg.n_layers):\n",
      "            # Get attention pattern: [batch, n_heads, seq_len, seq_len]\n",
      "            attn_pattern = self.cache[f\"blocks.{layer}.attn.hook_pattern\"]\n",
      "            attentions.append(attn_pattern)\n",
      "        return tuple(attentions)\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        if key == \"hidden_states\":\n",
      "            # Return hidden states from all layers (residual stream after each layer)\n",
      "            hidden_states = []\n",
      "            for layer in range(self.model_cfg.n_layers):\n",
      "                hidden_state = self.cache[f\"blocks.{layer}.hook_resid_post\"]\n",
      "                hidden_states.append(hidden_state)\n",
      "            return tuple(hidden_states)\n",
      "        elif key == \"logits\":\n",
      "            return logits\n",
      "        else:\n",
      "            raise KeyError(f\"Key {key} not found\")\n",
      "\n",
      "def process_example(example, tokenizer, model, bge_model, device, max_ctx, iter_step=1):\n",
      "    \"\"\"Process a single example to compute scores\"\"\"\n",
      "    response_rag = example['response']\n",
      "    prompt = example['prompt']\n",
      "    original_prompt_spans = example['prompt_spans']\n",
      "    original_response_spans = example['response_spans']\n",
      "\n",
      "    text = add_special_template(tokenizer, prompt)\n",
      "\n",
      "    prompt_ids = tokenizer([text], return_tensors=\"pt\").input_ids\n",
      "    response_ids = tokenizer([response_rag], return_tensors=\"pt\").input_ids\n",
      "    input_ids = torch.cat([prompt_ids, response_ids[:, 1:]], dim=1)\n",
      "\n",
      "    if input_ids.shape[-1] > max_ctx:\n",
      "        overflow = input_ids.shape[-1] - max_ctx\n",
      "        input_ids = input_ids[:, overflow:]\n",
      "        prompt_kept = max(prompt_ids.shape[-1] - overflow, 0)\n",
      "    else:\n",
      "        prompt_kept = prompt_ids.shape[-1]\n",
      "\n",
      "    input_ids = input_ids.to(device)\n",
      "    prefix_len = prompt_kept\n",
      "\n",
      "    if \"labels\" in example.keys():\n",
      "        hallucination_spans = calculate_hallucination_spans(example['labels'], text, response_rag, tokenizer, prefix_len)\n",
      "    else:\n",
      "        hallucination_spans = []\n",
      "\n",
      "    prompt_spans = calculate_prompt_spans(example['prompt_spans'], prompt, tokenizer)\n",
      "    respond_spans = calculate_respond_spans(example['response_spans'], text, response_rag, tokenizer)\n",
      "\n",
      "    # Run model with cache to get all intermediate activations\n",
      "    logits, cache = model.run_with_cache(\n",
      "        input_ids,\n",
      "        return_type=\"logits\"\n",
      "    )\n",
      "\n",
      "    outputs = MockOutputs(cache, model.cfg)\n",
      "\n",
      "    # skip tokens without hallucination\n",
      "    hidden_states = outputs[\"hidden_states\"]\n",
      "    last_hidden_states = hidden_states[-1][0, :, :]\n",
      "    del hidden_states\n",
      "\n",
      "    span_score_dict = []\n",
      "    for r_id, r_span in enumerate(respond_spans):\n",
      "        layer_head_span = {}\n",
      "        parameter_knowledge_dict = {}\n",
      "        for attentions_layer_id in range(0, model.cfg.n_layers, iter_step):\n",
      "            for head_id in range(model.cfg.n_heads):\n",
      "                layer_head = (attentions_layer_id, head_id)\n",
      "                p_span_score_dict = []\n",
      "                for p_span in prompt_spans:\n",
      "                    attention_score = outputs.attentions[attentions_layer_id][0, head_id, :, :]\n",
      "                    p_span_score_dict.append([p_span, torch.sum(attention_score[r_span[0]:r_span[1], p_span[0]:p_span[1]]).cpu().item()])\n",
      "                \n",
      "                # Get the span with maximum score\n",
      "                p_id = max(range(len(p_span_score_dict)), key=lambda i: p_span_score_dict[i][1])\n",
      "                prompt_span_text = prompt[original_prompt_spans[p_id][0]:original_prompt_spans[p_id][1]]\n",
      "                respond_span_text = response_rag[original_response_spans[r_id][0]:original_response_spans[r_id][1]]\n",
      "                layer_head_span[str(layer_head)] = calculate_sentence_similarity(bge_model, prompt_span_text, respond_span_text)\n",
      "\n",
      "            x_mid = cache[f\"blocks.{attentions_layer_id}.hook_resid_mid\"][0, r_span[0]:r_span[1], :]\n",
      "            x_post = cache[f\"blocks.{attentions_layer_id}.hook_resid_post\"][0, r_span[0]:r_span[1], :]\n",
      "\n",
      "            score = calculate_dist_2d(\n",
      "                x_mid @ model.W_U,\n",
      "                x_post @ model.W_U\n",
      "            )\n",
      "            parameter_knowledge_dict[f\"layer_{attentions_layer_id}\"] = score\n",
      "\n",
      "        span_score_dict.append({\n",
      "            \"pro\n"
     ]
    }
   ],
   "source": [
    "# Continue reading compute_scores.py\n",
    "print(compute_scores_content[5000:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3178fe04",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== classifier.py ===\n",
      "# %%\n",
      "# !pip install feature_engine\n",
      "# !pip install xgboost\n",
      "# !pip install lightgbm\n",
      "# !pip install optuna\n",
      "# !pip install --upgrade scikit-learn\n",
      "# !pip install unidecode\n",
      "\n",
      "import pandas as pd\n",
      "import json\n",
      "import numpy as np\n",
      "import os\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "import pickle\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "from tqdm import tqdm\n",
      "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
      "import argparse\n",
      "import sys\n",
      "\n",
      "def load_data(folder_path):\n",
      "    \"\"\"Load data from JSON files in the specified folder\"\"\"\n",
      "    print(f\"Loading data from {folder_path}...\")\n",
      "    \n",
      "    try:\n",
      "        response = []\n",
      "        json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
      "        \n",
      "        if not json_files:\n",
      "            print(f\"No JSON files found in {folder_path}\")\n",
      "            sys.exit(1)\n",
      "        \n",
      "        for file_path in json_files:\n",
      "            with open(file_path, \"r\") as f:\n",
      "                data = json.load(f)\n",
      "                response.extend(data)\n",
      "        \n",
      "        print(f\"Loaded {len(response)} examples from {len(json_files)} files\")\n",
      "        return response\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading data: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def preprocess_data(response, balance_classes=True, random_state=42):\n",
      "    \"\"\"Preprocess the loaded data into a DataFrame\"\"\"\n",
      "    print(\"Preprocessing data...\")\n",
      "    \n",
      "    if not response:\n",
      "        print(\"No data to preprocess\")\n",
      "        sys.exit(1)\n",
      "    \n",
      "    # Get column names from first example\n",
      "    ATTENTION_COLS = response[0]['scores'][0]['prompt_attention_score'].keys()\n",
      "    PARAMETER_COLS = response[0]['scores'][0]['parameter_knowledge_scores'].keys()\n",
      "    \n",
      "    data_dict = {\n",
      "        \"identifier\": [],\n",
      "        **{col: [] for col in ATTENTION_COLS},\n",
      "        **{col: [] for col in PARAMETER_COLS},\n",
      "        \"hallucination_label\": []\n",
      "    }\n",
      "    \n",
      "    for i, resp in enumerate(response):\n",
      "        for j in range(len(resp[\"scores\"])):\n",
      "            data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
      "            for col in ATTENTION_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
      "            \n",
      "            for col in PARAMETER_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
      "            data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
      "    \n",
      "    df = pd.DataFrame(data_dict)\n",
      "    \n",
      "    print(f\"Created DataFrame with {len(df)} samples\")\n",
      "    print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")\n",
      "    \n",
      "    # Balance classes if requested\n",
      "    if balance_classes:\n",
      "        min_count = df['hallucination_label'].value_counts().min()\n",
      "        df = (\n",
      "            df.groupby('hallucination_label', group_keys=False)\n",
      "              .apply(lambda x: x.sample(min_count, random_state=random_state))\n",
      "        )\n",
      "        print(f\"After balancing: {df['hallucination_label'].value_counts().to_dict()}\")\n",
      "    \n",
      "    return df, list(ATTENTION_COLS), list(PARAMETER_COLS)\n",
      "\n",
      "def split_data(df, test_size=0.1, random_state=42):\n",
      "    \"\"\"Split data into train and validation sets\"\"\"\n",
      "    print(\"Splitting data into train and validation sets...\")\n",
      "    \n",
      "    train, val = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df['hallucination_label'])\n",
      "    \n",
      "    features = [col for col in df.columns if col not in ['identifier', 'hallucination_label']]\n",
      "    \n",
      "    X_train = train[features]\n",
      "    y_train = train[\"hallucination_label\"]\n",
      "    X_val = val[features]\n",
      "    y_val = val[\"hallucination_label\"]\n",
      "    \n",
      "    print(f\"Train set: {len(X_train)} samples\")\n",
      "    print(f\"Validation set: {len(X_val)} samples\")\n",
      "    print(f\"Number of features: {len(features)}\")\n",
      "    \n",
      "    return X_train, X_val, y_train, y_val, features\n",
      "\n",
      "def create_preprocessor(use_feature_selection=False):\n",
      "    \"\"\"Create preprocessing pipeline\"\"\"\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "    from feature_engine.selection import DropConstantFeatures, SmartCorrelatedSelection, DropDuplicateFeatures\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    from sklearn.pipeline import Pipeline\n",
      "    \n",
      "    scaler = StandardScaler()\n",
      "    \n",
      "    if use_feature_selection:\n",
      "        drop_const = DropConstantFeatures(tol=0.95, missing_values='ignore')\n",
      "        drop_dup = DropDuplicateFeatures()\n",
      "        drop_corr = SmartCorrelatedSelection(\n",
      "            method='pearson', \n",
      "            threshold=0.90,\n",
      "            selection_method='model_performance',\n",
      "            estimator=RandomForestClassifier(max_depth=5, random_state=42)\n",
      "        )\n",
      "        \n",
      "        preprocessor = Pipeline([\n",
      "            ('scaler', scaler),\n",
      "            ('drop_constant', drop_const),\n",
      "            ('drop_duplicates', drop_dup),\n",
      "            ('smart_corr_selection', drop_\n"
     ]
    }
   ],
   "source": [
    "# Read classifier.py\n",
    "with open(os.path.join(scripts_path, 'classifier.py'), 'r') as f:\n",
    "    classifier_content = f.read()\n",
    "print(\"=== classifier.py ===\")\n",
    "print(classifier_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918efe97",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr),\n",
      "        ])\n",
      "    else:\n",
      "        preprocessor = Pipeline([\n",
      "            ('scaler', scaler),\n",
      "        ])\n",
      "    \n",
      "    return preprocessor\n",
      "\n",
      "def train_models(X_train, X_val, y_train, y_val, preprocessor, models_to_train=None):\n",
      "    \"\"\"Train multiple models and compare their performance\"\"\"\n",
      "    print(\"Training models...\")\n",
      "    \n",
      "    from sklearn.pipeline import make_pipeline\n",
      "    from sklearn.metrics import precision_recall_fscore_support\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from xgboost import XGBClassifier\n",
      "    \n",
      "    # Define models to train\n",
      "    if models_to_train is None:\n",
      "        models_to_train = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
      "    \n",
      "    models = []\n",
      "    if \"LR\" in models_to_train:\n",
      "        models.append((\"LR\", LogisticRegression()))\n",
      "    if \"SVC\" in models_to_train:\n",
      "        models.append(('SVC', SVC()))\n",
      "    if \"RandomForest\" in models_to_train:\n",
      "        models.append(('RandomForest', RandomForestClassifier(max_depth=5)))\n",
      "    if \"XGBoost\" in models_to_train:\n",
      "        models.append(('XGBoost', XGBClassifier(max_depth=5)))\n",
      "    \n",
      "    # Initialize lists for results\n",
      "    names = []\n",
      "    train_ps = []\n",
      "    train_rs = []\n",
      "    train_fs = []\n",
      "    val_ps = []\n",
      "    val_rs = []\n",
      "    val_fs = []\n",
      "    clfs = {}\n",
      "    \n",
      "    # Train each model\n",
      "    for name, model in models:\n",
      "        print(f\"Training {name}...\")\n",
      "        names.append(name)\n",
      "        clf = make_pipeline(preprocessor, model)\n",
      "        clf.fit(X_train, y_train)\n",
      "        \n",
      "        # Calculate metrics\n",
      "        tp, tr, tf, _ = precision_recall_fscore_support(y_train, clf.predict(X_train), average='binary')\n",
      "        train_ps.append(tp)\n",
      "        train_rs.append(tr)\n",
      "        train_fs.append(tf)\n",
      "        \n",
      "        vp, vr, vf, _ = precision_recall_fscore_support(y_val, clf.predict(X_val), average='binary')\n",
      "        val_ps.append(vp)\n",
      "        val_rs.append(vr)\n",
      "        val_fs.append(vf)\n",
      "        \n",
      "        clfs[name] = clf\n",
      "    \n",
      "    # Create comparison dataframe\n",
      "    model_comparison = pd.DataFrame({\n",
      "        'Algorithm': names,\n",
      "        'Train_p': train_ps,\n",
      "        'Val_p': val_ps,\n",
      "        'Train_r': train_rs,\n",
      "        'Val_r': val_rs,\n",
      "        'Train_f': train_fs,\n",
      "        'Val_f': val_fs,\n",
      "    })\n",
      "    \n",
      "    print(\"\\nModel Comparison:\")\n",
      "    print(model_comparison)\n",
      "    \n",
      "    return clfs, model_comparison\n",
      "\n",
      "def save_models(clfs, output_dir):\n",
      "    \"\"\"Save trained models\"\"\"\n",
      "    print(f\"Saving models to {output_dir}...\")\n",
      "    \n",
      "    os.makedirs(output_dir, exist_ok=True)\n",
      "    \n",
      "    for name, clf in clfs.items():\n",
      "        model_path = os.path.join(output_dir, f\"model_{name}_3000.pickle\")\n",
      "        with open(model_path, \"wb\") as fout:\n",
      "            pickle.dump(clf, fout)\n",
      "        print(f\"Saved {name} model to {model_path}\")\n",
      "\n",
      "\n",
      "\n",
      "def create_feature_importance_plot(clfs, X_train, output_dir):\n",
      "    \"\"\"Create feature importance plot for XGBoost model\"\"\"\n",
      "    print(\"Creating feature importance plot...\")\n",
      "    \n",
      "    if 'XGBoost' in clfs:\n",
      "        xgb_model = clfs['XGBoost']\n",
      "        feature_imp = pd.DataFrame(\n",
      "            sorted(zip(xgb_model.named_steps['xgbclassifier'].feature_importances_, X_train.columns)), \n",
      "            columns=['Value', 'Feature']\n",
      "        )\n",
      "        \n",
      "        plt.figure(figsize=(8, 6))\n",
      "        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:15])\n",
      "        plt.title('XGBoost Feature Importance')\n",
      "        plt.tight_layout()\n",
      "        \n",
      "        plot_path = os.path.join(output_dir, \"feature_importance.png\")\n",
      "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "        plt.close()\n",
      "        print(f\"Saved feature importance plot to {plot_path}\")\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to run the classifier training pipeline\"\"\"\n",
      "    parser = argparse.ArgumentParser(description='Train classifiers for hallucination detection')\n",
      "    parser.add_argument('--input_dir', type=str, \n",
      "                       default=\"../datasets/train\",\n",
      "                       help='Input directory containing JSON files with scores')\n",
      "    parser.add_argument('--output_dir', type=str, \n",
      "                       default=\"../trained_models\",\n",
      "                       help='Output directory for trained models')\n",
      "    parser.add_argument('--models', nargs='+',\n",
      "                       default=[\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"],\n",
      "                       choices=[\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"],\n",
      "                       help='Models to train')\n",
      "    parser.add_argument('--test_size', type=float,\n",
      "                       default=0.1,\n",
      "                       help='Test size for train/validation split')\n",
      "    parser.add_argument('--balance_classes', action='store_true',\n",
      "                       help='Balance classes by undersampling')\n",
      "    parser.add_argument('--use_feature_selection', action='store_true',\n",
      "                       help='Use feature selection in preprocessing')\n",
      "\n",
      "    parser.add_argument('--random_state', type=int,\n",
      "                       default=42,\n",
      "                       help='Random state for reproducibility')\n",
      "    parser.add_\n"
     ]
    }
   ],
   "source": [
    "# Continue reading classifier.py\n",
    "print(classifier_content[5000:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b5f7233",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== predict.py ===\n",
      "# %%\n",
      "# !pip install feature_engine\n",
      "# !pip install xgboost\n",
      "# !pip install lightgbm\n",
      "# !pip install optuna\n",
      "# !pip install --upgrade scikit-learn\n",
      "# !pip install unidecode\n",
      "\n",
      "# %%\n",
      "import pandas as pd\n",
      "import json\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "import pickle\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "from tqdm import tqdm\n",
      "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
      "import argparse\n",
      "import sys\n",
      "import os\n",
      "\n",
      "def load_data(data_path):\n",
      "    \"\"\"Load data from JSON file\"\"\"\n",
      "    print(f\"Loading data from {data_path}...\")\n",
      "    \n",
      "    try:\n",
      "        with open(data_path, \"r\") as f:\n",
      "            response = json.load(f)\n",
      "        \n",
      "        print(f\"Loaded {len(response)} examples\")\n",
      "        return response\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading data: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def preprocess_data(response):\n",
      "    \"\"\"Preprocess the loaded data into a DataFrame\"\"\"\n",
      "    print(\"Preprocessing data...\")\n",
      "    \n",
      "    if not response:\n",
      "        print(\"No data to preprocess\")\n",
      "        sys.exit(1)\n",
      "    \n",
      "    # Get column names from first example\n",
      "    ATTENTION_COLS = response[0]['scores'][0]['prompt_attention_score'].keys()\n",
      "    PARAMETER_COLS = response[0]['scores'][0]['parameter_knowledge_scores'].keys()\n",
      "    \n",
      "    data_dict = {\n",
      "        \"identifier\": [],\n",
      "        **{col: [] for col in ATTENTION_COLS},\n",
      "        **{col: [] for col in PARAMETER_COLS},\n",
      "        \"hallucination_label\": []\n",
      "    }\n",
      "    \n",
      "    for i, resp in enumerate(response):\n",
      "        for j in range(len(resp[\"scores\"])):\n",
      "            data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
      "            for col in ATTENTION_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
      "            \n",
      "            for col in PARAMETER_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
      "            data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
      "    \n",
      "    df = pd.DataFrame(data_dict)\n",
      "    \n",
      "    print(f\"Created DataFrame with {len(df)} samples\")\n",
      "    print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")\n",
      "    \n",
      "    return df\n",
      "\n",
      "def load_model(model_path):\n",
      "    \"\"\"Load trained model from pickle file\"\"\"\n",
      "    print(f\"Loading model from {model_path}...\")\n",
      "    \n",
      "    try:\n",
      "        with open(model_path, \"rb\") as f:\n",
      "            model = pickle.load(f)\n",
      "        print(\"Model loaded successfully\")\n",
      "        return model\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading model: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def make_predictions(df, model):\n",
      "    \"\"\"Make predictions using the loaded model\"\"\"\n",
      "    print(\"Making predictions...\")\n",
      "    \n",
      "    features = [col for col in df.columns if col not in ['identifier', 'hallucination_label']]\n",
      "    y_pred = model.predict(df[features])\n",
      "    df['pred'] = y_pred\n",
      "    \n",
      "    print(f\"Predictions completed for {len(df)} samples\")\n",
      "    return df\n",
      "\n",
      "def evaluate_span_level(df):\n",
      "    \"\"\"Evaluate predictions at span level\"\"\"\n",
      "    print(\"\\n=== Span-level Evaluation ===\")\n",
      "    \n",
      "    # Confusion matrix: tn, fp, fn, tp\n",
      "    tn, fp, fn, tp = confusion_matrix(df[\"hallucination_label\"], df[\"pred\"]).ravel()\n",
      "    \n",
      "    # Precision, recall, F1\n",
      "    precision = precision_score(df[\"hallucination_label\"], df[\"pred\"])\n",
      "    recall = recall_score(df[\"hallucination_label\"], df[\"pred\"])\n",
      "    f1 = f1_score(df[\"hallucination_label\"], df[\"pred\"])\n",
      "    \n",
      "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
      "    print(f\"Precision: {precision:.3f}\")\n",
      "    print(f\"Recall: {recall:.3f}\")\n",
      "    print(f\"F1 Score: {f1:.3f}\")\n",
      "    \n",
      "    return {\n",
      "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
      "        'precision': precision, 'recall': recall, 'f1': f1\n",
      "    }\n",
      "\n",
      "def evaluate_response_level(df):\n",
      "    \"\"\"Evaluate predictions at response level\"\"\"\n",
      "    print(\"\\n=== Response-level Evaluation ===\")\n",
      "    \n",
      "    # Extract response_id from identifier (everything before \"_item_\")\n",
      "    df[\"response_id\"] = df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
      "    \n",
      "    # Group by response_id, aggregate with OR (max works for binary 0/1)\n",
      "    agg_df = df.groupby(\"response_id\").agg({\n",
      "        \"pred\": \"max\",\n",
      "        \"hallucination_label\": \"max\"\n",
      "    }).reset_index()\n",
      "    \n",
      "    # Confusion matrix: tn, fp, fn, tp\n",
      "    tn, fp, fn, tp = confusion_matrix(agg_df[\"hallucination_label\"], agg_df[\"pred\"]).ravel()\n",
      "    \n",
      "    # Precision, recall, F1\n",
      "    precision = precision_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
      "    recall = recall_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
      "    f1 = f1_score(agg_df[\"hallucination_label\"], agg_df[\"pred\"])\n",
      "    \n",
      "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
      "    print(f\"Precision: {precision:.4f}\")\n",
      "    print(f\"Recall: {recall:.4f}\")\n",
      "    print(f\"F1 Score: {f1:.4f}\")\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Read predict.py\n",
    "with open(os.path.join(scripts_path, 'predict.py'), 'r') as f:\n",
    "    predict_content = f.read()\n",
    "print(\"=== predict.py ===\")\n",
    "print(predict_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22eee10",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_RandomForest_3000.pickle\n",
      "model_LR_3000.pickle\n",
      "model_SVC_3000.pickle\n",
      "model_XGBoost_3000.pickle\n"
     ]
    }
   ],
   "source": [
    "# Check the trained models directory\n",
    "trained_models_path = os.path.join(repo_path, 'trained_models')\n",
    "for item in os.listdir(trained_models_path):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77359f5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plots_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(scripts_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_correlation.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(plots_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     plot_notebook \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print all cells\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(plot_notebook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcells\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the plot_correlation notebook to understand the correlation analysis\n",
    "plots_path = os.path.join(scripts_path, 'plots', 'plot_correlation.ipynb')\n",
    "with open(plots_path, 'r') as f:\n",
    "    plot_notebook = json.load(f)\n",
    "    \n",
    "# Print all cells\n",
    "for i, cell in enumerate(plot_notebook['cells']):\n",
    "    cell_type = cell['cell_type']\n",
    "    source = ''.join(cell['source'])\n",
    "    print(f\"=== Cell {i} ({cell_type}) ===\")\n",
    "    print(source[:2000] if len(source) > 2000 else source)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb9915e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cell 0 (code) ===\n",
      "import json\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "import json\n",
      "import collections\n",
      "from collections import Counter\n",
      "from typing import List\n",
      "from scipy.stats import pearsonr\n",
      "import ast\n",
      "import torch\n",
      "import glob\n",
      "import os\n",
      "\n",
      "=== Cell 1 (markdown) ===\n",
      "### Load file\n",
      "\n",
      "=== Cell 2 (code) ===\n",
      "folder_path = \"../../datasets/train\"\n",
      "\n",
      "examples = []\n",
      "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
      "\n",
      "for file_path in json_files:\n",
      "    with open(file_path, \"r\") as f:\n",
      "        data = json.load(f)\n",
      "        examples.extend(data)\n",
      "\n",
      "=== Cell 3 (code) ===\n",
      "examples[0]['scores'][1].keys()\n",
      "\n",
      "=== Cell 4 (markdown) ===\n",
      "### Create subsets of Truthful and Hallucinated\n",
      "\n",
      "=== Cell 5 (code) ===\n",
      "ecs_truthful = collections.defaultdict(list)\n",
      "ecs_hallucinated = collections.defaultdict(list)\n",
      "pks_truthful = collections.defaultdict(list)\n",
      "pks_hallucinated = collections.defaultdict(list)\n",
      "\n",
      "for example in examples:\n",
      "    for score in example['scores']:\n",
      "        if score['hallucination_label']==0:\n",
      "            for k, v in score['prompt_attention_score'].items():\n",
      "                ecs_truthful[k].append(v)\n",
      "            for k, v in score['parameter_knowledge_scores'].items():\n",
      "                pks_truthful[k].append(v)\n",
      "        else:\n",
      "            for k, v in score['prompt_attention_score'].items():\n",
      "                ecs_hallucinated[k].append(v)\n",
      "            for k, v in score['parameter_knowledge_scores'].items():\n",
      "                pks_hallucinated[k].append(v)\n",
      "\n",
      "        \n",
      "\n",
      "=== Cell 6 (markdown) ===\n",
      "### Compute quanities\n",
      "\n",
      "=== Cell 7 (code) ===\n",
      "def pearson_corr(\n",
      "    attention_scores: List[float],\n",
      "    hallucination_labels: List[int],\n",
      "    inverse=False\n",
      "):\n",
      "    if len(attention_scores) != len(hallucination_labels):\n",
      "        raise ValueError(\"Lists must have the same length.\")\n",
      "    \n",
      "    scores = np.array(attention_scores, dtype=float)\n",
      "    labels = np.array(hallucination_labels, dtype=int)\n",
      "\n",
      "    if not np.isin(labels, [0, 1]).all():\n",
      "        raise ValueError(\"Hallucination labels must be 0 or 1.\")\n",
      "\n",
      "    if inverse:\n",
      "\n",
      "        inverse_labels = 1 - labels  # invert 0<->1\n",
      "        \n",
      "        r, p_value = pearsonr(scores, inverse_labels)\n",
      "    else:\n",
      "        r, p_value = pearsonr(scores, labels)\n",
      "\n",
      "    return r, p_value\n",
      "\n",
      "\n",
      "def pearson_corr_float(x, y):\n",
      "    if len(x) != len(y):\n",
      "        raise ValueError(\"Lists must have the same length\")\n",
      "    if len(x) < 2:\n",
      "        raise ValueError(\"At least two data points are required\")\n",
      "    \n",
      "    corr, p_value = pearsonr(x, y)\n",
      "    return corr, p_value\n",
      "\n",
      "=== Cell 8 (code) ===\n",
      "# I. Compute Difference of ECS (Truth - Hallucination)\n",
      "ecs_diff = {}\n",
      "for k, v in ecs_truthful.items():\n",
      "    mean_truthful = np.mean(v)\n",
      "    mean_hallucinated = np.mean(ecs_hallucinated[k])\n",
      "    ecs_diff[k] = mean_truthful - mean_hallucinated\n",
      "\n",
      "# II. Compute Pearson's r: ECS vs. Inverse Hallucination Label\n",
      "ecs_lst = collections.defaultdict(list) \n",
      "ecs_label_lst = collections.defaultdict(list) \n",
      "for k, v in ecs_truthful.items():\n",
      "    for a in v:\n",
      "        ecs_lst[k].append(a)\n",
      "        ecs_label_lst[k].append(0)\n",
      "    for a in ecs_hallucinated[k]:\n",
      "        ecs_lst[k].append(a)\n",
      "        ecs_label_lst[k].append(1)\n",
      "\n",
      "ecs_pcc = {}\n",
      "for k, v in ecs_lst.items():\n",
      "    r, p_val = pearson_corr(v, ecs_label_lst[k], inverse=True)\n",
      "    ecs_pcc[k] = r\n",
      "\n",
      "# III. Load coping head score# Load tensor.json\n",
      "with open(\"../../datasets/OV_copying_score.json\", \"r\") as f:\n",
      "    data = json.load(f)  # This is now a nested Python list\n",
      "\n",
      "# Convert to tensor (optional)\n",
      "tensor = torch.tensor(data)\n",
      "\n",
      "# Build dictionary {(row,col): value}\n",
      "copying_dict = {f\"({i},{j})\": float(tensor[i, j]) \n",
      "               for i in range(tensor.size(0)) \n",
      "               for j in range(tensor.size(1))}\n",
      "\n",
      "# IV. Compute Difference of PKS (Truth - Hallucination)\n",
      "pks_diff = {}\n",
      "for k, v in pks_truthful.items():\n",
      "    mean_truthful = np.mean(v)\n",
      "    mean_hallucinated = np.mean(pks_hallucinated[k])\n",
      "    pks_diff[k] = mean_hallucinated - mean_truthful\n",
      "    \n",
      "\n",
      "# V. Compute Pearson's r: PKS vs. Hallucination Label\n",
      "pks_lst = collections.defaultdict(list) \n",
      "pks_label_lst = collections.defaultdict(list) \n",
      "for k, v in pks_truthful.items():\n",
      "    for a in v:\n",
      "        pks_lst[k].append(a)\n",
      "        pks_label_lst[k].append(0)\n",
      "    for a in pks_hallucinated[k]:\n",
      "        pks_lst[k].append(a)\n",
      "        pks_label_lst[k].append(1)\n",
      "\n",
      "pks_pcc = {}\n",
      "for k, v in pks_lst.items():\n",
      "    r, p_val = pearson_corr(v, pks_label_lst[k], inverse=False)\n",
      "    pks_pcc[k] = r\n",
      "\n",
      "\n",
      "=== Cell 9 (markdown) ===\n",
      "### Plot\n",
      "\n",
      "=== Cell 10 (markdown) ===\n",
      "### - External Content\n",
      "\n",
      "=== Cell 11 (code) ===\n",
      "# Parse keys to find layer and head ranges\n",
      "# pairs = [eval(k) for k in ecs_diff.keys()]  # converts '(0, 0)' → (0, 0)\n",
      "# layers = sorted(set(p[0] for p in pairs))\n",
      "# heads = sorted(set(p[1] for p in pairs))\n",
      "\n",
      "n_layers = 28 #max(layers) + 1\n",
      "n_heads = 16 #max(heads) + 1\n",
      "\n",
      "def plot_heatmap(ax, data_dict, heads=n_heads, layers=n_layers, title=\"Heatmap\", label=\"\"):\n",
      "    \n",
      "    # Convert dictionary to 2D numpy array\n",
      "    arr = np.zeros((heads, layers))\n",
      "    for k, value in data_dict.items():\n",
      "        kk = ast.literal_eval(k)\n",
      "        layer, head = kk[0], kk[1]\n",
      "        arr[head, layer] = value\n",
      "\n",
      "    # Plot heatmap\n",
      "    im = ax.imshow(arr, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
      "    plt.colorbar(im, label=title)\n",
      "    ax.set_title(title)\n",
      "    ax.set_xlabel(\"Layer\")\n",
      "    ax.set_ylabel(\"Head\")\n",
      "    ax.set_xticks(range(0, n_layers+1, 5), range(0, n_layers+1, 5))\n",
      "    ax.set_yticks(range(0, n_heads+1, 5), range(0, n_heads+1, 5))\n",
      "\n",
      "    # Add subplot label at the bottom center\n",
      "    if label is not None:\n",
      "        ax.text(0.5, -0.15, label, transform=ax.transAxes,\n",
      "                fontsize=12, fontweight=\"bold\", va=\"top\", ha=\"center\")\n",
      "\n",
      "    return im  # return image object for shared colorbar if needed\n",
      "\n",
      "# Create 1 row with 3 columns\n",
      "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
      "\n",
      "# Plot each dictionary\n",
      "plot_heatmap(axes[0], ecs_diff, n_heads, n_layers, \"Difference in ECS (Truth - Hallucination)\", \"(a)\")\n",
      "plot_heatmap(axes[1], ecs_pcc, n_heads, n_layers, \"Pearson's r: ECS vs. Inverse Hallucination Label\", \"(b)\")\n",
      "plot_heatmap(axes[2], copying_dict, n_heads, n_layers, \"Copying Heads Score\", \"(c)\")\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "=== Cell 12 (code) ===\n",
      "r1, _ = pearson_corr_float(list(ecs_diff.values()), list(copying_dict.values()))\n",
      "r2, _ = pearson_corr_float(list(ecs_pcc.values()), list(copying_dict.values()))\n",
      "\n",
      "print(r1, r2)\n",
      "\n",
      "=== Cell 13 (markdown) ===\n",
      "### - Parametric Knowledge\n",
      "\n",
      "=== Cell 14 (code) ===\n",
      "def plot_1d(ax, data_dict, heads=n_heads, layers=n_layers, ylabel=\"y\", title=\"1d plot\", label=\"\"):\n",
      "    \n",
      "    # Convert dictionary to 2D numpy array\n",
      "    layers = [int(k.split('_')[1]) for k in data_dict.keys()]\n",
      "    values = list(data_dict.values())\n",
      "\n",
      "    # Plot 1d\n",
      "    ax.bar(layers, values, width=0.6, align='center')\n",
      "    ax.set_title(title)\n",
      "    ax.set_xlabel(\"Layer\")\n",
      "    ax.set_ylabel(ylabel)\n",
      "    ax.set_xticks(range(0, n_layers+1, 5), range(0, n_layers+1, 5))\n",
      "    #ax.set_yticks(range(0, n_heads+1, 5), range(0, n_heads+1, 5))\n",
      "\n",
      "    # Add subplot label at the bottom center\n",
      "    if label is not None:\n",
      "        ax.text(0.5, -0.15, label, transform=ax.transAxes,\n",
      "                fontsize=12, fontweight=\"bold\", va=\"top\", ha=\"center\")\n",
      "\n",
      "# Create 1 row with 3 columns\n",
      "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
      "\n",
      "# Plot each dictionary\n",
      "plot_1d(axes[0], pks_diff, n_heads, n_layers, \"Difference in PKS\", \"Difference in PKS (Hallucination - Truth)\", \"(a)\")\n",
      "plot_1d(axes[1], pks_pcc, n_heads, n_layers, \"Pearson Correlation\", \"Pearson's r: PKS vs. Hallucination Label\", \"(b)\")\n",
      "\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "=== Cell 15 (code) ===\n",
      "sorted_ecs_pcc = dict(sorted(ecs_pcc.items(), key=lambda x: x[1], reverse=True))\n",
      "\n",
      "cnt = 0\n",
      "for k, v in sorted_ecs_pcc.items():\n",
      "    print(k, v)\n",
      "    cnt += 1\n",
      "    if cnt==10:\n",
      "        break\n",
      "\n",
      "=== Cell 16 (code) ===\n",
      "sorted_pks_pcc = dict(sorted(pks_pcc.items(), key=lambda x: x[1], reverse=True))\n",
      "\n",
      "cnt = 0\n",
      "for k, v in sorted_pks_pcc.items():\n",
      "    print(k, v)\n",
      "    cnt += 1\n",
      "    if cnt==10:\n",
      "        break\n",
      "\n",
      "=== Cell 17 (code) ===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Check the plot_correlation notebook to understand the correlation analysis\n",
    "plots_path = os.path.join(scripts_path, 'plots', 'plot_correlation.ipynb')\n",
    "with open(plots_path, 'r') as f:\n",
    "    plot_notebook = json.load(f)\n",
    "    \n",
    "# Print all cells\n",
    "for i, cell in enumerate(plot_notebook['cells']):\n",
    "    cell_type = cell['cell_type']\n",
    "    source = ''.join(cell['source'])\n",
    "    print(f\"=== Cell {i} ({cell_type}) ===\")\n",
    "    print(source[:2000] if len(source) > 2000 else source)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ef4726",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scripts: ['run_refchecker.py', 'requirements.txt', 'run_hf.py', 'run_ragas.py', 'run_groq.py', 'run_trulens.py', 'run_gpt.py']\n",
      "\n",
      "Preprocess scripts: ['generate_response_hf.py', 'preprocess.py', 'helper.py', 'filter.py', 'datasets', 'README.md', 'generate_response_gpt.py', 'generate_labels.py']\n"
     ]
    }
   ],
   "source": [
    "# Check baseline scripts to see if they exist\n",
    "baseline_path = os.path.join(scripts_path, 'baseline')\n",
    "baseline_files = os.listdir(baseline_path)\n",
    "print(\"Baseline scripts:\", baseline_files)\n",
    "\n",
    "# Read preprocess folder structure\n",
    "preprocess_path = os.path.join(scripts_path, 'preprocess')\n",
    "preprocess_files = os.listdir(preprocess_path)\n",
    "print(\"\\nPreprocess scripts:\", preprocess_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f413ea",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Correlation Analysis: ECS vs Hallucination\n",
      "- Main result: All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\n",
      "\n",
      "### Correlation Analysis: PKS vs Hallucination\n",
      "- Main result: Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\n",
      "\n",
      "### Classifier Training and Selection\n",
      "- Main result: SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\n",
      "\n",
      "### Self-Evaluation Detection\n",
      "- Main result: Method achieved F1=74.68%, outperforming TruLens (67.32%) and llama-3.1-8b-instant (57.53%), comparable to RefChecker (75.86%).\n",
      "\n",
      "### Proxy-Based Evaluation Detection\n",
      "- Main result: Method achieved F1=75.36%, outperforming nearly all models except GPT-5 (76.92%) and RAGAS (76.19%), using only 0.6b parameter signals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's systematically verify CS1: Conclusions vs Results\n",
    "# The plan states specific results that we need to verify against actual implementation/data\n",
    "\n",
    "# First, let's understand the claims from the plan:\n",
    "plan_claims = \"\"\"\n",
    "### Correlation Analysis: ECS vs Hallucination\n",
    "- Main result: All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\n",
    "\n",
    "### Correlation Analysis: PKS vs Hallucination\n",
    "- Main result: Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\n",
    "\n",
    "### Classifier Training and Selection\n",
    "- Main result: SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\n",
    "\n",
    "### Self-Evaluation Detection\n",
    "- Main result: Method achieved F1=74.68%, outperforming TruLens (67.32%) and llama-3.1-8b-instant (57.53%), comparable to RefChecker (75.86%).\n",
    "\n",
    "### Proxy-Based Evaluation Detection\n",
    "- Main result: Method achieved F1=75.36%, outperforming nearly all models except GPT-5 (76.92%) and RAGAS (76.19%), using only 0.6b parameter signals.\n",
    "\"\"\"\n",
    "print(plan_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd49087",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1800 examples from 18 files\n"
     ]
    }
   ],
   "source": [
    "# Let's verify CS1 by running the correlation analysis using the training data\n",
    "# Load training data to compute ECS and PKS correlations\n",
    "\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load training data\n",
    "folder_path = os.path.join(repo_path, \"datasets/train\")\n",
    "\n",
    "examples = []\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        examples.extend(data)\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples from {len(json_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a534552",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attention heads: 448\n",
      "Number of FFN layers: 28\n",
      "Truthful spans: 4406\n",
      "Hallucinated spans: 3393\n"
     ]
    }
   ],
   "source": [
    "# Verify Claim 1: ECS vs Hallucination - All attention heads exhibit negative correlations\n",
    "ecs_truthful = collections.defaultdict(list)\n",
    "ecs_hallucinated = collections.defaultdict(list)\n",
    "pks_truthful = collections.defaultdict(list)\n",
    "pks_hallucinated = collections.defaultdict(list)\n",
    "\n",
    "for example in examples:\n",
    "    for score in example['scores']:\n",
    "        if score['hallucination_label']==0:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_truthful[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_truthful[k].append(v)\n",
    "        else:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_hallucinated[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_hallucinated[k].append(v)\n",
    "\n",
    "print(f\"Number of attention heads: {len(ecs_truthful)}\")\n",
    "print(f\"Number of FFN layers: {len(pks_truthful)}\")\n",
    "print(f\"Truthful spans: {len(list(ecs_truthful.values())[0])}\")\n",
    "print(f\"Hallucinated spans: {len(list(ecs_hallucinated.values())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb3d22bd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECS Correlation Analysis (ECS vs Inverse Hallucination Label):\n",
      "  Positive correlations: 448\n",
      "  Negative correlations: 0\n",
      "  Zero correlations: 0\n",
      "  Total heads: 448\n",
      "\n",
      "Claim: 'All attention heads exhibit negative correlations'\n",
      "Result: MATCHES the claim\n",
      "        (Note: The claim says negative correlation between ECS and hallucination, which means positive correlation with inverse hallucination label)\n"
     ]
    }
   ],
   "source": [
    "# Compute Pearson's r: ECS vs. Inverse Hallucination Label\n",
    "def pearson_corr(attention_scores, hallucination_labels, inverse=False):\n",
    "    scores = np.array(attention_scores, dtype=float)\n",
    "    labels = np.array(hallucination_labels, dtype=int)\n",
    "    \n",
    "    if inverse:\n",
    "        inverse_labels = 1 - labels  # invert 0<->1\n",
    "        r, p_value = pearsonr(scores, inverse_labels)\n",
    "    else:\n",
    "        r, p_value = pearsonr(scores, labels)\n",
    "    \n",
    "    return r, p_value\n",
    "\n",
    "# Aggregate ECS data for correlation\n",
    "ecs_lst = collections.defaultdict(list) \n",
    "ecs_label_lst = collections.defaultdict(list) \n",
    "for k, v in ecs_truthful.items():\n",
    "    for a in v:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(0)\n",
    "    for a in ecs_hallucinated[k]:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(1)\n",
    "\n",
    "# Compute ECS correlations\n",
    "ecs_pcc = {}\n",
    "for k, v in ecs_lst.items():\n",
    "    r, p_val = pearson_corr(v, ecs_label_lst[k], inverse=True)\n",
    "    ecs_pcc[k] = r\n",
    "\n",
    "# Count positive and negative correlations\n",
    "positive_corr = sum(1 for v in ecs_pcc.values() if v > 0)\n",
    "negative_corr = sum(1 for v in ecs_pcc.values() if v < 0)\n",
    "zero_corr = sum(1 for v in ecs_pcc.values() if v == 0)\n",
    "\n",
    "print(f\"ECS Correlation Analysis (ECS vs Inverse Hallucination Label):\")\n",
    "print(f\"  Positive correlations: {positive_corr}\")\n",
    "print(f\"  Negative correlations: {negative_corr}\")\n",
    "print(f\"  Zero correlations: {zero_corr}\")\n",
    "print(f\"  Total heads: {len(ecs_pcc)}\")\n",
    "print(f\"\\nClaim: 'All attention heads exhibit negative correlations'\")\n",
    "print(f\"Result: {'MATCHES' if positive_corr == len(ecs_pcc) else 'DOES NOT MATCH'} the claim\")\n",
    "print(f\"        (Note: The claim says negative correlation between ECS and hallucination, which means positive correlation with inverse hallucination label)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e07faa1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECS Correlation Analysis (ECS vs Hallucination Label directly):\n",
      "  Positive correlations: 0\n",
      "  Negative correlations: 448\n",
      "\n",
      "Claim: 'All attention heads exhibit negative correlations [between ECS and hallucination]'\n",
      "Result: MATCHES the claim\n",
      "\n",
      "Most negative correlations (top 5):\n",
      "  (5, 10): -0.3378\n",
      "  (8, 12): -0.3371\n",
      "  (5, 13): -0.3360\n",
      "  (4, 1): -0.3283\n",
      "  (14, 9): -0.3278\n",
      "\n",
      "Least negative correlations (bottom 5):\n",
      "  (11, 1): -0.0880\n",
      "  (10, 6): -0.0693\n",
      "  (11, 11): -0.0441\n",
      "  (2, 7): -0.0190\n",
      "  (19, 10): -0.0102\n"
     ]
    }
   ],
   "source": [
    "# Let me verify this more directly - compute ECS vs Hallucination (not inverse)\n",
    "ecs_pcc_direct = {}\n",
    "for k, v in ecs_lst.items():\n",
    "    r, p_val = pearson_corr(v, ecs_label_lst[k], inverse=False)\n",
    "    ecs_pcc_direct[k] = r\n",
    "\n",
    "# Count positive and negative correlations\n",
    "positive_corr_direct = sum(1 for v in ecs_pcc_direct.values() if v > 0)\n",
    "negative_corr_direct = sum(1 for v in ecs_pcc_direct.values() if v < 0)\n",
    "\n",
    "print(f\"ECS Correlation Analysis (ECS vs Hallucination Label directly):\")\n",
    "print(f\"  Positive correlations: {positive_corr_direct}\")\n",
    "print(f\"  Negative correlations: {negative_corr_direct}\")\n",
    "print(f\"\\nClaim: 'All attention heads exhibit negative correlations [between ECS and hallucination]'\")\n",
    "print(f\"Result: {'MATCHES' if negative_corr_direct == len(ecs_pcc_direct) else 'DOES NOT MATCH'} the claim\")\n",
    "\n",
    "# Print some example correlations\n",
    "sorted_ecs = sorted(ecs_pcc_direct.items(), key=lambda x: x[1])\n",
    "print(\"\\nMost negative correlations (top 5):\")\n",
    "for k, v in sorted_ecs[:5]:\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "print(\"\\nLeast negative correlations (bottom 5):\")\n",
    "for k, v in sorted_ecs[-5:]:\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e169f98",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKS Correlation Analysis (PKS vs Hallucination Label):\n",
      "  Positive correlations: 26\n",
      "  Negative correlations: 2\n",
      "  Total layers: 28\n",
      "\n",
      "PKS correlations by layer:\n",
      "  Layer  0: +0.0144\n",
      "  Layer  1: +0.0459\n",
      "  Layer  2: +0.0477\n",
      "  Layer  3: +0.0264\n",
      "  Layer  4: -0.0123\n",
      "  Layer  5: +0.0155\n",
      "  Layer  6: +0.0391\n",
      "  Layer  7: +0.0770\n",
      "  Layer  8: +0.1354\n",
      "  Layer  9: +0.1417\n",
      "  Layer 10: +0.1530\n",
      "  Layer 11: +0.1267\n",
      "  Layer 12: +0.1524\n",
      "  Layer 13: +0.0945\n",
      "  Layer 14: +0.1229\n",
      "  Layer 15: +0.1659\n",
      "  Layer 16: +0.1340\n",
      "  Layer 17: +0.1632\n",
      "  Layer 18: +0.2578\n",
      "  Layer 19: +0.2215\n",
      "  Layer 20: +0.2639\n",
      "  Layer 21: +0.3210\n",
      "  Layer 22: +0.1475\n",
      "  Layer 23: +0.3243\n",
      "  Layer 24: +0.3246\n",
      "  Layer 25: +0.3033\n",
      "  Layer 26: +0.2294\n",
      "  Layer 27: -0.0102\n",
      "\n",
      "Mean correlation - Early layers (0-13): 0.0755\n",
      "Mean correlation - Later layers (14-27): 0.2121\n",
      "\n",
      "Claim: 'Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated'\n",
      "Result: MATCHES the claim\n"
     ]
    }
   ],
   "source": [
    "# Verify Claim 2: PKS vs Hallucination - Later-layer FFNs exhibit higher PKS for hallucinated responses\n",
    "# and are positively correlated with hallucinations\n",
    "\n",
    "# Aggregate PKS data for correlation\n",
    "pks_lst = collections.defaultdict(list) \n",
    "pks_label_lst = collections.defaultdict(list) \n",
    "for k, v in pks_truthful.items():\n",
    "    for a in v:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(0)\n",
    "    for a in pks_hallucinated[k]:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(1)\n",
    "\n",
    "# Compute PKS correlations\n",
    "pks_pcc = {}\n",
    "for k, v in pks_lst.items():\n",
    "    r, p_val = pearson_corr(v, pks_label_lst[k], inverse=False)\n",
    "    pks_pcc[k] = r\n",
    "\n",
    "# Count positive and negative correlations\n",
    "positive_pks = sum(1 for v in pks_pcc.values() if v > 0)\n",
    "negative_pks = sum(1 for v in pks_pcc.values() if v < 0)\n",
    "\n",
    "print(f\"PKS Correlation Analysis (PKS vs Hallucination Label):\")\n",
    "print(f\"  Positive correlations: {positive_pks}\")\n",
    "print(f\"  Negative correlations: {negative_pks}\")\n",
    "print(f\"  Total layers: {len(pks_pcc)}\")\n",
    "\n",
    "# Sort by layer number and show correlations\n",
    "sorted_pks = sorted(pks_pcc.items(), key=lambda x: int(x[0].split('_')[1]))\n",
    "print(\"\\nPKS correlations by layer:\")\n",
    "for k, v in sorted_pks:\n",
    "    layer_num = int(k.split('_')[1])\n",
    "    sign = \"+\" if v > 0 else \"\"\n",
    "    print(f\"  Layer {layer_num:2d}: {sign}{v:.4f}\")\n",
    "\n",
    "# Check if later layers have higher positive correlations\n",
    "early_layers = [v for k, v in sorted_pks[:14]]  # layers 0-13\n",
    "later_layers = [v for k, v in sorted_pks[14:]]  # layers 14-27\n",
    "\n",
    "print(f\"\\nMean correlation - Early layers (0-13): {np.mean(early_layers):.4f}\")\n",
    "print(f\"Mean correlation - Later layers (14-27): {np.mean(later_layers):.4f}\")\n",
    "print(f\"\\nClaim: 'Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated'\")\n",
    "print(f\"Result: {'MATCHES' if np.mean(later_layers) > np.mean(early_layers) else 'DOES NOT MATCH'} the claim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7501c0b5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256 test examples\n"
     ]
    }
   ],
   "source": [
    "# Verify Claim 3: Classifier Training - SVC achieved highest validation F1 (76.60%)\n",
    "# Let's load the trained models and verify performance on test data\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load the test data with chunk scores\n",
    "test_data_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_response = json.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(test_response)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ab0df5b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 7799 samples\n",
      "Class distribution: {0: 4406, 1: 3393}\n"
     ]
    }
   ],
   "source": [
    "# Load training data for validation split to verify the classifier claim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "# Use the existing training data we loaded earlier to verify classifier performance\n",
    "# Preprocess data into dataframe\n",
    "ATTENTION_COLS = list(examples[0]['scores'][0]['prompt_attention_score'].keys())\n",
    "PARAMETER_COLS = list(examples[0]['scores'][0]['parameter_knowledge_scores'].keys())\n",
    "\n",
    "data_dict = {\n",
    "    \"identifier\": [],\n",
    "    **{col: [] for col in ATTENTION_COLS},\n",
    "    **{col: [] for col in PARAMETER_COLS},\n",
    "    \"hallucination_label\": []\n",
    "}\n",
    "\n",
    "for i, resp in enumerate(examples):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        \n",
    "        for col in PARAMETER_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(f\"Created DataFrame with {len(df)} samples\")\n",
    "print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8317eded",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: {0: 3393, 1: 3393}\n",
      "Train set: 6107 samples\n",
      "Validation set: 679 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3041302/45736592.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min_count, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Balance classes and split data (as done in classifier.py)\n",
    "min_count = df['hallucination_label'].value_counts().min()\n",
    "df_balanced = (\n",
    "    df.groupby('hallucination_label', group_keys=False)\n",
    "      .apply(lambda x: x.sample(min_count, random_state=42))\n",
    ")\n",
    "print(f\"After balancing: {df_balanced['hallucination_label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Split data\n",
    "train, val = train_test_split(df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['hallucination_label'])\n",
    "\n",
    "features = [col for col in df_balanced.columns if col not in ['identifier', 'hallucination_label']]\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[\"hallucination_label\"]\n",
    "X_val = val[features]\n",
    "y_val = val[\"hallucination_label\"]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a6fc237",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:\n",
      "  Precision: 0.7744 (77.44%)\n",
      "  Recall: 0.7493 (74.93%)\n",
      "  F1: 0.7616 (76.16%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:\n",
      "  Precision: 0.8258 (82.58%)\n",
      "  Recall: 0.7552 (75.52%)\n",
      "  F1: 0.7889 (78.89%)\n",
      "\n",
      "RandomForest:\n",
      "  Precision: 0.7792 (77.92%)\n",
      "  Recall: 0.7286 (72.86%)\n",
      "  F1: 0.7530 (75.30%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "  Precision: 0.9373 (93.73%)\n",
      "  Recall: 0.9705 (97.05%)\n",
      "  F1: 0.9536 (95.36%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3041302/2637166208.py:10: UserWarning: [22:51:18] WARNING: /workspace/src/collective/../data/../common/error_msg.h:83: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  model = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the pre-trained models\n",
    "models_path = os.path.join(repo_path, \"trained_models\")\n",
    "\n",
    "model_names = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
    "model_results = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model_file = os.path.join(models_path, f\"model_{name}_3000.pickle\")\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"  Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  F1: {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b243218",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Performance Comparison:\n",
      "============================================================\n",
      "Model           Val F1          Claimed F1     \n",
      "============================================================\n",
      "LR              76.16%          \n",
      "SVC             78.89%       76.60% (claimed)\n",
      "RandomForest    75.30%          \n",
      "XGBoost         95.36%          \n",
      "============================================================\n",
      "\n",
      "Best validation F1: XGBoost with 95.36%\n",
      "\n",
      "Claim: 'SVC achieved highest validation F1 (76.60%)'\n",
      "Actual SVC F1: 78.89%\n",
      "Difference: 2.29%\n",
      "\n",
      "Note: XGBoost shows 95.36% F1, suggesting overfitting as claimed in plan\n"
     ]
    }
   ],
   "source": [
    "# The results show that SVC has F1=78.89%, which is close to the claimed 76.60%\n",
    "# The XGBoost has extremely high validation performance (95.36%), which suggests potential overfitting\n",
    "# as mentioned in the plan\n",
    "\n",
    "# Let's verify by comparing training and validation performance\n",
    "print(\"Classifier Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<15} {'Val F1':<15} {'Claimed F1':<15}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'LR':<15} {model_results['LR']['f1']*100:.2f}%{'':<10}\")\n",
    "print(f\"{'SVC':<15} {model_results['SVC']['f1']*100:.2f}%{'':<6} 76.60% (claimed)\")\n",
    "print(f\"{'RandomForest':<15} {model_results['RandomForest']['f1']*100:.2f}%{'':<10}\")\n",
    "print(f\"{'XGBoost':<15} {model_results['XGBoost']['f1']*100:.2f}%{'':<10}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find the best model\n",
    "best_model = max(model_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"\\nBest validation F1: {best_model[0]} with {best_model[1]['f1']*100:.2f}%\")\n",
    "\n",
    "# Verify the claim about SVC\n",
    "svc_f1 = model_results['SVC']['f1'] * 100\n",
    "claimed_f1 = 76.60\n",
    "print(f\"\\nClaim: 'SVC achieved highest validation F1 (76.60%)'\")\n",
    "print(f\"Actual SVC F1: {svc_f1:.2f}%\")\n",
    "print(f\"Difference: {abs(svc_f1 - claimed_f1):.2f}%\")\n",
    "\n",
    "# The actual highest is XGBoost but it's likely overfitting as mentioned\n",
    "print(f\"\\nNote: XGBoost shows {model_results['XGBoost']['f1']*100:.2f}% F1, suggesting overfitting as claimed in plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fdbf0b6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vs Validation F1 Comparison:\n",
      "============================================================\n",
      "Model           Train F1        Val F1          Gap       \n",
      "============================================================\n",
      "LR              77.84%          76.16%          +1.68%\n",
      "SVC             80.95%          78.89%          +2.06%\n",
      "RandomForest    77.25%          75.30%          +1.95%\n",
      "XGBoost         95.02%          95.36%          -0.34%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The claim says \"SVC achieved highest validation F1\" but XGBoost has higher validation F1\n",
    "# However, the claim also mentions XGBoost overfitting - let's verify by checking training performance\n",
    "# Let's compute training F1 to see if XGBoost overfits\n",
    "\n",
    "model_train_results = {}\n",
    "for name in model_names:\n",
    "    model_file = os.path.join(models_path, f\"model_{name}_3000.pickle\")\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Make predictions on training set\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "    \n",
    "    model_train_results[name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"Training vs Validation F1 Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<15} {'Train F1':<15} {'Val F1':<15} {'Gap':<10}\")\n",
    "print(\"=\"*60)\n",
    "for name in model_names:\n",
    "    train_f1 = model_train_results[name]['f1'] * 100\n",
    "    val_f1 = model_results[name]['f1'] * 100\n",
    "    gap = train_f1 - val_f1\n",
    "    print(f\"{name:<15} {train_f1:.2f}%{'':<9} {val_f1:.2f}%{'':<9} {gap:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43d684d0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame: 975 samples\n",
      "Class distribution: {0: 699, 1: 276}\n"
     ]
    }
   ],
   "source": [
    "# Interesting - XGBoost doesn't show overfitting in the traditional sense (train >> val)\n",
    "# But still shows very high performance on both. The claim about SVC being highest is NOT accurate\n",
    "# based on our validation - XGBoost has higher F1.\n",
    "\n",
    "# The issue is the claim says \"SVC achieved highest validation F1\" but XGBoost is higher.\n",
    "# However, the claim also says \"XGBoost overfitted despite strong training performance\"\n",
    "# The XGBoost Train F1 (95.02%) and Val F1 (95.36%) are suspiciously high\n",
    "\n",
    "# Let's verify claims 4 and 5 about Self-Evaluation and Proxy-Based Detection\n",
    "# We need to check if response-level F1 matches the claims\n",
    "\n",
    "# Load SVC model and compute response-level metrics on test data\n",
    "test_data_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_response = json.load(f)\n",
    "\n",
    "# Preprocess test data\n",
    "test_data_dict = {\n",
    "    \"identifier\": [],\n",
    "    **{col: [] for col in ATTENTION_COLS},\n",
    "    **{col: [] for col in PARAMETER_COLS},\n",
    "    \"hallucination_label\": []\n",
    "}\n",
    "\n",
    "for i, resp in enumerate(test_response):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        \n",
    "        for col in PARAMETER_COLS:\n",
    "            test_data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_df = pd.DataFrame(test_data_dict)\n",
    "print(f\"Test DataFrame: {len(test_df)} samples\")\n",
    "print(f\"Class distribution: {test_df['hallucination_label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "422be131",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Evaluation Detection (Qwen3-0.6b signals, SVC classifier):\n",
      "  Response-level TP: 115, TN: 63, FP: 65, FN: 13\n",
      "  Precision: 0.6389 (63.89%)\n",
      "  Recall: 0.8984 (89.84%)\n",
      "  F1: 0.7468 (74.68%)\n",
      "\n",
      "Claim: 'Method achieved F1=74.68%'\n",
      "Actual F1: 74.68%\n",
      "Difference: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load SVC model and make predictions for response-level evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "svc_model_path = os.path.join(models_path, \"model_SVC_3000.pickle\")\n",
    "with open(svc_model_path, \"rb\") as f:\n",
    "    svc_model = pickle.load(f)\n",
    "\n",
    "test_features = [col for col in test_df.columns if col not in ['identifier', 'hallucination_label']]\n",
    "test_df['pred'] = svc_model.predict(test_df[test_features])\n",
    "\n",
    "# Response-level evaluation (aggregate span predictions to response level)\n",
    "test_df[\"response_id\"] = test_df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
    "\n",
    "# Group by response_id, aggregate with OR (max works for binary 0/1)\n",
    "agg_df = test_df.groupby(\"response_id\").agg({\n",
    "    \"pred\": \"max\",\n",
    "    \"hallucination_label\": \"max\"\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate response-level metrics\n",
    "tn, fp, fn, tp = confusion_matrix(agg_df[\"hallucination_label\"], agg_df[\"pred\"]).ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Self-Evaluation Detection (Qwen3-0.6b signals, SVC classifier):\")\n",
    "print(f\"  Response-level TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"  Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"  F1: {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(f\"\\nClaim: 'Method achieved F1=74.68%'\")\n",
    "print(f\"Actual F1: {f1*100:.2f}%\")\n",
    "print(f\"Difference: {abs(f1*100 - 74.68):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec564e5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Test DataFrame: 1105 samples\n",
      "Class distribution: {0: 835, 1: 270}\n"
     ]
    }
   ],
   "source": [
    "# Great! The Self-Evaluation F1 exactly matches the claim (74.68%)\n",
    "# Now let's verify the Proxy-Based Evaluation claim (GPT-4.1-mini responses)\n",
    "\n",
    "test_gpt_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\")\n",
    "with open(test_gpt_path, \"r\") as f:\n",
    "    test_gpt_response = json.load(f)\n",
    "\n",
    "# Preprocess GPT test data\n",
    "test_gpt_data_dict = {\n",
    "    \"identifier\": [],\n",
    "    **{col: [] for col in ATTENTION_COLS},\n",
    "    **{col: [] for col in PARAMETER_COLS},\n",
    "    \"hallucination_label\": []\n",
    "}\n",
    "\n",
    "for i, resp in enumerate(test_gpt_response):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_gpt_data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_gpt_data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        \n",
    "        for col in PARAMETER_COLS:\n",
    "            test_gpt_data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_gpt_data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_gpt_df = pd.DataFrame(test_gpt_data_dict)\n",
    "print(f\"GPT Test DataFrame: {len(test_gpt_df)} samples\")\n",
    "print(f\"Class distribution: {test_gpt_df['hallucination_label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1633cca",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy-Based Evaluation Detection (Qwen3-0.6b signals on GPT-4.1-mini responses, SVC classifier):\n",
      "  Response-level TP: 78, TN: 37, FP: 46, FN: 5\n",
      "  Precision: 0.6290 (62.90%)\n",
      "  Recall: 0.9398 (93.98%)\n",
      "  F1: 0.7536 (75.36%)\n",
      "\n",
      "Claim: 'Method achieved F1=75.36%'\n",
      "Actual F1: 75.36%\n",
      "Difference: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on GPT test data using SVC model\n",
    "test_gpt_features = [col for col in test_gpt_df.columns if col not in ['identifier', 'hallucination_label']]\n",
    "test_gpt_df['pred'] = svc_model.predict(test_gpt_df[test_gpt_features])\n",
    "\n",
    "# Response-level evaluation\n",
    "test_gpt_df[\"response_id\"] = test_gpt_df[\"identifier\"].str.extract(r\"(response_\\d+)_item_\\d+\")\n",
    "\n",
    "# Group by response_id\n",
    "agg_gpt_df = test_gpt_df.groupby(\"response_id\").agg({\n",
    "    \"pred\": \"max\",\n",
    "    \"hallucination_label\": \"max\"\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate response-level metrics\n",
    "tn_gpt, fp_gpt, fn_gpt, tp_gpt = confusion_matrix(agg_gpt_df[\"hallucination_label\"], agg_gpt_df[\"pred\"]).ravel()\n",
    "precision_gpt = tp_gpt / (tp_gpt + fp_gpt) if (tp_gpt + fp_gpt) > 0 else 0\n",
    "recall_gpt = tp_gpt / (tp_gpt + fn_gpt) if (tp_gpt + fn_gpt) > 0 else 0\n",
    "f1_gpt = 2 * precision_gpt * recall_gpt / (precision_gpt + recall_gpt) if (precision_gpt + recall_gpt) > 0 else 0\n",
    "\n",
    "print(\"Proxy-Based Evaluation Detection (Qwen3-0.6b signals on GPT-4.1-mini responses, SVC classifier):\")\n",
    "print(f\"  Response-level TP: {tp_gpt}, TN: {tn_gpt}, FP: {fp_gpt}, FN: {fn_gpt}\")\n",
    "print(f\"  Precision: {precision_gpt:.4f} ({precision_gpt*100:.2f}%)\")\n",
    "print(f\"  Recall: {recall_gpt:.4f} ({recall_gpt*100:.2f}%)\")\n",
    "print(f\"  F1: {f1_gpt:.4f} ({f1_gpt*100:.2f}%)\")\n",
    "print(f\"\\nClaim: 'Method achieved F1=75.36%'\")\n",
    "print(f\"Actual F1: {f1_gpt*100:.2f}%\")\n",
    "print(f\"Difference: {abs(f1_gpt*100 - 75.36):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72349a69",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CS1: CONCLUSIONS VS ORIGINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ECS_Correlation:\n",
      "  Claim: All attention heads exhibit negative correlations between ECS and hallucination\n",
      "  Status: ✓ MATCHES\n",
      "  Details: All 448 attention heads show negative correlation (range: -0.3378 to -0.0102)\n",
      "\n",
      "PKS_Correlation:\n",
      "  Claim: Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated\n",
      "  Status: ✓ MATCHES\n",
      "  Details: Early layers (0-13) mean r=0.0755, Later layers (14-27) mean r=0.2121\n",
      "\n",
      "Classifier_Selection:\n",
      "  Claim: SVC achieved highest validation F1 (76.60%) and was selected\n",
      "  Status: ✗ DOES NOT MATCH\n",
      "  Details: SVC achieved 78.89% but XGBoost achieved 95.36%\n",
      "\n",
      "Self_Evaluation:\n",
      "  Claim: Method achieved F1=74.68%\n",
      "  Status: ✓ MATCHES\n",
      "  Details: Actual F1=74.68% exactly matches the claim\n",
      "\n",
      "Proxy_Evaluation:\n",
      "  Claim: Method achieved F1=75.36%\n",
      "  Status: ✓ MATCHES\n",
      "  Details: Actual F1=75.36% exactly matches the claim\n"
     ]
    }
   ],
   "source": [
    "# Excellent! The Proxy-Based Evaluation F1 also exactly matches the claim (75.36%)\n",
    "\n",
    "# Now let's summarize CS1 findings\n",
    "print(\"=\"*80)\n",
    "print(\"CS1: CONCLUSIONS VS ORIGINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cs1_results = {\n",
    "    \"ECS_Correlation\": {\n",
    "        \"claim\": \"All attention heads exhibit negative correlations between ECS and hallucination\",\n",
    "        \"verified\": True,\n",
    "        \"details\": f\"All {len(ecs_pcc_direct)} attention heads show negative correlation (range: {min(ecs_pcc_direct.values()):.4f} to {max(ecs_pcc_direct.values()):.4f})\"\n",
    "    },\n",
    "    \"PKS_Correlation\": {\n",
    "        \"claim\": \"Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated\",\n",
    "        \"verified\": True,\n",
    "        \"details\": f\"Early layers (0-13) mean r={np.mean(early_layers):.4f}, Later layers (14-27) mean r={np.mean(later_layers):.4f}\"\n",
    "    },\n",
    "    \"Classifier_Selection\": {\n",
    "        \"claim\": \"SVC achieved highest validation F1 (76.60%) and was selected\",\n",
    "        \"verified\": False,\n",
    "        \"details\": f\"SVC achieved {model_results['SVC']['f1']*100:.2f}% but XGBoost achieved {model_results['XGBoost']['f1']*100:.2f}%\"\n",
    "    },\n",
    "    \"Self_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=74.68%\",\n",
    "        \"verified\": True,\n",
    "        \"details\": f\"Actual F1={f1*100:.2f}% exactly matches the claim\"\n",
    "    },\n",
    "    \"Proxy_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=75.36%\",\n",
    "        \"verified\": True,\n",
    "        \"details\": f\"Actual F1={f1_gpt*100:.2f}% exactly matches the claim\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, result in cs1_results.items():\n",
    "    status = \"✓ MATCHES\" if result['verified'] else \"✗ DOES NOT MATCH\"\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Claim: {result['claim']}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Details: {result['details']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a3bf8f9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Methodology from Plan:\n",
      "1. Compute External Context Score (ECS) per attention head and layer by identifying the most attended context chunk via attention weights, then measuring cosine similarity between response and context embeddings.\n",
      "2. Compute Parametric Knowledge Score (PKS) per FFN layer by measuring Jensen-Shannon divergence between vocabulary distributions before and after the FFN layer in the residual stream.\n",
      "3. Use TransformerLens library on Qwen3-0.6b model to extract internal mechanistic signals (ECS and PKS) at span level across 28 layers and 16 attention heads.\n",
      "4. Train binary classifiers (Logistic Regression, SVC, Random Forest, XGBoost) on standardized and correlation-filtered ECS/PKS features to predict span-level hallucinations, then aggregate to response-level.\n",
      "5. Evaluate both self-evaluation (same model generates responses and computes signals) and proxy-based evaluation (Qwen3-0.6b signals applied to GPT-4.1-mini responses) settings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CS2: Verify Plan Steps Appear in Implementation\n",
    "# Let's extract the plan steps and verify each one exists in the implementation\n",
    "\n",
    "plan_methodology = \"\"\"\n",
    "Methodology from Plan:\n",
    "1. Compute External Context Score (ECS) per attention head and layer by identifying the most attended context chunk via attention weights, then measuring cosine similarity between response and context embeddings.\n",
    "2. Compute Parametric Knowledge Score (PKS) per FFN layer by measuring Jensen-Shannon divergence between vocabulary distributions before and after the FFN layer in the residual stream.\n",
    "3. Use TransformerLens library on Qwen3-0.6b model to extract internal mechanistic signals (ECS and PKS) at span level across 28 layers and 16 attention heads.\n",
    "4. Train binary classifiers (Logistic Regression, SVC, Random Forest, XGBoost) on standardized and correlation-filtered ECS/PKS features to predict span-level hallucinations, then aggregate to response-level.\n",
    "5. Evaluate both self-evaluation (same model generates responses and computes signals) and proxy-based evaluation (Qwen3-0.6b signals applied to GPT-4.1-mini responses) settings.\n",
    "\"\"\"\n",
    "print(plan_methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92b9efe0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CS2: PLAN VS IMPLEMENTATION VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "1. ECS Computation (compute_scores.py):\n",
      "  attention_weights: ✓ Found\n",
      "  cosine_similarity: ✓ Found\n",
      "  bge_model: ✓ Found\n",
      "  per_head_layer: ✓ Found\n",
      "\n",
      "2. PKS Computation (compute_scores.py):\n",
      "  jensen_shannon: ✓ Found\n",
      "  kl_divergence: ✓ Found\n",
      "  resid_mid: ✓ Found\n",
      "  resid_post: ✓ Found\n",
      "  vocab_distribution: ✓ Found\n"
     ]
    }
   ],
   "source": [
    "# Now let's verify each step exists in the implementation\n",
    "\n",
    "# Step 1: ECS computation - Check compute_scores.py\n",
    "print(\"=\"*80)\n",
    "print(\"CS2: PLAN VS IMPLEMENTATION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check Step 1: ECS computation\n",
    "print(\"\\n1. ECS Computation (compute_scores.py):\")\n",
    "ecs_check = {\n",
    "    \"attention_weights\": \"attention_score = outputs.attentions[attentions_layer_id][0, head_id, :, :]\" in compute_scores_content,\n",
    "    \"cosine_similarity\": \"calculate_sentence_similarity\" in compute_scores_content,\n",
    "    \"bge_model\": \"SentenceTransformer\" in compute_scores_content,\n",
    "    \"per_head_layer\": \"for attentions_layer_id\" in compute_scores_content and \"for head_id\" in compute_scores_content\n",
    "}\n",
    "for key, found in ecs_check.items():\n",
    "    print(f\"  {key}: {'✓ Found' if found else '✗ NOT FOUND'}\")\n",
    "\n",
    "# Check Step 2: PKS computation\n",
    "print(\"\\n2. PKS Computation (compute_scores.py):\")\n",
    "pks_check = {\n",
    "    \"jensen_shannon\": \"calculate_dist_2d\" in compute_scores_content,\n",
    "    \"kl_divergence\": \"F.kl_div\" in compute_scores_content,\n",
    "    \"resid_mid\": \"hook_resid_mid\" in compute_scores_content,\n",
    "    \"resid_post\": \"hook_resid_post\" in compute_scores_content,\n",
    "    \"vocab_distribution\": \"W_U\" in compute_scores_content\n",
    "}\n",
    "for key, found in pks_check.items():\n",
    "    print(f\"  {key}: {'✓ Found' if found else '✗ NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8bff6a2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. TransformerLens on Qwen3-0.6b (compute_scores.py):\n",
      "  transformer_lens_import: ✓ Found\n",
      "  run_with_cache: ✓ Found\n",
      "  qwen_model: ✓ Found\n",
      "\n",
      "  28 layers: ✓ Found (28 layers in data)\n",
      "  16 heads per layer: ✓ Found (448 total = 28 layers × 16 heads)\n",
      "\n",
      "4. Classifier Training (classifier.py):\n",
      "  logistic_regression: ✓ Found\n",
      "  svc: ✓ Found\n",
      "  random_forest: ✓ Found\n",
      "  xgboost: ✓ Found\n",
      "  standardization: ✓ Found\n",
      "  span_level: ✓ Found\n"
     ]
    }
   ],
   "source": [
    "# Check Step 3: TransformerLens on Qwen3-0.6b\n",
    "print(\"3. TransformerLens on Qwen3-0.6b (compute_scores.py):\")\n",
    "transformerlens_check = {\n",
    "    \"transformer_lens_import\": \"from transformer_lens import HookedTransformer\" in compute_scores_content,\n",
    "    \"run_with_cache\": \"model.run_with_cache\" in compute_scores_content,\n",
    "    \"qwen_model\": \"Qwen\" in compute_scores_content or \"qwen\" in compute_scores_content.lower()\n",
    "}\n",
    "for key, found in transformerlens_check.items():\n",
    "    print(f\"  {key}: {'✓ Found' if found else '✗ NOT FOUND'}\")\n",
    "\n",
    "# Check model parameters in data (28 layers, 16 heads)\n",
    "print(f\"\\n  28 layers: ✓ Found ({len(pks_truthful)} layers in data)\")\n",
    "print(f\"  16 heads per layer: ✓ Found ({len(ecs_truthful)} total = 28 layers × 16 heads)\")\n",
    "\n",
    "# Check Step 4: Classifier training\n",
    "print(\"\\n4. Classifier Training (classifier.py):\")\n",
    "classifier_check = {\n",
    "    \"logistic_regression\": \"LogisticRegression\" in classifier_content,\n",
    "    \"svc\": \"SVC\" in classifier_content,\n",
    "    \"random_forest\": \"RandomForestClassifier\" in classifier_content,\n",
    "    \"xgboost\": \"XGBClassifier\" in classifier_content,\n",
    "    \"standardization\": \"StandardScaler\" in classifier_content,\n",
    "    \"span_level\": \"span\" in classifier_content.lower() or \"item\" in classifier_content\n",
    "}\n",
    "for key, found in classifier_check.items():\n",
    "    print(f\"  {key}: {'✓ Found' if found else '✗ NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41242eab",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Evaluation Settings (predict.py and data files):\n",
      "  response_level_agg: ✓ Found\n",
      "  span_to_response: ✓ Found\n",
      "  self_eval_data (Qwen test): ✓ Found\n",
      "  proxy_eval_data (GPT-4.1-mini test): ✓ Found\n",
      "\n",
      "  Baseline comparisons implemented:\n",
      "    - run_refchecker.py\n",
      "    - run_hf.py\n",
      "    - run_ragas.py\n",
      "    - run_groq.py\n",
      "    - run_trulens.py\n",
      "    - run_gpt.py\n"
     ]
    }
   ],
   "source": [
    "# Check Step 5: Self-evaluation and proxy-based evaluation\n",
    "print(\"5. Evaluation Settings (predict.py and data files):\")\n",
    "\n",
    "# Check predict.py for response-level aggregation\n",
    "predict_check = {\n",
    "    \"response_level_agg\": \"response_id\" in predict_content,\n",
    "    \"span_to_response\": \"groupby\" in predict_content,\n",
    "}\n",
    "for key, found in predict_check.items():\n",
    "    print(f\"  {key}: {'✓ Found' if found else '✗ NOT FOUND'}\")\n",
    "\n",
    "# Check for test data files\n",
    "test_qwen_exists = os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\"))\n",
    "test_gpt_exists = os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\"))\n",
    "\n",
    "print(f\"  self_eval_data (Qwen test): {'✓ Found' if test_qwen_exists else '✗ NOT FOUND'}\")\n",
    "print(f\"  proxy_eval_data (GPT-4.1-mini test): {'✓ Found' if test_gpt_exists else '✗ NOT FOUND'}\")\n",
    "\n",
    "# Check baseline implementations\n",
    "baseline_files = os.listdir(os.path.join(repo_path, \"scripts/baseline\"))\n",
    "print(f\"\\n  Baseline comparisons implemented:\")\n",
    "for f in baseline_files:\n",
    "    if f.endswith('.py'):\n",
    "        print(f\"    - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "150a23a4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CS2: PLAN VS IMPLEMENTATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Step1_ECS_Computation:\n",
      "  Description: Compute ECS per attention head/layer using attention weights and cosine similarity\n",
      "  Status: ✓ IMPLEMENTED\n",
      "  Details: Found in compute_scores.py: attention weights extraction, sentence similarity using BGE model\n",
      "\n",
      "Step2_PKS_Computation:\n",
      "  Description: Compute PKS per FFN layer using Jensen-Shannon divergence\n",
      "  Status: ✓ IMPLEMENTED\n",
      "  Details: Found in compute_scores.py: JS divergence via KL divergence, residual stream hooks (mid/post)\n",
      "\n",
      "Step3_TransformerLens_Qwen:\n",
      "  Description: Use TransformerLens on Qwen3-0.6b for 28 layers × 16 heads\n",
      "  Status: ✓ IMPLEMENTED\n",
      "  Details: Found: HookedTransformer import, run_with_cache, 448 attention heads (28×16)\n",
      "\n",
      "Step4_Classifier_Training:\n",
      "  Description: Train LR, SVC, RF, XGBoost with standardization on span-level features\n",
      "  Status: ✓ IMPLEMENTED\n",
      "  Details: Found in classifier.py: all 4 classifiers, StandardScaler, trained models saved\n",
      "\n",
      "Step5_Evaluation_Settings:\n",
      "  Description: Self-evaluation and proxy-based evaluation (Qwen → GPT-4.1-mini)\n",
      "  Status: ✓ IMPLEMENTED\n",
      "  Details: Found: response-level aggregation, both test datasets exist\n",
      "\n",
      "================================================================================\n",
      "CS2 OVERALL: PASS - All plan steps implemented\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summarize CS2 findings\n",
    "print(\"=\"*80)\n",
    "print(\"CS2: PLAN VS IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cs2_results = {\n",
    "    \"Step1_ECS_Computation\": {\n",
    "        \"description\": \"Compute ECS per attention head/layer using attention weights and cosine similarity\",\n",
    "        \"implemented\": True,\n",
    "        \"details\": \"Found in compute_scores.py: attention weights extraction, sentence similarity using BGE model\"\n",
    "    },\n",
    "    \"Step2_PKS_Computation\": {\n",
    "        \"description\": \"Compute PKS per FFN layer using Jensen-Shannon divergence\",\n",
    "        \"implemented\": True,\n",
    "        \"details\": \"Found in compute_scores.py: JS divergence via KL divergence, residual stream hooks (mid/post)\"\n",
    "    },\n",
    "    \"Step3_TransformerLens_Qwen\": {\n",
    "        \"description\": \"Use TransformerLens on Qwen3-0.6b for 28 layers × 16 heads\",\n",
    "        \"implemented\": True,\n",
    "        \"details\": \"Found: HookedTransformer import, run_with_cache, 448 attention heads (28×16)\"\n",
    "    },\n",
    "    \"Step4_Classifier_Training\": {\n",
    "        \"description\": \"Train LR, SVC, RF, XGBoost with standardization on span-level features\",\n",
    "        \"implemented\": True,\n",
    "        \"details\": \"Found in classifier.py: all 4 classifiers, StandardScaler, trained models saved\"\n",
    "    },\n",
    "    \"Step5_Evaluation_Settings\": {\n",
    "        \"description\": \"Self-evaluation and proxy-based evaluation (Qwen → GPT-4.1-mini)\",\n",
    "        \"implemented\": True,\n",
    "        \"details\": \"Found: response-level aggregation, both test datasets exist\"\n",
    "    }\n",
    "}\n",
    "\n",
    "all_implemented = True\n",
    "for key, result in cs2_results.items():\n",
    "    status = \"✓ IMPLEMENTED\" if result['implemented'] else \"✗ NOT IMPLEMENTED\"\n",
    "    if not result['implemented']:\n",
    "        all_implemented = False\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Description: {result['description']}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Details: {result['details']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CS2 OVERALL: {'PASS - All plan steps implemented' if all_implemented else 'FAIL - Some steps missing'}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9016de4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL CONSISTENCY EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "### CS1: Results vs Conclusions ###\n",
      "Status: FAIL\n",
      "\n",
      "Details:\n",
      "  ✓ ECS_Correlation\n",
      "  ✓ PKS_Correlation\n",
      "  ✗ Classifier_Selection\n",
      "      Issue: SVC achieved 78.89% but XGBoost achieved 95.36%\n",
      "  ✓ Self_Evaluation\n",
      "  ✓ Proxy_Evaluation\n",
      "\n",
      "### CS2: Plan vs Implementation ###\n",
      "Status: PASS\n",
      "\n",
      "Details:\n",
      "  ✓ Step1_ECS_Computation\n",
      "  ✓ Step2_PKS_Computation\n",
      "  ✓ Step3_TransformerLens_Qwen\n",
      "  ✓ Step4_Classifier_Training\n",
      "  ✓ Step5_Evaluation_Settings\n",
      "\n",
      "================================================================================\n",
      "OVERALL EVALUATION\n",
      "================================================================================\n",
      "CS1 (Results vs Conclusions): FAIL\n",
      "CS2 (Plan vs Implementation): PASS\n"
     ]
    }
   ],
   "source": [
    "# Final Summary for both CS1 and CS2\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL CONSISTENCY EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### CS1: Results vs Conclusions ###\")\n",
    "cs1_pass = all(result['verified'] for result in cs1_results.values())\n",
    "print(f\"Status: {'PASS' if cs1_pass else 'FAIL'}\")\n",
    "print(\"\\nDetails:\")\n",
    "for key, result in cs1_results.items():\n",
    "    status = \"✓\" if result['verified'] else \"✗\"\n",
    "    print(f\"  {status} {key}\")\n",
    "    if not result['verified']:\n",
    "        print(f\"      Issue: {result['details']}\")\n",
    "\n",
    "print(\"\\n### CS2: Plan vs Implementation ###\")\n",
    "cs2_pass = all(result['implemented'] for result in cs2_results.values())\n",
    "print(f\"Status: {'PASS' if cs2_pass else 'FAIL'}\")\n",
    "print(\"\\nDetails:\")\n",
    "for key, result in cs2_results.items():\n",
    "    status = \"✓\" if result['implemented'] else \"✗\"\n",
    "    print(f\"  {status} {key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"CS1 (Results vs Conclusions): {'PASS' if cs1_pass else 'FAIL'}\")\n",
    "print(f\"CS2 (Plan vs Implementation): {'PASS' if cs2_pass else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be7a9ed5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation directory created: /net/scratch2/smallyan/InterpDetect_eval/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation directory if it doesn't exist\n",
    "eval_dir = os.path.join(repo_path, 'evaluation')\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "print(f\"Evaluation directory created: {eval_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86d10524",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: /net/scratch2/smallyan/InterpDetect_eval/evaluation/self_matching.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Create the self_matching.ipynb notebook\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "\n",
    "# Create a new notebook\n",
    "nb = new_notebook()\n",
    "\n",
    "# Add cells to the notebook\n",
    "cells = []\n",
    "\n",
    "# Title and overview\n",
    "cells.append(new_markdown_cell(\"\"\"# Consistency Evaluation - Self Matching Analysis\n",
    "\n",
    "This notebook performs a consistency evaluation of the InterpDetect project, checking:\n",
    "1. **CS1**: Whether conclusions match the original recorded results\n",
    "2. **CS2**: Whether implementation follows the plan\n",
    "\n",
    "## Project Overview\n",
    "The InterpDetect project implements a mechanistic interpretability-based hallucination detection method for RAG systems using:\n",
    "- External Context Score (ECS) - measures attention to external context\n",
    "- Parametric Knowledge Score (PKS) - measures FFN contribution via Jensen-Shannon divergence\n",
    "\"\"\"))\n",
    "\n",
    "# Setup cell\n",
    "cells.append(new_code_cell(\"\"\"import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import torch\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "repo_path = '/net/scratch2/smallyan/InterpDetect_eval'\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\"\"\"))\n",
    "\n",
    "# Load data cell\n",
    "cells.append(new_markdown_cell(\"\"\"## Load Training Data\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Load training data\n",
    "folder_path = os.path.join(repo_path, \"datasets/train\")\n",
    "examples = []\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        examples.extend(data)\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples from {len(json_files)} files\")\"\"\"))\n",
    "\n",
    "# CS1 Analysis\n",
    "cells.append(new_markdown_cell(\"\"\"## CS1: Results vs Conclusions Analysis\n",
    "\n",
    "### Claim 1: ECS Correlation Analysis\n",
    "**Plan states**: \"All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\"\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Separate ECS and PKS data by hallucination label\n",
    "ecs_truthful = collections.defaultdict(list)\n",
    "ecs_hallucinated = collections.defaultdict(list)\n",
    "pks_truthful = collections.defaultdict(list)\n",
    "pks_hallucinated = collections.defaultdict(list)\n",
    "\n",
    "for example in examples:\n",
    "    for score in example['scores']:\n",
    "        if score['hallucination_label']==0:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_truthful[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_truthful[k].append(v)\n",
    "        else:\n",
    "            for k, v in score['prompt_attention_score'].items():\n",
    "                ecs_hallucinated[k].append(v)\n",
    "            for k, v in score['parameter_knowledge_scores'].items():\n",
    "                pks_hallucinated[k].append(v)\n",
    "\n",
    "print(f\"Number of attention heads: {len(ecs_truthful)}\")\n",
    "print(f\"Number of FFN layers: {len(pks_truthful)}\")\n",
    "print(f\"Truthful spans: {len(list(ecs_truthful.values())[0])}\")\n",
    "print(f\"Hallucinated spans: {len(list(ecs_hallucinated.values())[0])}\")\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Compute ECS vs Hallucination correlation\n",
    "def pearson_corr(attention_scores, hallucination_labels, inverse=False):\n",
    "    scores = np.array(attention_scores, dtype=float)\n",
    "    labels = np.array(hallucination_labels, dtype=int)\n",
    "    \n",
    "    if inverse:\n",
    "        inverse_labels = 1 - labels\n",
    "        r, p_value = pearsonr(scores, inverse_labels)\n",
    "    else:\n",
    "        r, p_value = pearsonr(scores, labels)\n",
    "    \n",
    "    return r, p_value\n",
    "\n",
    "# Aggregate ECS data for correlation\n",
    "ecs_lst = collections.defaultdict(list) \n",
    "ecs_label_lst = collections.defaultdict(list) \n",
    "for k, v in ecs_truthful.items():\n",
    "    for a in v:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(0)\n",
    "    for a in ecs_hallucinated[k]:\n",
    "        ecs_lst[k].append(a)\n",
    "        ecs_label_lst[k].append(1)\n",
    "\n",
    "# Compute ECS correlations (ECS vs Hallucination directly)\n",
    "ecs_pcc_direct = {}\n",
    "for k, v in ecs_lst.items():\n",
    "    r, p_val = pearson_corr(v, ecs_label_lst[k], inverse=False)\n",
    "    ecs_pcc_direct[k] = r\n",
    "\n",
    "# Count positive and negative correlations\n",
    "positive_corr = sum(1 for v in ecs_pcc_direct.values() if v > 0)\n",
    "negative_corr = sum(1 for v in ecs_pcc_direct.values() if v < 0)\n",
    "\n",
    "print(f\"ECS Correlation Analysis (ECS vs Hallucination Label):\")\n",
    "print(f\"  Positive correlations: {positive_corr}\")\n",
    "print(f\"  Negative correlations: {negative_corr}\")\n",
    "print(f\"  Total heads: {len(ecs_pcc_direct)}\")\n",
    "print(f\"\\\\nClaim: 'All attention heads exhibit negative correlations'\")\n",
    "print(f\"Result: {'MATCHES' if negative_corr == len(ecs_pcc_direct) else 'DOES NOT MATCH'}\")\"\"\"))\n",
    "\n",
    "cells.append(new_markdown_cell(\"\"\"### Claim 2: PKS Correlation Analysis\n",
    "**Plan states**: \"Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\"\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Compute PKS vs Hallucination correlation\n",
    "pks_lst = collections.defaultdict(list) \n",
    "pks_label_lst = collections.defaultdict(list) \n",
    "for k, v in pks_truthful.items():\n",
    "    for a in v:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(0)\n",
    "    for a in pks_hallucinated[k]:\n",
    "        pks_lst[k].append(a)\n",
    "        pks_label_lst[k].append(1)\n",
    "\n",
    "# Compute PKS correlations\n",
    "pks_pcc = {}\n",
    "for k, v in pks_lst.items():\n",
    "    r, p_val = pearson_corr(v, pks_label_lst[k], inverse=False)\n",
    "    pks_pcc[k] = r\n",
    "\n",
    "# Sort by layer number\n",
    "sorted_pks = sorted(pks_pcc.items(), key=lambda x: int(x[0].split('_')[1]))\n",
    "\n",
    "# Compare early vs later layers\n",
    "early_layers = [v for k, v in sorted_pks[:14]]  # layers 0-13\n",
    "later_layers = [v for k, v in sorted_pks[14:]]  # layers 14-27\n",
    "\n",
    "print(f\"PKS Correlation Analysis:\")\n",
    "print(f\"  Mean correlation - Early layers (0-13): {np.mean(early_layers):.4f}\")\n",
    "print(f\"  Mean correlation - Later layers (14-27): {np.mean(later_layers):.4f}\")\n",
    "print(f\"\\\\nClaim: 'Later-layer FFNs exhibit higher PKS correlation with hallucinations'\")\n",
    "print(f\"Result: {'MATCHES' if np.mean(later_layers) > np.mean(early_layers) else 'DOES NOT MATCH'}\")\"\"\"))\n",
    "\n",
    "cells.append(new_markdown_cell(\"\"\"### Claim 3: Classifier Performance\n",
    "**Plan states**: \"SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\"\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Prepare data for classifier evaluation\n",
    "ATTENTION_COLS = list(examples[0]['scores'][0]['prompt_attention_score'].keys())\n",
    "PARAMETER_COLS = list(examples[0]['scores'][0]['parameter_knowledge_scores'].keys())\n",
    "\n",
    "data_dict = {\n",
    "    \"identifier\": [],\n",
    "    **{col: [] for col in ATTENTION_COLS},\n",
    "    **{col: [] for col in PARAMETER_COLS},\n",
    "    \"hallucination_label\": []\n",
    "}\n",
    "\n",
    "for i, resp in enumerate(examples):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Balance and split\n",
    "min_count = df['hallucination_label'].value_counts().min()\n",
    "df_balanced = df.groupby('hallucination_label', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_count, random_state=42), include_groups=False\n",
    ").reset_index(drop=True)\n",
    "df_balanced['hallucination_label'] = df.groupby('hallucination_label', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_count, random_state=42)\n",
    ")['hallucination_label'].values\n",
    "\n",
    "train, val = train_test_split(df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['hallucination_label'])\n",
    "features = [col for col in df_balanced.columns if col not in ['identifier', 'hallucination_label']]\n",
    "\n",
    "X_train, y_train = train[features], train[\"hallucination_label\"]\n",
    "X_val, y_val = val[features], val[\"hallucination_label\"]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples, Validation set: {len(X_val)} samples\")\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Evaluate pre-trained models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "models_path = os.path.join(repo_path, \"trained_models\")\n",
    "model_names = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
    "model_results = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model_file = os.path.join(models_path, f\"model_{name}_3000.pickle\")\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    model_results[name] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    \n",
    "print(\"Classifier Performance Comparison:\")\n",
    "print(\"=\"*50)\n",
    "for name in model_names:\n",
    "    print(f\"{name}: F1 = {model_results[name]['f1']*100:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\\\nClaim: 'SVC achieved highest validation F1 (76.60%)'\")\n",
    "print(f\"Actual highest: {max(model_results.items(), key=lambda x: x[1]['f1'])[0]} with {max(model_results.values(), key=lambda x: x['f1'])['f1']*100:.2f}%\")\n",
    "print(f\"Result: {'MATCHES' if max(model_results.items(), key=lambda x: x[1]['f1'])[0] == 'SVC' else 'DOES NOT MATCH'}\")\"\"\"))\n",
    "\n",
    "cells.append(new_markdown_cell(\"\"\"### Claim 4 & 5: Detection Performance\n",
    "**Plan states**: \n",
    "- Self-Evaluation: \"Method achieved F1=74.68%\"\n",
    "- Proxy-Based: \"Method achieved F1=75.36%\"\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Load SVC model for response-level evaluation\n",
    "svc_model_path = os.path.join(models_path, \"model_SVC_3000.pickle\")\n",
    "with open(svc_model_path, \"rb\") as f:\n",
    "    svc_model = pickle.load(f)\n",
    "\n",
    "# Self-Evaluation (Qwen test data)\n",
    "test_qwen_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")\n",
    "with open(test_qwen_path, \"r\") as f:\n",
    "    test_qwen = json.load(f)\n",
    "\n",
    "test_qwen_dict = {\"identifier\": [], **{col: [] for col in ATTENTION_COLS}, \n",
    "                  **{col: [] for col in PARAMETER_COLS}, \"hallucination_label\": []}\n",
    "for i, resp in enumerate(test_qwen):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_qwen_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_qwen_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            test_qwen_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_qwen_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_qwen_df = pd.DataFrame(test_qwen_dict)\n",
    "test_qwen_df['pred'] = svc_model.predict(test_qwen_df[features])\n",
    "test_qwen_df[\"response_id\"] = test_qwen_df[\"identifier\"].str.extract(r\"(response_\\\\d+)_item_\\\\d+\")\n",
    "agg_qwen = test_qwen_df.groupby(\"response_id\").agg({\"pred\": \"max\", \"hallucination_label\": \"max\"}).reset_index()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(agg_qwen[\"hallucination_label\"], agg_qwen[\"pred\"]).ravel()\n",
    "f1_self = 2 * (tp/(tp+fp)) * (tp/(tp+fn)) / ((tp/(tp+fp)) + (tp/(tp+fn)))\n",
    "print(f\"Self-Evaluation F1: {f1_self*100:.2f}% (Claimed: 74.68%)\")\n",
    "print(f\"Result: {'MATCHES' if abs(f1_self*100 - 74.68) < 0.1 else 'DOES NOT MATCH'}\")\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Proxy-Based Evaluation (GPT-4.1-mini test data)\n",
    "test_gpt_path = os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\")\n",
    "with open(test_gpt_path, \"r\") as f:\n",
    "    test_gpt = json.load(f)\n",
    "\n",
    "test_gpt_dict = {\"identifier\": [], **{col: [] for col in ATTENTION_COLS}, \n",
    "                 **{col: [] for col in PARAMETER_COLS}, \"hallucination_label\": []}\n",
    "for i, resp in enumerate(test_gpt):\n",
    "    for j in range(len(resp[\"scores\"])):\n",
    "        test_gpt_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
    "        for col in ATTENTION_COLS:\n",
    "            test_gpt_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
    "        for col in PARAMETER_COLS:\n",
    "            test_gpt_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
    "        test_gpt_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
    "\n",
    "test_gpt_df = pd.DataFrame(test_gpt_dict)\n",
    "test_gpt_df['pred'] = svc_model.predict(test_gpt_df[features])\n",
    "test_gpt_df[\"response_id\"] = test_gpt_df[\"identifier\"].str.extract(r\"(response_\\\\d+)_item_\\\\d+\")\n",
    "agg_gpt = test_gpt_df.groupby(\"response_id\").agg({\"pred\": \"max\", \"hallucination_label\": \"max\"}).reset_index()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(agg_gpt[\"hallucination_label\"], agg_gpt[\"pred\"]).ravel()\n",
    "f1_proxy = 2 * (tp/(tp+fp)) * (tp/(tp+fn)) / ((tp/(tp+fp)) + (tp/(tp+fn)))\n",
    "print(f\"Proxy-Based Evaluation F1: {f1_proxy*100:.2f}% (Claimed: 75.36%)\")\n",
    "print(f\"Result: {'MATCHES' if abs(f1_proxy*100 - 75.36) < 0.1 else 'DOES NOT MATCH'}\")\"\"\"))\n",
    "\n",
    "# CS2 Analysis\n",
    "cells.append(new_markdown_cell(\"\"\"## CS2: Plan vs Implementation Analysis\n",
    "\n",
    "Verifying that all methodology steps from the plan are implemented in the code.\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Read implementation files\n",
    "scripts_path = os.path.join(repo_path, 'scripts')\n",
    "with open(os.path.join(scripts_path, 'compute_scores.py'), 'r') as f:\n",
    "    compute_scores_content = f.read()\n",
    "with open(os.path.join(scripts_path, 'classifier.py'), 'r') as f:\n",
    "    classifier_content = f.read()\n",
    "with open(os.path.join(scripts_path, 'predict.py'), 'r') as f:\n",
    "    predict_content = f.read()\n",
    "\n",
    "# Verify each plan step\n",
    "cs2_checks = {\n",
    "    \"Step1_ECS\": {\n",
    "        \"attention_weights\": \"outputs.attentions\" in compute_scores_content,\n",
    "        \"cosine_similarity\": \"calculate_sentence_similarity\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step2_PKS\": {\n",
    "        \"jensen_shannon\": \"calculate_dist_2d\" in compute_scores_content,\n",
    "        \"kl_divergence\": \"F.kl_div\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step3_TransformerLens\": {\n",
    "        \"hooked_transformer\": \"HookedTransformer\" in compute_scores_content,\n",
    "        \"run_with_cache\": \"run_with_cache\" in compute_scores_content,\n",
    "    },\n",
    "    \"Step4_Classifiers\": {\n",
    "        \"LR\": \"LogisticRegression\" in classifier_content,\n",
    "        \"SVC\": \"SVC\" in classifier_content,\n",
    "        \"RF\": \"RandomForestClassifier\" in classifier_content,\n",
    "        \"XGB\": \"XGBClassifier\" in classifier_content,\n",
    "    },\n",
    "    \"Step5_Evaluation\": {\n",
    "        \"response_level\": \"response_id\" in predict_content,\n",
    "        \"test_data_qwen\": os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_qwen06b.json\")),\n",
    "        \"test_data_gpt\": os.path.exists(os.path.join(repo_path, \"datasets/test/test_w_chunk_score_gpt41mini.json\")),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"CS2: Plan vs Implementation Verification\")\n",
    "print(\"=\"*50)\n",
    "all_pass = True\n",
    "for step, checks in cs2_checks.items():\n",
    "    step_pass = all(checks.values())\n",
    "    all_pass = all_pass and step_pass\n",
    "    print(f\"\\\\n{step}: {'PASS' if step_pass else 'FAIL'}\")\n",
    "    for check, result in checks.items():\n",
    "        print(f\"  {'✓' if result else '✗'} {check}\")\n",
    "\n",
    "print(f\"\\\\n{'='*50}\")\n",
    "print(f\"CS2 Overall: {'PASS' if all_pass else 'FAIL'}\")\"\"\"))\n",
    "\n",
    "# Summary\n",
    "cells.append(new_markdown_cell(\"\"\"## Summary\n",
    "\n",
    "### CS1: Results vs Conclusions\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"# Final CS1 Summary\n",
    "cs1_results = {\n",
    "    \"ECS_Correlation\": {\n",
    "        \"claim\": \"All attention heads exhibit negative correlations\",\n",
    "        \"verified\": negative_corr == len(ecs_pcc_direct),\n",
    "        \"details\": f\"All {len(ecs_pcc_direct)} heads show negative correlation\"\n",
    "    },\n",
    "    \"PKS_Correlation\": {\n",
    "        \"claim\": \"Later-layer FFNs have higher positive correlation\",\n",
    "        \"verified\": np.mean(later_layers) > np.mean(early_layers),\n",
    "        \"details\": f\"Early: {np.mean(early_layers):.4f}, Later: {np.mean(later_layers):.4f}\"\n",
    "    },\n",
    "    \"Classifier_Selection\": {\n",
    "        \"claim\": \"SVC achieved highest validation F1 (76.60%)\",\n",
    "        \"verified\": max(model_results.items(), key=lambda x: x[1]['f1'])[0] == 'SVC',\n",
    "        \"details\": f\"Best: {max(model_results.items(), key=lambda x: x[1]['f1'])[0]} with {max(model_results.values(), key=lambda x: x['f1'])['f1']*100:.2f}%\"\n",
    "    },\n",
    "    \"Self_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=74.68%\",\n",
    "        \"verified\": abs(f1_self*100 - 74.68) < 0.1,\n",
    "        \"details\": f\"Actual: {f1_self*100:.2f}%\"\n",
    "    },\n",
    "    \"Proxy_Evaluation\": {\n",
    "        \"claim\": \"Method achieved F1=75.36%\",\n",
    "        \"verified\": abs(f1_proxy*100 - 75.36) < 0.1,\n",
    "        \"details\": f\"Actual: {f1_proxy*100:.2f}%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"CS1: Results vs Conclusions\")\n",
    "print(\"=\"*60)\n",
    "for key, result in cs1_results.items():\n",
    "    status = \"PASS\" if result['verified'] else \"FAIL\"\n",
    "    print(f\"\\\\n{key}: {status}\")\n",
    "    print(f\"  Claim: {result['claim']}\")\n",
    "    print(f\"  Details: {result['details']}\")\n",
    "\n",
    "cs1_pass = all(r['verified'] for r in cs1_results.values())\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(f\"CS1 Overall: {'PASS' if cs1_pass else 'FAIL'}\")\"\"\"))\n",
    "\n",
    "cells.append(new_markdown_cell(\"\"\"### Binary Checklist Summary\"\"\"))\n",
    "\n",
    "cells.append(new_code_cell(\"\"\"print(\"=\"*60)\n",
    "print(\"BINARY CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\\nCS1 (Results vs Conclusions): {'PASS' if cs1_pass else 'FAIL'}\")\n",
    "print(f\"CS2 (Plan vs Implementation): {'PASS' if all_pass else 'FAIL'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not cs1_pass:\n",
    "    print(\"\\\\nCS1 FAIL Reason:\")\n",
    "    for key, result in cs1_results.items():\n",
    "        if not result['verified']:\n",
    "            print(f\"  - {key}: {result['details']}\")\"\"\"))\n",
    "\n",
    "nb['cells'] = cells\n",
    "\n",
    "# Save the notebook\n",
    "notebook_path = os.path.join(eval_dir, 'self_matching.ipynb')\n",
    "with open(notebook_path, 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(f\"Notebook saved to: {notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a67e2d18",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to: /net/scratch2/smallyan/InterpDetect_eval/evaluation/consistency_evaluation.json\n",
      "\n",
      "Content:\n",
      "{\n",
      "    \"Checklist\": {\n",
      "        \"CS1_Results_vs_Conclusion\": \"FAIL\",\n",
      "        \"CS2_Plan_vs_Implementation\": \"PASS\"\n",
      "    },\n",
      "    \"Rationale\": {\n",
      "        \"CS1_Results_vs_Conclusion\": \"FAIL because the claim 'SVC achieved highest validation F1 (76.60%)' does not match the actual results. While 4 out of 5 conclusions match (ECS negative correlations verified, PKS later-layer correlation verified, Self-Evaluation F1=74.68% verified, Proxy-Based F1=75.36% verified), the classifier selection claim is contradicted by the data: XGBoost achieves 95.36% validation F1, which is higher than SVC's 78.89%. The claim states SVC had the highest validation F1, but XGBoost outperforms it significantly.\",\n",
      "        \"CS2_Plan_vs_Implementation\": \"PASS because all 5 methodology steps from the plan are fully implemented: (1) ECS computation using attention weights and cosine similarity is in compute_scores.py, (2) PKS computation using Jensen-Shannon divergence via KL divergence is in compute_scores.py, (3) TransformerLens with Qwen3-0.6b model extracting signals across 28 layers and 16 heads is implemented, (4) All 4 classifiers (LR, SVC, RandomForest, XGBoost) with StandardScaler are in classifier.py, (5) Both self-evaluation and proxy-based evaluation are supported with test data for Qwen and GPT-4.1-mini responses.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create consistency_evaluation.json\n",
    "consistency_evaluation = {\n",
    "    \"Checklist\": {\n",
    "        \"CS1_Results_vs_Conclusion\": \"FAIL\",\n",
    "        \"CS2_Plan_vs_Implementation\": \"PASS\"\n",
    "    },\n",
    "    \"Rationale\": {\n",
    "        \"CS1_Results_vs_Conclusion\": \"FAIL because the claim 'SVC achieved highest validation F1 (76.60%)' does not match the actual results. While 4 out of 5 conclusions match (ECS negative correlations verified, PKS later-layer correlation verified, Self-Evaluation F1=74.68% verified, Proxy-Based F1=75.36% verified), the classifier selection claim is contradicted by the data: XGBoost achieves 95.36% validation F1, which is higher than SVC's 78.89%. The claim states SVC had the highest validation F1, but XGBoost outperforms it significantly.\",\n",
    "        \"CS2_Plan_vs_Implementation\": \"PASS because all 5 methodology steps from the plan are fully implemented: (1) ECS computation using attention weights and cosine similarity is in compute_scores.py, (2) PKS computation using Jensen-Shannon divergence via KL divergence is in compute_scores.py, (3) TransformerLens with Qwen3-0.6b model extracting signals across 28 layers and 16 heads is implemented, (4) All 4 classifiers (LR, SVC, RandomForest, XGBoost) with StandardScaler are in classifier.py, (5) Both self-evaluation and proxy-based evaluation are supported with test data for Qwen and GPT-4.1-mini responses.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the JSON file\n",
    "json_path = os.path.join(eval_dir, 'consistency_evaluation.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(consistency_evaluation, f, indent=4)\n",
    "\n",
    "print(f\"JSON saved to: {json_path}\")\n",
    "print(\"\\nContent:\")\n",
    "print(json.dumps(consistency_evaluation, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fdc0b53",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation files created:\n",
      "  self_matching.ipynb: 22389 bytes\n",
      "  replications: 7 bytes\n",
      "  consistency_evaluation.json: 1317 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify both files were created\n",
    "print(\"Evaluation files created:\")\n",
    "for f in os.listdir(eval_dir):\n",
    "    full_path = os.path.join(eval_dir, f)\n",
    "    size = os.path.getsize(full_path)\n",
    "    print(f\"  {f}: {size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e431fb8c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replications file exists, checking if it's a directory or file...\n",
      "It's a directory\n"
     ]
    }
   ],
   "source": [
    "# Let me check what the 'replications' file is\n",
    "rep_path = os.path.join(eval_dir, 'replications')\n",
    "if os.path.exists(rep_path):\n",
    "    print(f\"Replications file exists, checking if it's a directory or file...\")\n",
    "    if os.path.isdir(rep_path):\n",
    "        print(\"It's a directory\")\n",
    "    else:\n",
    "        with open(rep_path, 'r') as f:\n",
    "            print(f\"Content: {f.read()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4c3b6d6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONSISTENCY EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Files created:\n",
      "1. /net/scratch2/smallyan/InterpDetect_eval/evaluation/self_matching.ipynb\n",
      "2. /net/scratch2/smallyan/InterpDetect_eval/evaluation/consistency_evaluation.json\n",
      "\n",
      "Binary Checklist Results:\n",
      "- CS1 (Results vs Conclusions): FAIL\n",
      "  Reason: The claim 'SVC achieved highest validation F1 (76.60%)' is contradicted\n",
      "  by the data showing XGBoost achieves 95.36% validation F1, higher than SVC's 78.89%.\n",
      "  \n",
      "- CS2 (Plan vs Implementation): PASS\n",
      "  All 5 methodology steps are fully implemented in the codebase.\n",
      "\n",
      "Summary of Mismatches (CS1):\n",
      "- Classifier Selection claim does not match actual results:\n",
      "  * Plan claims: SVC has highest validation F1 (76.60%)\n",
      "  * Actual results: XGBoost has highest validation F1 (95.36%)\n",
      "  * SVC actual validation F1: 78.89%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"CONSISTENCY EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Files created:\n",
    "1. {os.path.join(eval_dir, 'self_matching.ipynb')}\n",
    "2. {os.path.join(eval_dir, 'consistency_evaluation.json')}\n",
    "\n",
    "Binary Checklist Results:\n",
    "- CS1 (Results vs Conclusions): FAIL\n",
    "  Reason: The claim 'SVC achieved highest validation F1 (76.60%)' is contradicted\n",
    "  by the data showing XGBoost achieves 95.36% validation F1, higher than SVC's 78.89%.\n",
    "  \n",
    "- CS2 (Plan vs Implementation): PASS\n",
    "  All 5 methodology steps are fully implemented in the codebase.\n",
    "\n",
    "Summary of Mismatches (CS1):\n",
    "- Classifier Selection claim does not match actual results:\n",
    "  * Plan claims: SVC has highest validation F1 (76.60%)\n",
    "  * Actual results: XGBoost has highest validation F1 (95.36%)\n",
    "  * SVC actual validation F1: 78.89%\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-23-22-47_CircuitAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
