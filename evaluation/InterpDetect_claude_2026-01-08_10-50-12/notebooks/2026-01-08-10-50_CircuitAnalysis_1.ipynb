{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978dafd9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ee524",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for InterpDetect\n",
    "\n",
    "This notebook evaluates the generalizability of the circuit/neuron findings from the InterpDetect repository.\n",
    "\n",
    "## Evaluation Checklist:\n",
    "- **GT1**: Generalization to a New Model\n",
    "- **GT2**: Generalization to New Data  \n",
    "- **GT3**: Method/Specificity Generalizability\n",
    "\n",
    "Let's first explore the repository structure to understand the research findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81092744",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository contents:\n",
      "total 1016\n",
      "drwxrwx---  7 smallyan smallyan     13 Jan  7 14:53 .\n",
      "drwxrwx--- 37 smallyan smallyan     37 Jan  7 19:41 ..\n",
      "-rw-rw----  1 smallyan smallyan  10840 Dec 22 21:55 CodeWalkthrough.md\n",
      "drwxrwx---  4 smallyan smallyan      5 Dec 22 21:55 datasets\n",
      "-rw-r-----  1 smallyan smallyan 906708 Dec 23 01:40 documentation.pdf\n",
      "drwxrwx---  3 smallyan smallyan      5 Jan  7 16:17 evaluation\n",
      "drwxrwx---  7 smallyan smallyan     15 Jan  7 19:50 .git\n",
      "-rw-rw----  1 smallyan smallyan   5030 Dec 22 21:55 .gitignore\n",
      "-rw-rw----  1 smallyan smallyan   1066 Dec 22 21:55 LICENSE\n",
      "-rw-rw----  1 smallyan smallyan   3777 Dec 23 01:52 plan.md\n",
      "-rw-rw----  1 smallyan smallyan   1048 Dec 22 21:55 requirements.txt\n",
      "drwxrwx---  6 smallyan smallyan     10 Dec 24 20:47 scripts\n",
      "drwxrwx---  2 smallyan smallyan      6 Dec 22 21:55 trained_models\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the repository structure\n",
    "repo_path = '/net/scratch2/smallyan/InterpDetect_eval'\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['ls', '-la', repo_path], capture_output=True, text=True)\n",
    "print(\"Repository contents:\")\n",
    "print(result.stdout)\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e348d21",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Plan\n",
      "## Objective\n",
      "Develop a mechanistic interpretability-based hallucination detection method for Retrieval-Augmented Generation (RAG) systems by computing External Context Scores (ECS) across layers and attention heads and Parametric Knowledge Scores (PKS) across layers (FFN), training regression-based classifiers on these signals, and demonstrating generalization from a small proxy model (Qwen3-0.6b) to larger production models (GPT-4.1-mini).\n",
      "\n",
      "## Hypothesis\n",
      "1. RAG hallucinations correlate with:  later-layer FFN modules disproportionately inject parametric knowledge into the residual stream while attention heads fail to adequately exploit external context.\n",
      "2. External Context Score (ECS) and Parametric Knowledge Score (PKS) are correlated with hallucination occurrence and can serve as predictive features for hallucination detection.\n",
      "3. Mechanistic signals extracted from a small proxy model (0.6b parameters) can generalize to detect hallucinations in responses from larger production-level models.\n",
      "\n",
      "## Methodology\n",
      "1. Compute External Context Score (ECS) per attention head and layer by identifying the most attended context chunk via attention weights, then measuring cosine similarity between response and context embeddings.\n",
      "2. Compute Parametric Knowledge Score (PKS) per FFN layer by measuring Jensen-Shannon divergence between vocabulary distributions before and after the FFN layer in the residual stream.\n",
      "3. Use TransformerLens library on Qwen3-0.6b model to extract internal mechanistic signals (ECS and PKS) at span level across 28 layers and 16 attention heads.\n",
      "4. Train binary classifiers (Logistic Regression, SVC, Random Forest, XGBoost) on standardized and correlation-filtered ECS/PKS features to predict span-level hallucinations, then aggregate to response-level.\n",
      "5. Evaluate both self-evaluation (same model generates responses and computes signals) and proxy-based evaluation (Qwen3-0.6b signals applied to GPT-4.1-mini responses) settings.\n",
      "\n",
      "## Experiments\n",
      "### Correlation Analysis: ECS vs Hallucination\n",
      "- What varied: Comparing ECS values between truthful and hallucinated responses across layers and attention heads\n",
      "- Metric: Pearson Correlation Coefficient between inverse hallucination label and ECS\n",
      "- Main result: All attention heads exhibit negative correlations; hallucinated responses utilize less external context than truthful ones.\n",
      "\n",
      "### Correlation Analysis: PKS vs Hallucination\n",
      "- What varied: Comparing PKS values between truthful and hallucinated responses across FFN layers\n",
      "- Metric: Pearson correlation between hallucination labels and PKS\n",
      "- Main result: Later-layer FFNs exhibit substantially higher PKS for hallucinated responses and are positively correlated with hallucinations.\n",
      "\n",
      "### Classifier Training and Selection\n",
      "- What varied: Four classifier types: Logistic Regression, SVC, Random Forest, XGBoost trained on 7,799 span-level samples\n",
      "- Metric: Validation F1 score, precision, and recall at span level\n",
      "- Main result: SVC achieved highest validation F1 (76.60%) and was selected; XGBoost overfitted despite strong training performance.\n",
      "\n",
      "### Self-Evaluation Detection\n",
      "- What varied: Comparing proposed method against baselines (LLMs and commercial tools) on Qwen3-0.6b generated responses\n",
      "- Metric: Response-level Precision, Recall, F1\n",
      "- Main result: Method achieved F1=74.68%, outperforming TruLens (67.32%) and llama-3.1-8b-instant (57.53%), comparable to RefChecker (75.86%).\n",
      "\n",
      "### Proxy-Based Evaluation Detection\n",
      "- What varied: Applying Qwen3-0.6b trained classifier to GPT-4.1-mini responses against same baselines\n",
      "- Metric: Response-level Precision, Recall, F1\n",
      "- Main result: Method achieved F1=75.36%, outperforming nearly all models except GPT-5 (76.92%) and RAGAS (76.19%), using only 0.6b parameter signals.\n"
     ]
    }
   ],
   "source": [
    "# Read the plan to understand the research\n",
    "with open(f'{repo_path}/plan.md', 'r') as f:\n",
    "    plan_content = f.read()\n",
    "print(plan_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331bbabb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python scripts:\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_refchecker.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_hf.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_ragas.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_groq.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_trulens.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/baseline/run_gpt.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/predict.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/generate_response_hf.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/preprocess.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/helper.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/filter.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/generate_response_gpt.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/preprocess/generate_labels.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/compute_scores.py\n",
      "/net/scratch2/smallyan/InterpDetect_eval/scripts/classifier.py\n",
      "\n",
      "\n",
      "Scripts directory:\n",
      "total 120\n",
      "drwxrwx--- 6 smallyan smallyan    10 Dec 24 20:47 .\n",
      "drwxrwx--- 7 smallyan smallyan    13 Jan  7 14:53 ..\n",
      "drwxrwx--- 2 smallyan smallyan     9 Dec 22 21:55 baseline\n",
      "-rw-rw---- 1 smallyan smallyan 11368 Dec 22 21:55 classifier.py\n",
      "-rw-rw---- 1 smallyan smallyan 17630 Dec 22 21:55 compute_scores.py\n",
      "-rw-rw---- 1 smallyan smallyan  6148 Dec 22 21:55 .DS_Store\n",
      "drwxrwx--- 2 smallyan smallyan     3 Dec 22 21:55 plots\n",
      "-rw-rw---- 1 smallyan smallyan  9182 Dec 22 21:55 predict.py\n",
      "drwxrwx--- 4 smallyan smallyan    11 Dec 24 20:46 preprocess\n",
      "drwxrwx--- 2 smallyan smallyan     5 Dec 24 20:49 __pycache__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the scripts directory to understand the implementation\n",
    "result = subprocess.run(['find', f'{repo_path}/scripts', '-type', 'f', '-name', '*.py'], capture_output=True, text=True)\n",
    "print(\"Python scripts:\")\n",
    "print(result.stdout)\n",
    "\n",
    "# Also list the directories\n",
    "result2 = subprocess.run(['ls', '-la', f'{repo_path}/scripts'], capture_output=True, text=True)\n",
    "print(\"\\nScripts directory:\")\n",
    "print(result2.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1faf70c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# %%\n",
      "#!pip install transformer_lens\n",
      "\n",
      "import torch\n",
      "from transformers import AutoTokenizer\n",
      "from transformer_lens import HookedTransformer\n",
      "import json\n",
      "from torch.nn import functional as F\n",
      "from typing import Dict, List, Tuple\n",
      "import pdb\n",
      "from sentence_transformers import SentenceTransformer\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import argparse\n",
      "import sys\n",
      "import os\n",
      "import gc\n",
      "from tqdm import tqdm\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy.stats import pointbiserialr\n",
      "\n",
      "def load_examples(file_path):\n",
      "    \"\"\"Load examples from JSONL file\"\"\"\n",
      "    print(f\"Loading examples from {file_path}...\")\n",
      "    \n",
      "    try:\n",
      "        examples = []\n",
      "        with open(file_path, 'r') as f:\n",
      "            for line in f:\n",
      "                data = json.loads(line)\n",
      "                examples.append(data)\n",
      "        \n",
      "        print(f\"Loaded {len(examples)} examples\")\n",
      "        return examples\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading examples: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def setup_models(model_name, hf_model_name, device=\"cuda\"):\n",
      "    \"\"\"Setup tokenizer, model, and sentence transformer\"\"\"\n",
      "    print(f\"Setting up models: {model_name}, {hf_model_name}\")\n",
      "    \n",
      "    try:\n",
      "        tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
      "        \n",
      "        model = HookedTransformer.from_pretrained(\n",
      "            model_name,\n",
      "            device=\"cpu\",\n",
      "            torch_dtype=torch.float16\n",
      "        )\n",
      "        model.to(device)\n",
      "        \n",
      "        bge_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\").to(device)\n",
      "        \n",
      "        return tokenizer, model, bge_model\n",
      "    except Exception as e:\n",
      "        print(f\"Error setting up models: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def calculate_dist_2d(sep_vocabulary_dist, sep_attention_dist):\n",
      "    \"\"\"Calculate Jensen-Shannon divergence between distributions\"\"\"\n",
      "    # Calculate softmax\n",
      "    softmax_mature_layer = F.softmax(sep_vocabulary_dist, dim=-1)\n",
      "    softmax_anchor_layer = F.softmax(sep_attention_dist, dim=-1)\n",
      "\n",
      "    # Calculate the average distribution M\n",
      "    M = 0.5 * (softmax_mature_layer + softmax_anchor_layer)\n",
      "\n",
      "    # Calculate log-softmax for the KL divergence\n",
      "    log_softmax_mature_layer = F.log_softmax(sep_vocabulary_dist, dim=-1)\n",
      "    log_softmax_anchor_layer = F.log_softmax(sep_attention_dist, dim=-1)\n",
      "\n",
      "    # Calculate the KL divergences and then the JS divergences\n",
      "    kl1 = F.kl_div(log_softmax_mature_layer, M, reduction='none').sum(dim=-1)\n",
      "    kl2 = F.kl_div(log_softmax_anchor_layer, M, reduction='none').sum(dim=-1)\n",
      "    js_divs = 0.5 * (kl1 + kl2)\n",
      "\n",
      "    scores = js_divs.cpu().tolist()\n",
      "    return sum(scores)\n",
      "\n",
      "def add_special_template(tokenizer, prompt):\n",
      "    \"\"\"Add special template to prompt\"\"\"\n",
      "    messages = [\n",
      "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "        {\"role\": \"user\", \"content\": prompt}\n",
      "    ]\n",
      "    text = tokenizer.apply_chat_template(\n",
      "        messages,\n",
      "        tokenize=False,\n",
      "        add_generation_prompt=True,\n",
      "    )\n",
      "    return text\n",
      "\n",
      "def is_hallucination_span(r_span, hallucination_spans):\n",
      "    \"\"\"Check if a span contains hallucination\"\"\"\n",
      "    for token_id in range(r_span[0], r_span[1]):\n",
      "        for span in hallucination_spans:\n",
      "            if token_id >= span[0] and token_id <= span[1]:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "def calculate_hallucination_spans(response, text, response_rag, tokenizer, prefix_len):\n",
      "    \"\"\"Calculate hallucination spans\"\"\"\n",
      "    hallucination_span = []\n",
      "    for item in response:\n",
      "        start_id = item['start']\n",
      "        end_id = item['end']\n",
      "        start_text = text + response_rag[:start_id]\n",
      "        end_text = text + response_rag[:end_id]\n",
      "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
      "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
      "        start_id = start_text_id.shape[-1]\n",
      "        end_id = end_text_id.shape[-1]\n",
      "        hallucination_span.append([start_id, end_id])\n",
      "    return hallucination_span\n",
      "\n",
      "def calculate_respond_spans(raw_response_spans, text, response_rag, tokenizer):\n",
      "    \"\"\"Calculate response spans\"\"\"\n",
      "    respond_spans = []\n",
      "    for item in raw_response_spans:\n",
      "        start_id = item[0]\n",
      "        end_id = item[1]\n",
      "        start_text = text + response_rag[:start_id]\n",
      "        end_text = text + response_rag[:end_id]\n",
      "        start_text_id = tokenizer(start_text, return_tensors=\"pt\").input_ids\n",
      "        end_text_id = tokenizer(end_text, return_tensors=\"pt\").input_ids\n",
      "        start_id = start_text_id.shape[-1]\n",
      "        end_id = end_text_id.shape[-1]\n",
      "        respond_spans.append([start_id, end_id])\n",
      "    return respond_spans\n",
      "\n",
      "def calculate_prompt_spans(raw_prompt_spans, prompt, tokenizer):\n",
      "    \"\"\"Calculate prompt spans\"\"\"\n",
      "    prompt_spans = []\n",
      "    for item in raw_prompt_spans:\n",
      "        start_id = item[0]\n",
      "        end_id = item[1]\n",
      "        start_text = prompt[:start_id]\n",
      "        end_text = prompt[:end_id]\n",
      "        added_start_text = add_special_template(tokenizer, start_text)\n",
      "        added_end_text = add_special_template(tokenizer, end_text)\n",
      "        start_text_id = tokenizer(added_start_text, return_tensors=\"pt\").input_ids.shape[-1] - 4\n",
      "        end_text_id = tokenizer(added_end_text, return_tensors=\"pt\").input_ids.shape[-1] - 4\n",
      "        prompt_spans.append([start_text_id, end_text_id])\n",
      "    return prompt_spans\n",
      "\n",
      "def calculate_sentence_similarity(bge_model, r_text, p_text):\n",
      "    \"\"\"Calculate sentence similarity using BGE model\"\"\"\n",
      "    part_embedding = bge_model.encode([r_text], normalize_embeddings=True)\n",
      "    q_embeddings = bge_model.encode([p_text], normalize_embeddings=True)\n",
      "    \n",
      "    # Calculate similarity score\n",
      "    scores_named = np.matmul(q_embeddings, part_embedding.T).flatten()\n",
      "    return float(scores_named[0])\n",
      "\n",
      "class MockOutputs:\n",
      "    \"\"\"Mock outputs class for transformer lens compatibility\"\"\"\n",
      "    def __init__(self, cache, model_cfg):\n",
      "        self.cache = cache\n",
      "        self.model_cfg = model_cfg\n",
      "\n",
      "    @property\n",
      "    def attentions(self):\n",
      "        # Return attention patterns in the expected format\n",
      "        attentions = []\n",
      "        for layer in range(self.model_cfg.n_layers):\n",
      "            # Get attention pattern: [batch, n_heads, seq_len, seq_len]\n",
      "            attn_pattern = self.cache[f\"blocks.{layer}.attn.hook_pattern\"]\n",
      "            attentions.append(attn_pattern)\n",
      "        return tuple(attentions)\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        if key == \"hidden_states\":\n",
      "            # Return hidden states from all layers (residual stream after each layer)\n",
      "            hidden_states = []\n",
      "            for layer in range(self.model_cfg.n_layers):\n",
      "                hidden_state = self.cache[f\"blocks.{layer}.hook_resid_post\"]\n",
      "                hidden_states.append(hidden_state)\n",
      "            return tuple(hidden_states)\n",
      "        elif key == \"logits\":\n",
      "            return logits\n",
      "        else:\n",
      "            raise KeyError(f\"Key {key} not found\")\n",
      "\n",
      "def process_example(example, tokenizer, model, bge_model, device, max_ctx, iter_step=1):\n",
      "    \"\"\"Process a single example to compute scores\"\"\"\n",
      "    response_rag = example['response']\n",
      "    prompt = example['prompt']\n",
      "    original_prompt_spans = example['prompt_spans']\n",
      "    original_response_spans = example['response_spans']\n",
      "\n",
      "    text = add_special_template(tokenizer, prompt)\n",
      "\n",
      "    prompt_ids = tokenizer([text], return_tensors=\"pt\").input_ids\n",
      "    response_ids = tokenizer([response_rag], return_tensors=\"pt\").input_ids\n",
      "    input_ids = torch.cat([prompt_ids, response_ids[:, 1:]], dim=1)\n",
      "\n",
      "    if input_ids.shape[-1] > max_ctx:\n",
      "        overflow = input_ids.shape[-1] - max_ctx\n",
      "        input_ids = input_ids[:, overflow:]\n",
      "        prompt_kept = max(prompt_ids.shape[-1] - overflow, 0)\n",
      "    else:\n",
      "        prompt_kept = prompt_ids.shape[-1]\n",
      "\n",
      "    input_ids = input_ids.to(device)\n",
      "    prefix_len = prompt_kept\n",
      "\n",
      "    if \"labels\" in example.keys():\n",
      "        hallucination_spans = calculate_hallucination_spans(example['labels'], text, response_rag, tokenizer, prefix_len)\n",
      "    else:\n",
      "        hallucination_spans = []\n",
      "\n",
      "    prompt_spans = calculate_prompt_spans(example['prompt_spans'], prompt, tokenizer)\n",
      "    respond_spans = calculate_respond_spans(example['response_spans'], text, response_rag, tokenizer)\n",
      "\n",
      "    # Run model with cache to get all intermediate activations\n",
      "    logits, cache = model.run_with_cache(\n",
      "        input_ids,\n",
      "        return_type=\"logits\"\n",
      "    )\n",
      "\n",
      "    outputs = MockOutputs(cache, model.cfg)\n",
      "\n",
      "    # skip tokens without hallucination\n",
      "    hidden_states = outputs[\"hidden_states\"]\n",
      "    last_hidden_states = hidden_states[-1][0, :, :]\n",
      "    del hidden_states\n",
      "\n",
      "    span_score_dict = []\n",
      "    for r_id, r_span in enumerate(respond_spans):\n",
      "        layer_head_span = {}\n",
      "        parameter_knowledge_dict = {}\n",
      "        for attentions_layer_id in range(0, model.cfg.n_layers, iter_step):\n",
      "            for head_id in range(model.cfg.n_heads):\n",
      "                layer_head = (attentions_layer_id, head_id)\n",
      "                p_span_score_dict = []\n",
      "                for p_span in prompt_spans:\n",
      "                    attention_score = outputs.attentions[attentions_layer_id][0, head_id, :, :]\n",
      "                    p_span_score_dict.append([p_span, torch.sum(attention_score[r_span[0]:r_span[1], p_span[0]:p_span[1]]).cpu().item()])\n",
      "                \n",
      "                # Get the span with maximum score\n",
      "                p_id = max(range(len(p_span_score_dict)), key=lambda i: p_span_score_dict[i][1])\n",
      "                prompt_span_text = prompt[original_prompt_spans[p_id][0]:original_prompt_spans[p_id][1]]\n",
      "                respond_span_text = response_rag[original_response_spans[r_id][0]:original_response_spans[r_id][1]]\n",
      "                layer_head_span[str(layer_head)] = calculate_sentence_similarity(bge_model, prompt_span_text, respond_span_text)\n",
      "\n",
      "            x_mid = cache[f\"blocks.{attentions_layer_id}.hook_resid_mid\"][0, r_span[0]:r_span[1], :]\n",
      "            x_post = cache[f\"blocks.{attentions_layer_id}.hook_resid_post\"][0, r_span[0]:r_span[1], :]\n",
      "\n",
      "            score = calculate_dist_2d(\n",
      "                x_mid @ model.W_U,\n",
      "                x_post @ model.W_U\n",
      "            )\n",
      "            parameter_knowledge_dict[f\"layer_{attentions_layer_id}\"] = score\n",
      "\n",
      "        span_score_dict.append({\n",
      "            \"prompt_attention_score\": layer_head_span,\n",
      "            \"r_span\": r_span,\n",
      "            \"hallucination_label\": 1 if is_hallucination_span(r_span, hallucination_spans) else 0,\n",
      "            \"parameter_knowledge_scores\": parameter_knowledge_dict\n",
      "        })\n",
      "\n",
      "    example[\"scores\"] = span_score_dict\n",
      "    return example\n",
      "\n",
      "def save_batch(select_response, batch_num, save_dir):\n",
      "    \"\"\"Save a batch of processed examples\"\"\"\n",
      "    save_path = os.path.join(save_dir, f\"train3000_w_chunk_score_part{batch_num}.json\")\n",
      "    with open(save_path, \"w\") as f:\n",
      "        json.dump(select_response, f, ensure_ascii=False)\n",
      "    print(f\"Saved batch {batch_num} to {save_path}\")\n",
      "\n",
      "def plot_binary_correlation(numerical_values, binary_labels, title=\"Correlation with Binary Label\"):\n",
      "    \"\"\"Plot correlation between numerical values and binary labels\"\"\"\n",
      "    assert len(numerical_values) == len(binary_labels), \"Lists must be the same length\"\n",
      "\n",
      "    numerical_values = np.array(numerical_values)\n",
      "    binary_labels = np.array(binary_labels)\n",
      "\n",
      "    # Compute correlation\n",
      "    corr, p_val = pointbiserialr(binary_labels, numerical_values)\n",
      "\n",
      "    # Plot\n",
      "    plt.figure(figsize=(8, 3))\n",
      "\n",
      "    # Scatter plot\n",
      "    plt.subplot(1, 2, 1)\n",
      "    sns.stripplot(x=binary_labels, y=numerical_values, jitter=True, alpha=0.7)\n",
      "    plt.title(f\"Scatter Plot\\nPoint-Biserial Correlation = {corr:.2f} (p={p_val:.2e})\")\n",
      "    plt.xlabel(\"Binary Label (0/1)\")\n",
      "    plt.ylabel(\"Numerical Value\")\n",
      "\n",
      "    # Boxplot\n",
      "    plt.subplot(1, 2, 2)\n",
      "    sns.boxplot(x=binary_labels, y=numerical_values)\n",
      "    plt.title(\"Boxplot by Binary Class\")\n",
      "    plt.xlabel(\"Binary Label (0/1)\")\n",
      "    plt.ylabel(\"Numerical Value\")\n",
      "\n",
      "    plt.suptitle(title)\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      "def analyze_scores(select_response, save_plots=False, plots_dir=\"plots\"):\n",
      "    \"\"\"Analyze computed scores and create visualizations\"\"\"\n",
      "    print(\"Analyzing scores...\")\n",
      "    \n",
      "    prompt_attention_scores = []\n",
      "    hallucination_labels = []\n",
      "    parameter_knowledge_scores = []\n",
      "    ratios = []\n",
      "\n",
      "    for item in select_response:\n",
      "        scores = item['scores']\n",
      "        for score in scores:\n",
      "            pas_sum = sum(score['prompt_attention_score'].values())\n",
      "            pks_sum = sum(score['parameter_knowledge_scores'].values())\n",
      "            prompt_attention_scores.append(pas_sum)\n",
      "            parameter_knowledge_scores.append(pks_sum)\n",
      "            ratios.append(pks_sum / pas_sum if pas_sum > 0 else 0)\n",
      "            hallucination_labels.append(score['hallucination_label'])\n",
      "\n",
      "    # Create plots\n",
      "    if save_plots:\n",
      "        os.makedirs(plots_dir, exist_ok=True)\n",
      "        \n",
      "        plt.figure(figsize=(15, 5))\n",
      "        \n",
      "        # Plot 1: Prompt Attention Scores\n",
      "        plt.subplot(1, 3, 1)\n",
      "        plot_binary_correlation(prompt_attention_scores, hallucination_labels, \"Correlation with ECS Score\")\n",
      "        \n",
      "        # Plot 2: Parameter Knowledge Scores\n",
      "        plt.subplot(1, 3, 2)\n",
      "        plot_binary_correlation(parameter_knowledge_scores, hallucination_labels, \"Correlation with PKS Score\")\n",
      "        \n",
      "        # Plot 3: Ratio Scores\n",
      "        plt.subplot(1, 3, 3)\n",
      "        plot_binary_correlation(ratios, hallucination_labels, \"Correlation with Ratio Score\")\n",
      "        \n",
      "        plt.savefig(os.path.join(plots_dir, \"score_analysis.png\"), dpi=300, bbox_inches='tight')\n",
      "        plt.close()\n",
      "    \n",
      "    # Print statistics\n",
      "    print(f\"Score ranges:\")\n",
      "    print(f\"Prompt attention scores: {min(prompt_attention_scores):.4f} - {max(prompt_attention_scores):.4f}\")\n",
      "    print(f\"Parameter knowledge scores: {min(parameter_knowledge_scores):.4f} - {max(parameter_knowledge_scores):.4f}\")\n",
      "    print(f\"Ratios: {min(ratios):.4f} - {max(ratios):.4f}\")\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to run the score computation pipeline\"\"\"\n",
      "    parser = argparse.ArgumentParser(description='Compute interpretability scores for hallucination detection')\n",
      "    parser.add_argument('--input_path', type=str, \n",
      "                       default=\"preprocess/datasets/train/train3000_w_labels_filtered.jsonl\",\n",
      "                       help='Path to input dataset')\n",
      "    parser.add_argument('--output_dir', type=str, \n",
      "                       default=\"../datasets/train\",\n",
      "                       help='Output directory for computed scores')\n",
      "    parser.add_argument('--model_name', type=str,\n",
      "                       default=\"qwen3-0.6b\",\n",
      "                       help='TransformerLens model name')\n",
      "    parser.add_argument('--hf_model_name', type=str,\n",
      "                       default=\"Qwen/Qwen3-0.6B\",\n",
      "                       help='HuggingFace model name')\n",
      "    parser.add_argument('--device', type=str,\n",
      "                       default=\"cuda\",\n",
      "                       help='Device to run models on')\n",
      "    parser.add_argument('--batch_size', type=int,\n",
      "                       default=100,\n",
      "                       help='Batch size for processing')\n",
      "    parser.add_argument('--iter_step', type=int,\n",
      "                       default=1,\n",
      "                       help='Step size for layer iteration')\n",
      "    parser.add_argument('--save_plots', action='store_true',\n",
      "                       help='Save analysis plots')\n",
      "    parser.add_argument('--plots_dir', type=str,\n",
      "                       default=\"plots\",\n",
      "                       help='Directory to save plots')\n",
      "    parser.add_argument('--verbose', action='store_true',\n",
      "                       help='Enable verbose output')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    if args.verbose:\n",
      "        print(\"Starting score computation pipeline...\")\n",
      "    \n",
      "    # Load examples\n",
      "    examples = load_examples(args.input_path)\n",
      "    \n",
      "    # Setup models\n",
      "    tokenizer, model, bge_model = setup_models(args.model_name, args.hf_model_name, args.device)\n",
      "    \n",
      "    # Set model to evaluation mode\n",
      "    model.eval()\n",
      "    torch.set_grad_enabled(False)\n",
      "    \n",
      "    max_ctx = model.cfg.n_ctx\n",
      "    select_response = []\n",
      "    \n",
      "    # Process examples\n",
      "    for i in tqdm(range(len(examples)), desc=\"Processing examples\"):\n",
      "        try:\n",
      "            example = process_example(\n",
      "                examples[i], tokenizer, model, bge_model, \n",
      "                args.device, max_ctx, args.iter_step\n",
      "            )\n",
      "            select_response.append(example)\n",
      "            \n",
      "            # Save batch if needed\n",
      "            if (i + 1) % args.batch_size == 0:\n",
      "                batch_num = i // args.batch_size\n",
      "                save_batch(select_response, batch_num, args.output_dir)\n",
      "                select_response = []\n",
      "                \n",
      "        except Exception as e:\n",
      "            print(f\"Error processing example {i}: {e}\")\n",
      "            continue\n",
      "        \n",
      "        # Clean up memory\n",
      "        if i % args.batch_size == 0:\n",
      "            for name in [\n",
      "                \"input_ids\", \"logits\", \"cache\", \"outputs\", \"logits_dict\",\n",
      "                \"last_hidden_states\", \"attention_score\",\n",
      "                \"parameter_knowledge_scores\", \"parameter_knowledge_dict\",\n",
      "            ]:\n",
      "                if name in locals():\n",
      "                    try:\n",
      "                        del locals()[name]\n",
      "                    except Exception:\n",
      "                        pass\n",
      "            \n",
      "            gc.collect()\n",
      "            if torch.cuda.is_available():\n",
      "                torch.cuda.empty_cache()\n",
      "                try:\n",
      "                    torch.cuda.ipc_collect()\n",
      "                except Exception:\n",
      "                    pass\n",
      "    \n",
      "    # Save remaining examples\n",
      "    if select_response:\n",
      "        batch_num = len(examples) // args.batch_size\n",
      "        save_batch(select_response, batch_num, args.output_dir)\n",
      "    \n",
      "    # Analyze scores if requested\n",
      "    if args.save_plots and select_response:\n",
      "        analyze_scores(select_response, args.save_plots, args.plots_dir)\n",
      "    \n",
      "    print(\"Score computation completed successfully!\")\n",
      "    \n",
      "    return select_response\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    results = main()\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the compute_scores.py to understand how ECS and PKS are computed\n",
    "with open(f'{repo_path}/scripts/compute_scores.py', 'r') as f:\n",
    "    compute_scores_content = f.read()\n",
    "print(compute_scores_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c298e14b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets directory:\n",
      "total 43\n",
      "drwxrwx--- 4 smallyan smallyan    5 Dec 22 21:55 .\n",
      "drwxrwx--- 7 smallyan smallyan   13 Jan  7 14:53 ..\n",
      "-rw-rw---- 1 smallyan smallyan 9283 Dec 22 21:55 OV_copying_score.json\n",
      "drwxrwx--- 2 smallyan smallyan    4 Dec 22 21:55 test\n",
      "drwxrwx--- 2 smallyan smallyan   20 Dec 22 21:55 train\n",
      "\n",
      "\n",
      "Trained models directory:\n",
      "total 15236\n",
      "drwxrwx--- 2 smallyan smallyan        6 Dec 22 21:55 .\n",
      "drwxrwx--- 7 smallyan smallyan       13 Jan  7 14:53 ..\n",
      "-rw-rw---- 1 smallyan smallyan    21327 Dec 22 21:55 model_LR_3000.pickle\n",
      "-rw-rw---- 1 smallyan smallyan   550685 Dec 22 21:55 model_RandomForest_3000.pickle\n",
      "-rw-rw---- 1 smallyan smallyan 15123341 Dec 22 21:55 model_SVC_3000.pickle\n",
      "-rw-rw---- 1 smallyan smallyan   275818 Dec 22 21:55 model_XGBoost_3000.pickle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the datasets and trained models\n",
    "result = subprocess.run(['ls', '-la', f'{repo_path}/datasets'], capture_output=True, text=True)\n",
    "print(\"Datasets directory:\")\n",
    "print(result.stdout)\n",
    "\n",
    "result2 = subprocess.run(['ls', '-la', f'{repo_path}/trained_models'], capture_output=True, text=True)\n",
    "print(\"\\nTrained models directory:\")\n",
    "print(result2.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5adc1f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets:\n",
      "total 41710\n",
      "drwxrwx--- 2 smallyan smallyan      20 Dec 22 21:55 .\n",
      "drwxrwx--- 4 smallyan smallyan       5 Dec 22 21:55 ..\n",
      "-rw-rw---- 1 smallyan smallyan 7480050 Dec 22 21:55 train3000_w_chunk_score_part0.json\n",
      "-rw-rw---- 1 smallyan smallyan 8320169 Dec 22 21:55 train3000_w_chunk_score_part10.json\n",
      "-rw-rw---- 1 smallyan smallyan 7572237 Dec 22 21:55 train3000_w_chunk_score_part11.json\n",
      "-rw-rw---- 1 smallyan smallyan 7261920 Dec 22 21:55 train3000_w_chunk_score_part12.json\n",
      "-rw-rw---- 1 smallyan smallyan 7490607 Dec 22 21:55 train3000_w_chunk_score_part13.json\n",
      "-rw-rw---- 1 smallyan smallyan 7789460 Dec 22 21:55 train3000_w_chunk_score_part14.json\n",
      "-rw-rw---- 1 smallyan smallyan 7444065 Dec 22 21:55 train3000_w_chunk_score_part15.json\n",
      "-rw-rw---- 1 smallyan smallyan 7850314 Dec 22 21:55 train3000_w_chunk_score_part16.json\n",
      "-rw-rw---- 1 smallyan smallyan 7124779 Dec 22 21:55 train3000_w_chunk_score_part17.json\n",
      "-rw-rw---- 1 smallyan smallyan 7783995 Dec 22 21:55 train3000_w_chunk_score_part1.json\n",
      "-rw-rw---- 1 smallyan smallyan 7652154 Dec 22 21:55 train3000_w_chunk_score_part2.json\n",
      "-rw-rw---- 1 smallyan smallyan 8110518 Dec 22 21:55 train3000_w_chunk_score_part3.json\n",
      "-rw-rw---- 1 smallyan smallyan 7297703 Dec 22 21:55 train3000_w_chunk_score_part4.json\n",
      "-rw-rw---- 1 smallyan smallyan 7350956 Dec 22 21:55 train3000_w_chunk_score_part5.json\n",
      "-rw-rw---- 1 smallyan smallyan 7856612 Dec 22 21:55 train3000_w_chunk_score_part6.json\n",
      "-rw-rw---- 1 smallyan smallyan 7669627 Dec 22 21:55 train3000_w_chunk_score_part7.json\n",
      "-rw-rw---- 1 smallyan smallyan 7778944 Dec 22 21:55 train3000_w_chunk_score_part8.json\n",
      "-rw-rw---- 1 smallyan smallyan 7687186 Dec 22 21:55 train3000_w_chunk_score_part9.json\n",
      "\n",
      "\n",
      "Test datasets:\n",
      "total 9679\n",
      "drwxrwx--- 2 smallyan smallyan        4 Dec 22 21:55 .\n",
      "drwxrwx--- 4 smallyan smallyan        5 Dec 22 21:55 ..\n",
      "-rw-rw---- 1 smallyan smallyan 14312423 Dec 22 21:55 test_w_chunk_score_gpt41mini.json\n",
      "-rw-rw---- 1 smallyan smallyan 17477232 Dec 22 21:55 test_w_chunk_score_qwen06b.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore datasets - train and test\n",
    "result = subprocess.run(['ls', '-la', f'{repo_path}/datasets/train'], capture_output=True, text=True)\n",
    "print(\"Train datasets:\")\n",
    "print(result.stdout)\n",
    "\n",
    "result2 = subprocess.run(['ls', '-la', f'{repo_path}/datasets/test'], capture_output=True, text=True)\n",
    "print(\"\\nTest datasets:\")\n",
    "print(result2.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32459c54",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# %%\n",
      "# !pip install feature_engine\n",
      "# !pip install xgboost\n",
      "# !pip install lightgbm\n",
      "# !pip install optuna\n",
      "# !pip install --upgrade scikit-learn\n",
      "# !pip install unidecode\n",
      "\n",
      "import pandas as pd\n",
      "import json\n",
      "import numpy as np\n",
      "import os\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "import pickle\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "from tqdm import tqdm\n",
      "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
      "import argparse\n",
      "import sys\n",
      "\n",
      "def load_data(folder_path):\n",
      "    \"\"\"Load data from JSON files in the specified folder\"\"\"\n",
      "    print(f\"Loading data from {folder_path}...\")\n",
      "    \n",
      "    try:\n",
      "        response = []\n",
      "        json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
      "        \n",
      "        if not json_files:\n",
      "            print(f\"No JSON files found in {folder_path}\")\n",
      "            sys.exit(1)\n",
      "        \n",
      "        for file_path in json_files:\n",
      "            with open(file_path, \"r\") as f:\n",
      "                data = json.load(f)\n",
      "                response.extend(data)\n",
      "        \n",
      "        print(f\"Loaded {len(response)} examples from {len(json_files)} files\")\n",
      "        return response\n",
      "    except Exception as e:\n",
      "        print(f\"Error loading data: {e}\")\n",
      "        sys.exit(1)\n",
      "\n",
      "def preprocess_data(response, balance_classes=True, random_state=42):\n",
      "    \"\"\"Preprocess the loaded data into a DataFrame\"\"\"\n",
      "    print(\"Preprocessing data...\")\n",
      "    \n",
      "    if not response:\n",
      "        print(\"No data to preprocess\")\n",
      "        sys.exit(1)\n",
      "    \n",
      "    # Get column names from first example\n",
      "    ATTENTION_COLS = response[0]['scores'][0]['prompt_attention_score'].keys()\n",
      "    PARAMETER_COLS = response[0]['scores'][0]['parameter_knowledge_scores'].keys()\n",
      "    \n",
      "    data_dict = {\n",
      "        \"identifier\": [],\n",
      "        **{col: [] for col in ATTENTION_COLS},\n",
      "        **{col: [] for col in PARAMETER_COLS},\n",
      "        \"hallucination_label\": []\n",
      "    }\n",
      "    \n",
      "    for i, resp in enumerate(response):\n",
      "        for j in range(len(resp[\"scores\"])):\n",
      "            data_dict[\"identifier\"].append(f\"response_{i}_item_{j}\")\n",
      "            for col in ATTENTION_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['prompt_attention_score'][col])\n",
      "            \n",
      "            for col in PARAMETER_COLS:\n",
      "                data_dict[col].append(resp[\"scores\"][j]['parameter_knowledge_scores'][col])\n",
      "            data_dict[\"hallucination_label\"].append(resp[\"scores\"][j][\"hallucination_label\"])\n",
      "    \n",
      "    df = pd.DataFrame(data_dict)\n",
      "    \n",
      "    print(f\"Created DataFrame with {len(df)} samples\")\n",
      "    print(f\"Class distribution: {df['hallucination_label'].value_counts().to_dict()}\")\n",
      "    \n",
      "    # Balance classes if requested\n",
      "    if balance_classes:\n",
      "        min_count = df['hallucination_label'].value_counts().min()\n",
      "        df = (\n",
      "            df.groupby('hallucination_label', group_keys=False)\n",
      "              .apply(lambda x: x.sample(min_count, random_state=random_state))\n",
      "        )\n",
      "        print(f\"After balancing: {df['hallucination_label'].value_counts().to_dict()}\")\n",
      "    \n",
      "    return df, list(ATTENTION_COLS), list(PARAMETER_COLS)\n",
      "\n",
      "def split_data(df, test_size=0.1, random_state=42):\n",
      "    \"\"\"Split data into train and validation sets\"\"\"\n",
      "    print(\"Splitting data into train and validation sets...\")\n",
      "    \n",
      "    train, val = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df['hallucination_label'])\n",
      "    \n",
      "    features = [col for col in df.columns if col not in ['identifier', 'hallucination_label']]\n",
      "    \n",
      "    X_train = train[features]\n",
      "    y_train = train[\"hallucination_label\"]\n",
      "    X_val = val[features]\n",
      "    y_val = val[\"hallucination_label\"]\n",
      "    \n",
      "    print(f\"Train set: {len(X_train)} samples\")\n",
      "    print(f\"Validation set: {len(X_val)} samples\")\n",
      "    print(f\"Number of features: {len(features)}\")\n",
      "    \n",
      "    return X_train, X_val, y_train, y_val, features\n",
      "\n",
      "def create_preprocessor(use_feature_selection=False):\n",
      "    \"\"\"Create preprocessing pipeline\"\"\"\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "    from feature_engine.selection import DropConstantFeatures, SmartCorrelatedSelection, DropDuplicateFeatures\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    from sklearn.pipeline import Pipeline\n",
      "    \n",
      "    scaler = StandardScaler()\n",
      "    \n",
      "    if use_feature_selection:\n",
      "        drop_const = DropConstantFeatures(tol=0.95, missing_values='ignore')\n",
      "        drop_dup = DropDuplicateFeatures()\n",
      "        drop_corr = SmartCorrelatedSelection(\n",
      "            method='pearson', \n",
      "            threshold=0.90,\n",
      "            selection_method='model_performance',\n",
      "            estimator=RandomForestClassifier(max_depth=5, random_state=42)\n",
      "        )\n",
      "        \n",
      "        preprocessor = Pipeline([\n",
      "            ('scaler', scaler),\n",
      "            ('drop_constant', drop_const),\n",
      "            ('drop_duplicates', drop_dup),\n",
      "            ('smart_corr_selection', drop_corr),\n",
      "        ])\n",
      "    else:\n",
      "        preprocessor = Pipeline([\n",
      "            ('scaler', scaler),\n",
      "        ])\n",
      "    \n",
      "    return preprocessor\n",
      "\n",
      "def train_models(X_train, X_val, y_train, y_val, preprocessor, models_to_train=None):\n",
      "    \"\"\"Train multiple models and compare their performance\"\"\"\n",
      "    print(\"Training models...\")\n",
      "    \n",
      "    from sklearn.pipeline import make_pipeline\n",
      "    from sklearn.metrics import precision_recall_fscore_support\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from xgboost import XGBClassifier\n",
      "    \n",
      "    # Define models to train\n",
      "    if models_to_train is None:\n",
      "        models_to_train = [\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"]\n",
      "    \n",
      "    models = []\n",
      "    if \"LR\" in models_to_train:\n",
      "        models.append((\"LR\", LogisticRegression()))\n",
      "    if \"SVC\" in models_to_train:\n",
      "        models.append(('SVC', SVC()))\n",
      "    if \"RandomForest\" in models_to_train:\n",
      "        models.append(('RandomForest', RandomForestClassifier(max_depth=5)))\n",
      "    if \"XGBoost\" in models_to_train:\n",
      "        models.append(('XGBoost', XGBClassifier(max_depth=5)))\n",
      "    \n",
      "    # Initialize lists for results\n",
      "    names = []\n",
      "    train_ps = []\n",
      "    train_rs = []\n",
      "    train_fs = []\n",
      "    val_ps = []\n",
      "    val_rs = []\n",
      "    val_fs = []\n",
      "    clfs = {}\n",
      "    \n",
      "    # Train each model\n",
      "    for name, model in models:\n",
      "        print(f\"Training {name}...\")\n",
      "        names.append(name)\n",
      "        clf = make_pipeline(preprocessor, model)\n",
      "        clf.fit(X_train, y_train)\n",
      "        \n",
      "        # Calculate metrics\n",
      "        tp, tr, tf, _ = precision_recall_fscore_support(y_train, clf.predict(X_train), average='binary')\n",
      "        train_ps.append(tp)\n",
      "        train_rs.append(tr)\n",
      "        train_fs.append(tf)\n",
      "        \n",
      "        vp, vr, vf, _ = precision_recall_fscore_support(y_val, clf.predict(X_val), average='binary')\n",
      "        val_ps.append(vp)\n",
      "        val_rs.append(vr)\n",
      "        val_fs.append(vf)\n",
      "        \n",
      "        clfs[name] = clf\n",
      "    \n",
      "    # Create comparison dataframe\n",
      "    model_comparison = pd.DataFrame({\n",
      "        'Algorithm': names,\n",
      "        'Train_p': train_ps,\n",
      "        'Val_p': val_ps,\n",
      "        'Train_r': train_rs,\n",
      "        'Val_r': val_rs,\n",
      "        'Train_f': train_fs,\n",
      "        'Val_f': val_fs,\n",
      "    })\n",
      "    \n",
      "    print(\"\\nModel Comparison:\")\n",
      "    print(model_comparison)\n",
      "    \n",
      "    return clfs, model_comparison\n",
      "\n",
      "def save_models(clfs, output_dir):\n",
      "    \"\"\"Save trained models\"\"\"\n",
      "    print(f\"Saving models to {output_dir}...\")\n",
      "    \n",
      "    os.makedirs(output_dir, exist_ok=True)\n",
      "    \n",
      "    for name, clf in clfs.items():\n",
      "        model_path = os.path.join(output_dir, f\"model_{name}_3000.pickle\")\n",
      "        with open(model_path, \"wb\") as fout:\n",
      "            pickle.dump(clf, fout)\n",
      "        print(f\"Saved {name} model to {model_path}\")\n",
      "\n",
      "\n",
      "\n",
      "def create_feature_importance_plot(clfs, X_train, output_dir):\n",
      "    \"\"\"Create feature importance plot for XGBoost model\"\"\"\n",
      "    print(\"Creating feature importance plot...\")\n",
      "    \n",
      "    if 'XGBoost' in clfs:\n",
      "        xgb_model = clfs['XGBoost']\n",
      "        feature_imp = pd.DataFrame(\n",
      "            sorted(zip(xgb_model.named_steps['xgbclassifier'].feature_importances_, X_train.columns)), \n",
      "            columns=['Value', 'Feature']\n",
      "        )\n",
      "        \n",
      "        plt.figure(figsize=(8, 6))\n",
      "        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:15])\n",
      "        plt.title('XGBoost Feature Importance')\n",
      "        plt.tight_layout()\n",
      "        \n",
      "        plot_path = os.path.join(output_dir, \"feature_importance.png\")\n",
      "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "        plt.close()\n",
      "        print(f\"Saved feature importance plot to {plot_path}\")\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to run the classifier training pipeline\"\"\"\n",
      "    parser = argparse.ArgumentParser(description='Train classifiers for hallucination detection')\n",
      "    parser.add_argument('--input_dir', type=str, \n",
      "                       default=\"../datasets/train\",\n",
      "                       help='Input directory containing JSON files with scores')\n",
      "    parser.add_argument('--output_dir', type=str, \n",
      "                       default=\"../trained_models\",\n",
      "                       help='Output directory for trained models')\n",
      "    parser.add_argument('--models', nargs='+',\n",
      "                       default=[\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"],\n",
      "                       choices=[\"LR\", \"SVC\", \"RandomForest\", \"XGBoost\"],\n",
      "                       help='Models to train')\n",
      "    parser.add_argument('--test_size', type=float,\n",
      "                       default=0.1,\n",
      "                       help='Test size for train/validation split')\n",
      "    parser.add_argument('--balance_classes', action='store_true',\n",
      "                       help='Balance classes by undersampling')\n",
      "    parser.add_argument('--use_feature_selection', action='store_true',\n",
      "                       help='Use feature selection in preprocessing')\n",
      "\n",
      "    parser.add_argument('--random_state', type=int,\n",
      "                       default=42,\n",
      "                       help='Random state for reproducibility')\n",
      "    parser.add_argument('--save_plots', action='store_true',\n",
      "                       help='Save feature importance plots')\n",
      "    parser.add_argument('--verbose', action='store_true',\n",
      "                       help='Enable verbose output')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    if args.verbose:\n",
      "        print(\"Starting classifier training pipeline...\")\n",
      "    \n",
      "    # Load data\n",
      "    response = load_data(args.input_dir)\n",
      "    \n",
      "    # Preprocess data\n",
      "    df, attention_cols, parameter_cols = preprocess_data(\n",
      "        response, args.balance_classes, args.random_state\n",
      "    )\n",
      "    \n",
      "    # Split data\n",
      "    X_train, X_val, y_train, y_val, features = split_data(\n",
      "        df, args.test_size, args.random_state\n",
      "    )\n",
      "    \n",
      "    # Create preprocessor\n",
      "    preprocessor = create_preprocessor(args.use_feature_selection)\n",
      "    \n",
      "    # Train models\n",
      "    clfs, model_comparison = train_models(\n",
      "        X_train, X_val, y_train, y_val, preprocessor, args.models\n",
      "    )\n",
      "    \n",
      "    # Save models\n",
      "    save_models(clfs, args.output_dir)\n",
      "    \n",
      "    # Create feature importance plot if requested\n",
      "    if args.save_plots:\n",
      "        create_feature_importance_plot(clfs, X_train, args.output_dir)\n",
      "    \n",
      "    print(\"Classifier training completed successfully!\")\n",
      "    \n",
      "    return {\n",
      "        'models': clfs,\n",
      "        'model_comparison': model_comparison,\n",
      "        'features': features\n",
      "    }\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    results = main()\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the classifier script to understand how the model is trained\n",
    "with open(f'{repo_path}/scripts/classifier.py', 'r') as f:\n",
    "    classifier_content = f.read()\n",
    "print(classifier_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3a921",
   "metadata": {},
   "source": [
    "## Summary of the Research\n",
    "\n",
    "The repository implements a **mechanistic interpretability-based hallucination detection method** for RAG systems:\n",
    "\n",
    "### Key Findings:\n",
    "1. **External Context Score (ECS)**: Computed per attention head by measuring how much the model attends to external context and uses it in responses. Lower ECS correlates with hallucinations.\n",
    "\n",
    "2. **Parametric Knowledge Score (PKS)**: Computed per FFN layer using Jensen-Shannon divergence between vocabulary distributions before and after the FFN layer. Higher PKS in later layers correlates with hallucinations.\n",
    "\n",
    "3. **Proxy Model Evaluation**: The method uses Qwen3-0.6B to extract signals but claims to generalize to larger models like GPT-4.1-mini.\n",
    "\n",
    "### Models Used in Original Work:\n",
    "- **Signal extraction model**: Qwen3-0.6B\n",
    "- **Response generation models tested**: Qwen3-0.6B (self-evaluation), GPT-4.1-mini (proxy evaluation)\n",
    "\n",
    "### Trained Classifiers:\n",
    "- Logistic Regression, SVC, Random Forest, XGBoost (SVC selected as best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a3032b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A40\n",
      "CUDA device count: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability and setup\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd8cf76",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples (Qwen): 256\n",
      "\n",
      "First example keys:\n",
      "dict_keys(['id', 'question', 'documents', 'documents_sentences', 'prompt', 'prompt_spans', 'num_tokens', 'response', 'response_spans', 'labels', 'hallucinated_llama-4-maverick-17b-128e-instruct', 'hallucinated_gpt-oss-120b', 'labels_llama', 'labels_gpt', 'scores'])\n",
      "\n",
      "First example structure:\n",
      "\n",
      "prompt: Given the context, please answer the question based on the provided information from the context. Include any reasoning with the answer\n",
      "\n",
      "Context:Stockholder return performance graph the following grap...\n",
      "\n",
      "response: The rate of return in Cadence Design Systems Inc. for an investment from 2010 to 2011 can be calculated by comparing the cumulative total return on the investment to the initial investment. Given that...\n",
      "\n",
      "scores:\n",
      "  Number of score spans: 5\n",
      "  First score span keys: dict_keys(['prompt_attention_score', 'r_span', 'hallucination_label', 'parameter_knowledge_scores'])\n",
      "  Hallucination label: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the test data to understand the data format\n",
    "import json\n",
    "\n",
    "with open(f'{repo_path}/datasets/test/test_w_chunk_score_qwen06b.json', 'r') as f:\n",
    "    test_data_qwen = json.load(f)\n",
    "\n",
    "print(f\"Number of test examples (Qwen): {len(test_data_qwen)}\")\n",
    "print(\"\\nFirst example keys:\")\n",
    "print(test_data_qwen[0].keys())\n",
    "print(\"\\nFirst example structure:\")\n",
    "for key in ['prompt', 'response', 'scores']:\n",
    "    if key == 'scores':\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Number of score spans: {len(test_data_qwen[0][key])}\")\n",
    "        print(f\"  First score span keys: {test_data_qwen[0][key][0].keys()}\")\n",
    "        print(f\"  Hallucination label: {test_data_qwen[0][key][0].get('hallucination_label', 'N/A')}\")\n",
    "    else:\n",
    "        val = test_data_qwen[0].get(key, 'N/A')\n",
    "        if isinstance(val, str) and len(val) > 200:\n",
    "            val = val[:200] + \"...\"\n",
    "        print(f\"\\n{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1608c2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SVC model\n",
      "Model type: <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries for evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "\n",
    "# Load the trained SVC model (best performing model according to the plan)\n",
    "model_path = f'{repo_path}/trained_models/model_SVC_3000.pickle'\n",
    "with open(model_path, 'rb') as f:\n",
    "    svc_model = pickle.load(f)\n",
    "\n",
    "print(\"Loaded SVC model\")\n",
    "print(f\"Model type: {type(svc_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e6da123",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "  pipeline: <class 'sklearn.pipeline.Pipeline'>\n",
      "  svc: <class 'sklearn.svm._classes.SVC'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of attention columns (ECS features): 448\n",
      "Number of parameter knowledge columns (PKS features): 28\n",
      "Total features: 476\n"
     ]
    }
   ],
   "source": [
    "# Let's look at what features the model expects\n",
    "print(\"Pipeline steps:\")\n",
    "for name, step in svc_model.named_steps.items():\n",
    "    print(f\"  {name}: {type(step)}\")\n",
    "\n",
    "# Get the feature names from the training data format\n",
    "with open(f'{repo_path}/datasets/train/train3000_w_chunk_score_part0.json', 'r') as f:\n",
    "    train_sample = json.load(f)\n",
    "\n",
    "# Extract column names from the training data\n",
    "ATTENTION_COLS = list(train_sample[0]['scores'][0]['prompt_attention_score'].keys())\n",
    "PARAMETER_COLS = list(train_sample[0]['scores'][0]['parameter_knowledge_scores'].keys())\n",
    "\n",
    "print(f\"\\nNumber of attention columns (ECS features): {len(ATTENTION_COLS)}\")\n",
    "print(f\"Number of parameter knowledge columns (PKS features): {len(PARAMETER_COLS)}\")\n",
    "print(f\"Total features: {len(ATTENTION_COLS) + len(PARAMETER_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2aa5ce",
   "metadata": {},
   "source": [
    "---\n",
    "# GT1: Model Generalization Evaluation\n",
    "\n",
    "The original work uses **Qwen3-0.6B** for signal extraction. To test model generalization, we need to verify if the neuron-level findings (ECS and PKS patterns) are predictable on a **new model**.\n",
    "\n",
    "**Approach**: We will test using a different model architecture to extract ECS/PKS signals and see if the hallucination patterns still hold.\n",
    "\n",
    "**New models to try** (not used in original work):\n",
    "1. Qwen2-0.5B (different model family version)\n",
    "2. GPT-2 (completely different architecture)\n",
    "3. Pythia models (different training approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d573d21",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# First, let's understand what exactly we need to test for GT1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The key finding is that ECS (External Context Score) and PKS (Parametric Knowledge Score)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# correlate with hallucination in specific ways:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Let's check what models TransformerLens supports\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# List available models\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformer_lens/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loading_from_pretrained \u001b[38;5;28;01mas\u001b[39;00m loading\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patching\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpast_key_value_caching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     HookedTransformerKeyValueCache \u001b[38;5;28;01mas\u001b[39;00m EasyTransformerKeyValueCache,\n\u001b[1;32m     19\u001b[0m     HookedTransformerKeyValueCacheEntry \u001b[38;5;28;01mas\u001b[39;00m EasyTransformerKeyValueCacheEntry,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHookedTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer \u001b[38;5;28;01mas\u001b[39;00m EasyTransformer\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformer_lens/train.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     30\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/sdk/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertLevel  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_artifacts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _attach, init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/sdk/wandb_artifacts.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata_types\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env, util\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InternalApi, PublicApi\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact \u001b[38;5;28;01mas\u001b[39;00m PublicArtifact\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CommError\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/apis/__init__.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m     _disable_ssl()\n\u001b[1;32m     40\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     45\u001b[0m reset_path()\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/apis/internal.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mApi\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/sdk/internal/internal_api.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, env, util\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CommError, UsageError\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msagemaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_sm_secrets\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/apis/normalize.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CommError\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmailbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextCancelledError\n\u001b[1;32m     15\u001b[0m _F \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_F\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mCallable)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_exceptions\u001b[39m(func: _F) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _F:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:102\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m found\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01m_MailboxSlot\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_result\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResult\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_event\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvent\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:103\u001b[0m, in \u001b[0;36m_MailboxSlot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_MailboxSlot\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     _result: Optional[\u001b[43mpb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResult\u001b[49m]\n\u001b[1;32m    104\u001b[0m     _event: threading\u001b[38;5;241m.\u001b[39mEvent\n\u001b[1;32m    105\u001b[0m     _lock: threading\u001b[38;5;241m.\u001b[39mLock\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'"
     ]
    }
   ],
   "source": [
    "# First, let's understand what exactly we need to test for GT1\n",
    "# The key finding is that ECS (External Context Score) and PKS (Parametric Knowledge Score)\n",
    "# correlate with hallucination in specific ways:\n",
    "# - ECS: Negative correlation (lower ECS = more hallucination)\n",
    "# - PKS: Positive correlation in later layers (higher PKS = more hallucination)\n",
    "\n",
    "# For GT1, we need to test if these patterns hold when using a DIFFERENT model for signal extraction\n",
    "# The original uses Qwen3-0.6B which has 28 layers\n",
    "\n",
    "# Let's check what models TransformerLens supports\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# List available models\n",
    "try:\n",
    "    print(\"Checking TransformerLens available models...\")\n",
    "    # Some common small models in TransformerLens\n",
    "    available_models = [\n",
    "        \"gpt2-small\",  # 12 layers, 12 heads - different architecture\n",
    "        \"pythia-70m\",   # EleutherAI - different architecture\n",
    "        \"pythia-160m\",  # EleutherAI - different architecture  \n",
    "    ]\n",
    "    print(\"Models we can try:\", available_models)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebebb781",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still getting error: module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'\n",
      "Let's try a different approach using transformers directly\n"
     ]
    }
   ],
   "source": [
    "# Let's work around the wandb issue by disabling it\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "# Try importing transformer_lens again\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove wandb from cached modules to force reimport\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'wandb' in key or 'transformer_lens' in key]\n",
    "for mod in modules_to_remove:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "# Now import without wandb\n",
    "try:\n",
    "    from transformer_lens import HookedTransformer\n",
    "    print(\"Successfully imported HookedTransformer\")\n",
    "except Exception as e:\n",
    "    print(f\"Still getting error: {e}\")\n",
    "    print(\"Let's try a different approach using transformers directly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d08188c7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models for GT1 evaluation...\n",
      "Using device: cuda\n",
      "Loading new model for GT1: gpt2\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1124\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1122\u001b[0m pointer_path \u001b[38;5;241m=\u001b[39m _get_pointer_path(storage_folder, commit_hash, relative_filename)\n\u001b[0;32m-> 1124\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pointer_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/net/projects2/chacha/hub'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m gt1_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# NOT used in original work\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading new model for GT1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt1_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m tokenizer_gt1 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt1_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(gt1_model_name, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m model_gt1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1089\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m   1091\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    920\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 921\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    938\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:524\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    531\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    532\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    534\u001b[0m ]\n",
      "\u001b[0;31mOSError\u001b[0m: PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal."
     ]
    }
   ],
   "source": [
    "# Let's use transformers directly instead of transformer_lens\n",
    "# We can implement the ECS and PKS computations using HuggingFace transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setting up models for GT1 evaluation...\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For GT1, we'll use a different model than Qwen3-0.6B\n",
    "# Let's try GPT-2 which has a different architecture\n",
    "# GPT-2 small has 12 layers and 12 heads (vs Qwen3-0.6B with 28 layers and 16 heads)\n",
    "\n",
    "gt1_model_name = \"gpt2\"  # NOT used in original work\n",
    "print(f\"Loading new model for GT1: {gt1_model_name}\")\n",
    "\n",
    "tokenizer_gt1 = AutoTokenizer.from_pretrained(gt1_model_name)\n",
    "model_gt1 = AutoModelForCausalLM.from_pretrained(gt1_model_name, output_attentions=True, output_hidden_states=True)\n",
    "model_gt1 = model_gt1.to(device)\n",
    "model_gt1.eval()\n",
    "\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25e3c6e4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models for GT1 evaluation...\n",
      "Using device: cuda\n",
      "Loading new model for GT1: gpt2\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1124\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1122\u001b[0m pointer_path \u001b[38;5;241m=\u001b[39m _get_pointer_path(storage_folder, commit_hash, relative_filename)\n\u001b[0;32m-> 1124\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pointer_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/net/projects2/chacha/hub'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m gt1_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# NOT used in original work\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading new model for GT1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt1_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m tokenizer_gt1 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt1_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(gt1_model_name, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m model_gt1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1089\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m   1091\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    920\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 921\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    938\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:524\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    531\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    532\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    534\u001b[0m ]\n",
      "\u001b[0;31mOSError\u001b[0m: PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal."
     ]
    }
   ],
   "source": [
    "# Set a local cache directory that we have permissions for\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/home/smallyan/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smallyan/.cache/huggingface/transformers'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('/home/smallyan/.cache/huggingface/transformers', exist_ok=True)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Setting up models for GT1 evaluation...\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For GT1, we'll use a different model than Qwen3-0.6B\n",
    "gt1_model_name = \"gpt2\"  # NOT used in original work\n",
    "print(f\"Loading new model for GT1: {gt1_model_name}\")\n",
    "\n",
    "tokenizer_gt1 = AutoTokenizer.from_pretrained(gt1_model_name)\n",
    "model_gt1 = AutoModelForCausalLM.from_pretrained(gt1_model_name, output_attentions=True, output_hidden_states=True)\n",
    "model_gt1 = model_gt1.to(device)\n",
    "model_gt1.eval()\n",
    "\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "804def5b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current HF cache: /net/projects2/chacha/hub\n",
      "Loading new model for GT1: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f061f4bd95f4dd5ba8426d51533995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c806c81ee19e4c84a769bdea68b780dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d894f5a59743bc92ffd91857dae87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9ba60d54554a75b2417ea42335df7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319e54f587dd44c8b78619824a344b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/net/projects2/chacha/hub'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:402\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/gpt2/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1460\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:283\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:307\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:413\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    412\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-695fe110-3e424a4948d1c63e2d3944ac;55427fa3-c97c-4b2e-941f-39c0d10f3174)\n\nEntry Not Found for url: https://huggingface.co/gpt2/resolve/main/adapter_config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/net/projects2/chacha/hub/models--gpt2/refs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/net/projects2/chacha/hub/models--gpt2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1070\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1559\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1556\u001b[0m             logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m   1557\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not cache non-existence of file. Will ignore error and continue. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m             )\n\u001b[0;32m-> 1559\u001b[0m         \u001b[43m_cache_commit_hash_for_specific_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:763\u001b[0m, in \u001b[0;36m_cache_commit_hash_for_specific_revision\u001b[0;34m(storage_folder, revision, commit_hash)\u001b[0m\n\u001b[1;32m    762\u001b[0m ref_path \u001b[38;5;241m=\u001b[39m Path(storage_folder) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m revision\n\u001b[0;32m--> 763\u001b[0m \u001b[43mref_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_path\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m commit_hash \u001b[38;5;241m!=\u001b[39m ref_path\u001b[38;5;241m.\u001b[39mread_text():\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# Update ref only if has been updated. Could cause useless error in case\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# repo is already cached and user doesn't have write access to cache folder.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/huggingface_hub/issues/1216.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/net/projects2/chacha/hub'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading new model for GT1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt1_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer_gt1 \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(gt1_model_name, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 16\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt1_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model_gt1 \u001b[38;5;241m=\u001b[39m model_gt1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m model_gt1\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m         adapter_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 526\u001b[0m maybe_adapter_path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_adapter_config_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maybe_adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(maybe_adapter_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/peft_utils.py:88\u001b[0m, in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     86\u001b[0m         adapter_cached_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, ADAPTER_CONFIG_NAME)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     adapter_cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mADAPTER_CONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_cached_filename\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:524\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    531\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    532\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    534\u001b[0m ]\n",
      "\u001b[0;31mOSError\u001b[0m: PermissionError at /net/projects2/chacha/hub when downloading gpt2. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal."
     ]
    }
   ],
   "source": [
    "# Need to set it before importing transformers - let's check the huggingface_hub config\n",
    "import huggingface_hub\n",
    "print(f\"Current HF cache: {huggingface_hub.constants.HF_HUB_CACHE}\")\n",
    "\n",
    "# Let's set it properly\n",
    "cache_dir = '/home/smallyan/.cache/huggingface'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Force the cache directory\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "gt1_model_name = \"gpt2\"\n",
    "print(f\"Loading new model for GT1: {gt1_model_name}\")\n",
    "\n",
    "tokenizer_gt1 = AutoTokenizer.from_pretrained(gt1_model_name, cache_dir=cache_dir)\n",
    "model_gt1 = AutoModelForCausalLM.from_pretrained(gt1_model_name, output_attentions=True, output_hidden_states=True, cache_dir=cache_dir)\n",
    "model_gt1 = model_gt1.to(device)\n",
    "model_gt1.eval()\n",
    "\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4059783a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /home/smallyan/.cache/huggingface\n",
      "TRANSFORMERS_CACHE: /home/smallyan/.cache/huggingface/hub\n",
      "HF_HUB_CACHE: /home/smallyan/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "# Let's set the HF_HOME env variable properly before any huggingface imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Remove all huggingface-related modules to force fresh import\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'transformers' in key or 'huggingface' in key]\n",
    "for mod in modules_to_remove:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "# Set environment variable BEFORE importing\n",
    "os.environ['HF_HOME'] = '/home/smallyan/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smallyan/.cache/huggingface/hub'\n",
    "os.environ['HF_HUB_CACHE'] = '/home/smallyan/.cache/huggingface/hub'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/home/smallyan/.cache/huggingface/hub', exist_ok=True)\n",
    "\n",
    "# Check it's set\n",
    "print(f\"HF_HOME: {os.environ.get('HF_HOME')}\")\n",
    "print(f\"TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}\")\n",
    "print(f\"HF_HUB_CACHE: {os.environ.get('HF_HUB_CACHE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "301fd0ef",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "No huggingface_hub attribute constants",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now import huggingface_hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF cache after import: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241m.\u001b[39mHF_HUB_CACHE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import transformers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/__init__.py:1059\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attr\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: No huggingface_hub attribute constants"
     ]
    }
   ],
   "source": [
    "# Now import huggingface_hub\n",
    "import huggingface_hub\n",
    "print(f\"HF cache after import: {huggingface_hub.constants.HF_HUB_CACHE}\")\n",
    "\n",
    "# Import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "cache_dir = '/home/smallyan/.cache/huggingface/hub'\n",
    "\n",
    "gt1_model_name = \"gpt2\"\n",
    "print(f\"\\nLoading new model for GT1: {gt1_model_name}\")\n",
    "\n",
    "tokenizer_gt1 = AutoTokenizer.from_pretrained(gt1_model_name, cache_dir=cache_dir)\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "model_gt1 = AutoModelForCausalLM.from_pretrained(gt1_model_name, output_attentions=True, output_hidden_states=True, cache_dir=cache_dir)\n",
    "model_gt1 = model_gt1.to(device)\n",
    "model_gt1.eval()\n",
    "\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e46c1cc3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new model for GT1: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef78185fb74e4f71b6068683d247e1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c82def3fb9d446b9775d8cfe8ac4ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7875f16453a645228bd3ad58b7484e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0a10313edc4f398990a590cfa73627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb546805db41c99e11c3c1a899bea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5c5a92213b4ecda224ea43b7e52a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just import and load directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "cache_dir = '/home/smallyan/.cache/huggingface/hub'\n",
    "\n",
    "gt1_model_name = \"gpt2\"\n",
    "print(f\"Loading new model for GT1: {gt1_model_name}\")\n",
    "\n",
    "tokenizer_gt1 = AutoTokenizer.from_pretrained(gt1_model_name, cache_dir=cache_dir)\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "model_gt1 = AutoModelForCausalLM.from_pretrained(gt1_model_name, output_attentions=True, output_hidden_states=True, cache_dir=cache_dir)\n",
    "model_gt1 = model_gt1.to(device)\n",
    "model_gt1.eval()\n",
    "\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52505046",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check if loading completed\n",
    "print(f\"Model loaded: {gt1_model_name}\")\n",
    "print(f\"Number of layers: {model_gt1.config.n_layer}\")\n",
    "print(f\"Number of heads: {model_gt1.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5158c10c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check model status\n",
    "print(\"Checking model status...\")\n",
    "try:\n",
    "    print(f\"model_gt1 type: {type(model_gt1)}\")\n",
    "    print(f\"Config: {model_gt1.config.n_layer} layers, {model_gt1.config.n_head} heads\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Model may not have loaded properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7073e984",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-08-10-50_CircuitAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
