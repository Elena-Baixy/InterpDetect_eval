{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf7eb74",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 103 characters, 21 objects, 23 states\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the repository path\n",
    "repo_path = '/net/scratch2/smallyan/belief_tracking_eval'\n",
    "sys.path.insert(0, repo_path)\n",
    "sys.path.insert(0, os.path.join(repo_path, 'notebooks', 'causalToM_novis'))\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Import dataset utilities\n",
    "from src.dataset import Sample, Dataset\n",
    "from utils import error_detection, get_answer_lookback_payload\n",
    "\n",
    "# Load synthetic entities\n",
    "data_path = os.path.join(repo_path, 'data', 'synthetic_entities')\n",
    "with open(os.path.join(data_path, 'characters.json'), 'r') as f:\n",
    "    all_characters = json.load(f)\n",
    "with open(os.path.join(data_path, 'bottles.json'), 'r') as f:\n",
    "    all_objects = json.load(f)\n",
    "with open(os.path.join(data_path, 'drinks.json'), 'r') as f:\n",
    "    all_states = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_characters)} characters, {len(all_objects)} objects, {len(all_states)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4748eec2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/adapter_config.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21b9eaef4e440fe818f25b3cb74c3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: meta-llama/Llama-3.1-8B-Instruct\n",
      "Number of layers: 32\n"
     ]
    }
   ],
   "source": [
    "# Load Llama-3.1-8B-Instruct for GT1 evaluation\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "print(\"Loading Llama-3.1-8B-Instruct...\")\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    dispatch=True,\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbc7c27",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 10 samples\n",
      "\n",
      "Detecting errors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:01<00:11,  1.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:02<00:08,  1.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:03<00:07,  1.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:04<00:06,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:05<00:05,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:07<00:03,  1.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:08<00:02,  1.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:09<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:10<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:10<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples for IIA: 3 (7 errors)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate counterfactual dataset for answer lookback payload test\n",
    "n_samples = 10\n",
    "batch_size = 1\n",
    "\n",
    "dataset_payload = get_answer_lookback_payload(\n",
    "    all_characters,\n",
    "    all_objects,\n",
    "    all_states,\n",
    "    n_samples,\n",
    ")\n",
    "dataloader_payload = DataLoader(dataset_payload, batch_size=batch_size, shuffle=False)\n",
    "print(f\"Created dataset with {len(dataset_payload)} samples\")\n",
    "\n",
    "# Detect errors (samples where model doesn't answer correctly)\n",
    "print(\"\\nDetecting errors...\")\n",
    "_, errors = error_detection(model, dataloader_payload, is_remote=False)\n",
    "valid_samples = len(dataset_payload) - len(errors)\n",
    "print(f\"Valid samples for IIA: {valid_samples} ({len(errors)} errors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4392073",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 30 samples\n",
      "\n",
      "Detecting errors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/30 [00:01<00:55,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [00:03<00:53,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [00:05<00:52,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [00:07<00:50,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [00:09<00:48,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [00:11<00:46,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [00:13<00:46,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 8/30 [00:15<00:43,  2.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 9/30 [00:17<00:41,  1.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 10/30 [00:19<00:39,  1.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 11/30 [00:20<00:32,  1.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 12/30 [00:21<00:26,  1.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 13/30 [00:22<00:22,  1.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 14/30 [00:23<00:19,  1.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [00:24<00:17,  1.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 16/30 [00:25<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 17/30 [00:26<00:13,  1.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 18/30 [00:27<00:12,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 19/30 [00:28<00:10,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 20/30 [00:29<00:09,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 21/30 [00:30<00:08,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 22/30 [00:31<00:07,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 23/30 [00:32<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 24/30 [00:33<00:06,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 25/30 [00:34<00:04,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 26/30 [00:35<00:03,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 27/30 [00:35<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 28/30 [00:36<00:01,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 29/30 [00:37<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 30/30 [00:38<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 30/30 [00:38<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples for IIA: 9 (21 errors)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Need more samples since many have errors - generate larger dataset\n",
    "n_samples = 30\n",
    "dataset_payload = get_answer_lookback_payload(\n",
    "    all_characters,\n",
    "    all_objects,\n",
    "    all_states,\n",
    "    n_samples,\n",
    ")\n",
    "dataloader_payload = DataLoader(dataset_payload, batch_size=1, shuffle=False)\n",
    "print(f\"Created dataset with {len(dataset_payload)} samples\")\n",
    "\n",
    "# Detect errors \n",
    "print(\"\\nDetecting errors...\")\n",
    "_, errors = error_detection(model, dataloader_payload, is_remote=False)\n",
    "valid_samples = len(dataset_payload) - len(errors)\n",
    "print(f\"Valid samples for IIA: {valid_samples} ({len(errors)} errors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a94d36",
   "metadata": {
    "execution_status": "running"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IIA across layers for answer lookback payload...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  0: IIA = 0.000 (0/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  8: IIA = 0.000 (0/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16: IIA = 0.000 (0/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20: IIA = 0.000 (0/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 22: IIA = 0.000 (0/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24: IIA = 0.222 (2/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 26: IIA = 0.556 (5/9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28: IIA = 0.889 (8/9)\n"
     ]
    }
   ],
   "source": [
    "# Run IIA experiment at key layers\n",
    "# For 32-layer model, we scale the layer findings from 80-layer model\n",
    "# 70B has 80 layers with effect at ~70% (layer 56+)\n",
    "# So for 32 layers, expect effects around layer 22+\n",
    "\n",
    "accs_answer_lookback_payload = {}\n",
    "patch_layers = [0, 8, 16, 20, 22, 24, 26, 28, 30, 31]\n",
    "\n",
    "print(\"Testing IIA across layers for answer lookback payload...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for layer_idx in patch_layers:\n",
    "    correct, total = 0, 0\n",
    "    for bi, batch in enumerate(dataloader_payload):\n",
    "        if bi in errors:\n",
    "            continue\n",
    "        counterfactual_prompt = batch[\"counterfactual_prompt\"][0]\n",
    "        clean_prompt = batch[\"clean_prompt\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get the counterfactual layer output (hidden states are first element of tuple)\n",
    "            with model.trace(counterfactual_prompt):\n",
    "                counterfactual_layer_out = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "            \n",
    "            # Patch into clean and get prediction\n",
    "            with model.trace(clean_prompt):\n",
    "                model.model.layers[layer_idx].output[0][0, -1] = counterfactual_layer_out\n",
    "                pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "\n",
    "            pred_text = model.tokenizer.decode([pred]).lower().strip()\n",
    "            if pred_text == target.lower().strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            del pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acc = round(correct / total, 3) if total > 0 else 0.0\n",
    "    print(f\"Layer {layer_idx:2d}: IIA = {acc:.3f} ({correct}/{total})\")\n",
    "    accs_answer_lookback_payload[layer_idx] = acc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "peak_layer = max(accs_answer_lookback_payload, key=accs_answer_lookback_payload.get)\n",
    "peak_iia = accs_answer_lookback_payload[peak_layer]\n",
    "print(f\"Peak IIA: {peak_iia:.3f} at layer {peak_layer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-20-53_GeneralizationEval2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
