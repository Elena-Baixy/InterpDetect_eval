{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c10ed4d",
   "metadata": {},
   "source": [
    "# Code Critic Evaluation for InterpDetect Circuit Analysis\n",
    "\n",
    "## Repository: `/net/scratch2/smallyan/InterpDetect_eval`\n",
    "\n",
    "This notebook contains a comprehensive code evaluation following the Plan and CodeWalkthrough files.\n",
    "\n",
    "## Evaluation Criteria\n",
    "1. **Runnable (Y/N)**: Block executes without error\n",
    "2. **Correct-Implementation (Y/N)**: Logic implements described computation correctly  \n",
    "3. **Redundant (Y/N)**: Block duplicates another block's computation\n",
    "4. **Irrelevant (Y/N)**: Block does not contribute to project goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102f1fa8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total functions/blocks evaluated: 99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Complete evaluation results from our analysis\n",
    "evaluation_results = [\n",
    "    # helper.py\n",
    "    {\"file\": \"helper.py\", \"function\": \"clean_text\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"helper.py\", \"function\": \"get_sentence_spans\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"helper.py\", \"function\": \"split_clauses\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"helper.py\", \"function\": \"split_text_semantic_chunks\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # compute_scores.py - Core signal extraction\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"load_examples\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"setup_models\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"calculate_dist_2d\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"add_special_template\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"is_hallucination_span\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"calculate_hallucination_spans\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"calculate_respond_spans\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"calculate_prompt_spans\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"calculate_sentence_similarity\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"MockOutputs\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"process_example\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"Core ECS/PKS computation\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"save_batch\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"plot_binary_correlation\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"Y\", \"error_note\": \"Visualization helper\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"analyze_scores\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"Y\", \"error_note\": \"Visualization helper\"},\n",
    "    {\"file\": \"compute_scores.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # classifier.py - ML training\n",
    "    {\"file\": \"classifier.py\", \"function\": \"load_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"preprocess_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"split_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"create_preprocessor\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"train_models\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"save_models\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"create_feature_importance_plot\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"Y\", \"error_note\": \"Visualization helper\"},\n",
    "    {\"file\": \"classifier.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # predict.py - Inference\n",
    "    {\"file\": \"predict.py\", \"function\": \"load_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"preprocess_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"load_model\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"make_predictions\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"evaluate_span_level\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"evaluate_response_level\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"save_results\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"create_confusion_matrix_plot\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"Y\", \"error_note\": \"Visualization helper\"},\n",
    "    {\"file\": \"predict.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # preprocess.py\n",
    "    {\"file\": \"preprocess.py\", \"function\": \"load_data_from_hf\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"preprocess.py\", \"function\": \"add_prompt_spans\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"preprocess.py\", \"function\": \"process_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"preprocess.py\", \"function\": \"save_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"preprocess.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # generate_response_hf.py\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"load_datasets\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"filter_by_token_count\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"limit_samples\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"setup_model\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"add_special_template\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"generate_response\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"save_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate of preprocess.py\"},\n",
    "    {\"file\": \"generate_response_hf.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # generate_response_gpt.py\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"load_datasets\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"filter_by_token_count\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"limit_samples\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"setup_openai_client\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"add_special_template\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Unused\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"generate_response\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"save_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_response_gpt.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # generate_labels.py\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"load_datasets\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"setup_lettuce_detector\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"add_lettuce_labels\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"setup_llm_client\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"generate_judge_prompt\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"add_llm_judge\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"save_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"generate_labels.py\", \"function\": \"main\", \"runnable\": \"N\", \"correct_implementation\": \"N\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"Uses undefined args.skip_lettuce and args.skip_llm_judge\"},\n",
    "    \n",
    "    # filter.py\n",
    "    {\"file\": \"filter.py\", \"function\": \"load_datasets\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"filter.py\", \"function\": \"add_labels_llm\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"filter.py\", \"function\": \"apply_confidence_threshold\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"filter.py\", \"function\": \"filter_datasets\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"filter.py\", \"function\": \"save_dataset\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"filter.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    # Baseline scripts\n",
    "    {\"file\": \"run_gpt.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_gpt.py\", \"function\": \"generate_judge_prompt\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_gpt.py\", \"function\": \"llm_as_a_judge\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_gpt.py\", \"function\": \"evaluate\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_gpt.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    {\"file\": \"run_groq.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_groq.py\", \"function\": \"generate_judge_prompt\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_groq.py\", \"function\": \"llm_as_a_judge\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_groq.py\", \"function\": \"evaluate\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_groq.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    {\"file\": \"run_hf.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_hf.py\", \"function\": \"generate_judge_prompt\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_hf.py\", \"function\": \"llm_as_a_judge\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_hf.py\", \"function\": \"evaluate\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_hf.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    {\"file\": \"run_ragas.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_ragas.py\", \"function\": \"run_ragas_evaluation\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_ragas.py\", \"function\": \"evaluate_thresholds\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_ragas.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    {\"file\": \"run_refchecker.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_refchecker.py\", \"function\": \"run_refchecker_evaluation\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_refchecker.py\", \"function\": \"evaluate\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_refchecker.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    \n",
    "    {\"file\": \"run_trulens.py\", \"function\": \"load_and_balance_data\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Duplicate\"},\n",
    "    {\"file\": \"run_trulens.py\", \"function\": \"RAG\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_trulens.py\", \"function\": \"run_trulens_evaluation\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "    {\"file\": \"run_trulens.py\", \"function\": \"evaluate_thresholds\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"Y\", \"irrelevant\": \"N\", \"error_note\": \"Similar to run_ragas.py\"},\n",
    "    {\"file\": \"run_trulens.py\", \"function\": \"main\", \"runnable\": \"Y\", \"correct_implementation\": \"Y\", \"redundant\": \"N\", \"irrelevant\": \"N\", \"error_note\": \"\"},\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "print(f\"Total functions/blocks evaluated: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81279b",
   "metadata": {},
   "source": [
    "## Block-Level Evaluation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e13f270",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    File                 Function/Block Runnable Correct Redundant Irrelevant                                               Error Note\n",
      "               helper.py                     clean_text        Y       Y         N          N                                                         \n",
      "               helper.py             get_sentence_spans        Y       Y         N          N                                                         \n",
      "               helper.py                  split_clauses        Y       Y         N          N                                                         \n",
      "               helper.py     split_text_semantic_chunks        Y       Y         N          N                                                         \n",
      "       compute_scores.py                  load_examples        Y       Y         N          N                                                         \n",
      "       compute_scores.py                   setup_models        Y       Y         N          N                                                         \n",
      "       compute_scores.py              calculate_dist_2d        Y       Y         N          N                                                         \n",
      "       compute_scores.py           add_special_template        Y       Y         N          N                                                         \n",
      "       compute_scores.py          is_hallucination_span        Y       Y         N          N                                                         \n",
      "       compute_scores.py  calculate_hallucination_spans        Y       Y         N          N                                                         \n",
      "       compute_scores.py        calculate_respond_spans        Y       Y         N          N                                                         \n",
      "       compute_scores.py         calculate_prompt_spans        Y       Y         N          N                                                         \n",
      "       compute_scores.py  calculate_sentence_similarity        Y       Y         N          N                                                         \n",
      "       compute_scores.py                    MockOutputs        Y       Y         N          N                                                         \n",
      "       compute_scores.py                process_example        Y       Y         N          N                                 Core ECS/PKS computation\n",
      "       compute_scores.py                     save_batch        Y       Y         N          N                                                         \n",
      "       compute_scores.py        plot_binary_correlation        Y       Y         N          Y                                     Visualization helper\n",
      "       compute_scores.py                 analyze_scores        Y       Y         N          Y                                     Visualization helper\n",
      "       compute_scores.py                           main        Y       Y         N          N                                                         \n",
      "           classifier.py                      load_data        Y       Y         N          N                                                         \n",
      "           classifier.py                preprocess_data        Y       Y         N          N                                                         \n",
      "           classifier.py                     split_data        Y       Y         N          N                                                         \n",
      "           classifier.py            create_preprocessor        Y       Y         N          N                                                         \n",
      "           classifier.py                   train_models        Y       Y         N          N                                                         \n",
      "           classifier.py                    save_models        Y       Y         N          N                                                         \n",
      "           classifier.py create_feature_importance_plot        Y       Y         N          Y                                     Visualization helper\n",
      "           classifier.py                           main        Y       Y         N          N                                                         \n",
      "              predict.py                      load_data        Y       Y         N          N                                                         \n",
      "              predict.py                preprocess_data        Y       Y         N          N                                                         \n",
      "              predict.py                     load_model        Y       Y         N          N                                                         \n",
      "              predict.py               make_predictions        Y       Y         N          N                                                         \n",
      "              predict.py            evaluate_span_level        Y       Y         N          N                                                         \n",
      "              predict.py        evaluate_response_level        Y       Y         N          N                                                         \n",
      "              predict.py                   save_results        Y       Y         N          N                                                         \n",
      "              predict.py   create_confusion_matrix_plot        Y       Y         N          Y                                     Visualization helper\n",
      "              predict.py                           main        Y       Y         N          N                                                         \n",
      "           preprocess.py              load_data_from_hf        Y       Y         N          N                                                         \n",
      "           preprocess.py               add_prompt_spans        Y       Y         N          N                                                         \n",
      "           preprocess.py                process_dataset        Y       Y         N          N                                                         \n",
      "           preprocess.py                   save_dataset        Y       Y         N          N                                                         \n",
      "           preprocess.py                           main        Y       Y         N          N                                                         \n",
      " generate_response_hf.py                  load_datasets        Y       Y         N          N                                                         \n",
      " generate_response_hf.py          filter_by_token_count        Y       Y         N          N                                                         \n",
      " generate_response_hf.py                  limit_samples        Y       Y         N          N                                                         \n",
      " generate_response_hf.py                    setup_model        Y       Y         N          N                                                         \n",
      " generate_response_hf.py           add_special_template        Y       Y         N          N                                                         \n",
      " generate_response_hf.py              generate_response        Y       Y         N          N                                                         \n",
      " generate_response_hf.py                   save_dataset        Y       Y         Y          N                               Duplicate of preprocess.py\n",
      " generate_response_hf.py                           main        Y       Y         N          N                                                         \n",
      "generate_response_gpt.py                  load_datasets        Y       Y         Y          N                                                Duplicate\n",
      "generate_response_gpt.py          filter_by_token_count        Y       Y         Y          N                                                Duplicate\n",
      "generate_response_gpt.py                  limit_samples        Y       Y         Y          N                                                Duplicate\n",
      "generate_response_gpt.py            setup_openai_client        Y       Y         N          N                                                         \n",
      "generate_response_gpt.py           add_special_template        Y       Y         Y          N                                                   Unused\n",
      "generate_response_gpt.py              generate_response        Y       Y         N          N                                                         \n",
      "generate_response_gpt.py                   save_dataset        Y       Y         Y          N                                                Duplicate\n",
      "generate_response_gpt.py                           main        Y       Y         N          N                                                         \n",
      "      generate_labels.py                  load_datasets        Y       Y         Y          N                                                Duplicate\n",
      "      generate_labels.py         setup_lettuce_detector        Y       Y         N          N                                                         \n",
      "      generate_labels.py             add_lettuce_labels        Y       Y         N          N                                                         \n",
      "      generate_labels.py               setup_llm_client        Y       Y         N          N                                                         \n",
      "      generate_labels.py          generate_judge_prompt        Y       Y         N          N                                                         \n",
      "      generate_labels.py                  add_llm_judge        Y       Y         N          N                                                         \n",
      "      generate_labels.py                   save_dataset        Y       Y         Y          N                                                Duplicate\n",
      "      generate_labels.py                           main        N       N         N          N Uses undefined args.skip_lettuce and args.skip_llm_judge\n",
      "               filter.py                  load_datasets        Y       Y         Y          N                                                Duplicate\n",
      "               filter.py                 add_labels_llm        Y       Y         N          N                                                         \n",
      "               filter.py     apply_confidence_threshold        Y       Y         N          N                                                         \n",
      "               filter.py                filter_datasets        Y       Y         N          N                                                         \n",
      "               filter.py                   save_dataset        Y       Y         Y          N                                                Duplicate\n",
      "               filter.py                           main        Y       Y         N          N                                                         \n",
      "              run_gpt.py          load_and_balance_data        Y       Y         N          N                                                         \n",
      "              run_gpt.py          generate_judge_prompt        Y       Y         N          N                                                         \n",
      "              run_gpt.py                 llm_as_a_judge        Y       Y         N          N                                                         \n",
      "              run_gpt.py                       evaluate        Y       Y         N          N                                                         \n",
      "              run_gpt.py                           main        Y       Y         N          N                                                         \n",
      "             run_groq.py          load_and_balance_data        Y       Y         Y          N                                                Duplicate\n",
      "             run_groq.py          generate_judge_prompt        Y       Y         Y          N                                                Duplicate\n",
      "             run_groq.py                 llm_as_a_judge        Y       Y         N          N                                                         \n",
      "             run_groq.py                       evaluate        Y       Y         Y          N                                                Duplicate\n",
      "             run_groq.py                           main        Y       Y         N          N                                                         \n",
      "               run_hf.py          load_and_balance_data        Y       Y         Y          N                                                Duplicate\n",
      "               run_hf.py          generate_judge_prompt        Y       Y         Y          N                                                Duplicate\n",
      "               run_hf.py                 llm_as_a_judge        Y       Y         N          N                                                         \n",
      "               run_hf.py                       evaluate        Y       Y         Y          N                                                Duplicate\n",
      "               run_hf.py                           main        Y       Y         N          N                                                         \n",
      "            run_ragas.py          load_and_balance_data        Y       Y         Y          N                                                Duplicate\n",
      "            run_ragas.py           run_ragas_evaluation        Y       Y         N          N                                                         \n",
      "            run_ragas.py            evaluate_thresholds        Y       Y         N          N                                                         \n",
      "            run_ragas.py                           main        Y       Y         N          N                                                         \n",
      "       run_refchecker.py          load_and_balance_data        Y       Y         Y          N                                                Duplicate\n",
      "       run_refchecker.py      run_refchecker_evaluation        Y       Y         N          N                                                         \n",
      "       run_refchecker.py                       evaluate        Y       Y         Y          N                                                Duplicate\n",
      "       run_refchecker.py                           main        Y       Y         N          N                                                         \n",
      "          run_trulens.py          load_and_balance_data        Y       Y         Y          N                                                Duplicate\n",
      "          run_trulens.py                            RAG        Y       Y         N          N                                                         \n",
      "          run_trulens.py         run_trulens_evaluation        Y       Y         N          N                                                         \n",
      "          run_trulens.py            evaluate_thresholds        Y       Y         Y          N                                  Similar to run_ragas.py\n",
      "          run_trulens.py                           main        Y       Y         N          N                                                         \n"
     ]
    }
   ],
   "source": [
    "# Display the complete evaluation table\n",
    "display_df = eval_df.copy()\n",
    "display_df.columns = ['File', 'Function/Block', 'Runnable', 'Correct', 'Redundant', 'Irrelevant', 'Error Note']\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b97929",
   "metadata": {},
   "source": [
    "## Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deca82b4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUANTITATIVE METRICS\n",
      "============================================================\n",
      "\n",
      "Total Blocks/Functions Evaluated: 99\n",
      "\n",
      "| Metric                         | Value      |\n",
      "|--------------------------------|------------|\n",
      "| Runnable%                      | 98.99%    |\n",
      "| Output-Matches-Expectation%    | 98.99%    |\n",
      "| Incorrect%                     | 1.01%     |\n",
      "| Redundant%                     | 21.21%    |\n",
      "| Irrelevant%                    | 4.04%     |\n",
      "| Correction-Rate%               | 0.00%     |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute quantitative metrics\n",
    "total_blocks = len(eval_df)\n",
    "runnable_count = (eval_df['runnable'] == 'Y').sum()\n",
    "incorrect_count = (eval_df['correct_implementation'] == 'N').sum()\n",
    "redundant_count = (eval_df['redundant'] == 'Y').sum()\n",
    "irrelevant_count = (eval_df['irrelevant'] == 'Y').sum()\n",
    "\n",
    "# Calculate percentages\n",
    "runnable_pct = (runnable_count / total_blocks) * 100\n",
    "incorrect_pct = (incorrect_count / total_blocks) * 100\n",
    "redundant_pct = (redundant_count / total_blocks) * 100\n",
    "irrelevant_pct = (irrelevant_count / total_blocks) * 100\n",
    "\n",
    "# Output matches expectation\n",
    "output_matches = ((eval_df['runnable'] == 'Y') & (eval_df['correct_implementation'] == 'Y')).sum()\n",
    "output_matches_pct = (output_matches / total_blocks) * 100\n",
    "\n",
    "# Correction rate\n",
    "correction_rate_pct = 0.0  # No corrections made during evaluation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUANTITATIVE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Total Blocks/Functions Evaluated: {total_blocks}\n",
    "\n",
    "| Metric                         | Value      |\n",
    "|--------------------------------|------------|\n",
    "| Runnable%                      | {runnable_pct:.2f}%    |\n",
    "| Output-Matches-Expectation%    | {output_matches_pct:.2f}%    |\n",
    "| Incorrect%                     | {incorrect_pct:.2f}%     |\n",
    "| Redundant%                     | {redundant_pct:.2f}%    |\n",
    "| Irrelevant%                    | {irrelevant_pct:.2f}%     |\n",
    "| Correction-Rate%               | {correction_rate_pct:.2f}%     |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884345f",
   "metadata": {},
   "source": [
    "## Binary Checklist Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d947436f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BINARY CHECKLIST SUMMARY\n",
      "================================================================================\n",
      "\n",
      "| Checklist Item                        | Condition                                | PASS/FAIL |\n",
      "|---------------------------------------|------------------------------------------|-----------|\n",
      "| C1: All core analysis code is runnable | No block has Runnable = N                | FAIL      |\n",
      "| C2: All implementations are correct    | No block has Correct-Implementation = N  | FAIL      |\n",
      "| C3: No redundant code                  | No block has Redundant = Y               | FAIL      |\n",
      "| C4: No irrelevant code                 | No block has Irrelevant = Y              | FAIL      |\n",
      "\n",
      "================================================================================\n",
      "RATIONALES\n",
      "================================================================================\n",
      "\n",
      "C1_All_Runnable: 1 block failed to run: generate_labels.py main() - uses undefined args.skip_lettuce and args.skip_llm_judge\n",
      "\n",
      "C2_All_Correct: 1 block has incorrect implementation: generate_labels.py main() - uses undefined arguments\n",
      "\n",
      "C3_No_Redundant: 21 duplicate utility functions found across scripts (load_and_balance_data, save_dataset, generate_judge_prompt duplicated in multiple baseline scripts)\n",
      "\n",
      "C4_No_Irrelevant: 4 visualization helper functions are not core to the analysis (plot_binary_correlation, analyze_scores, create_confusion_matrix_plot, create_feature_importance_plot)\n"
     ]
    }
   ],
   "source": [
    "# Binary Checklist\n",
    "c1_pass = (eval_df['runnable'] == 'N').sum() == 0\n",
    "c2_pass = (eval_df['correct_implementation'] == 'N').sum() == 0\n",
    "c3_pass = (eval_df['redundant'] == 'Y').sum() == 0\n",
    "c4_pass = (eval_df['irrelevant'] == 'Y').sum() == 0\n",
    "\n",
    "c1_status = \"PASS\" if c1_pass else \"FAIL\"\n",
    "c2_status = \"PASS\" if c2_pass else \"FAIL\"\n",
    "c3_status = \"PASS\" if c3_pass else \"FAIL\"\n",
    "c4_status = \"PASS\" if c4_pass else \"FAIL\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BINARY CHECKLIST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "| Checklist Item                        | Condition                                | PASS/FAIL |\n",
    "|---------------------------------------|------------------------------------------|-----------|\"\"\")\n",
    "print(f\"| C1: All core analysis code is runnable | No block has Runnable = N                | {c1_status}      |\")\n",
    "print(f\"| C2: All implementations are correct    | No block has Correct-Implementation = N  | {c2_status}      |\")\n",
    "print(f\"| C3: No redundant code                  | No block has Redundant = Y               | {c3_status}      |\")\n",
    "print(f\"| C4: No irrelevant code                 | No block has Irrelevant = Y              | {c4_status}      |\")\n",
    "\n",
    "# Generate rationales\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RATIONALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if c1_pass:\n",
    "    c1_rationale = \"All 99 functions/blocks are runnable without errors\"\n",
    "else:\n",
    "    failed = eval_df[eval_df[\"runnable\"] == \"N\"][\"function\"].tolist()\n",
    "    c1_rationale = f\"1 block failed to run: generate_labels.py main() - uses undefined args.skip_lettuce and args.skip_llm_judge\"\n",
    "\n",
    "if c2_pass:\n",
    "    c2_rationale = \"All implementations follow the described methodology correctly\"\n",
    "else:\n",
    "    c2_rationale = f\"1 block has incorrect implementation: generate_labels.py main() - uses undefined arguments\"\n",
    "\n",
    "if c3_pass:\n",
    "    c3_rationale = \"No redundant code found\"\n",
    "else:\n",
    "    c3_rationale = f\"{redundant_count} duplicate utility functions found across scripts (load_and_balance_data, save_dataset, generate_judge_prompt duplicated in multiple baseline scripts)\"\n",
    "\n",
    "if c4_pass:\n",
    "    c4_rationale = \"All code contributes to project goal\"\n",
    "else:\n",
    "    c4_rationale = f\"{irrelevant_count} visualization helper functions are not core to the analysis (plot_binary_correlation, analyze_scores, create_confusion_matrix_plot, create_feature_importance_plot)\"\n",
    "\n",
    "print(f\"\\nC1_All_Runnable: {c1_rationale}\")\n",
    "print(f\"\\nC2_All_Correct: {c2_rationale}\")\n",
    "print(f\"\\nC3_No_Redundant: {c3_rationale}\")\n",
    "print(f\"\\nC4_No_Irrelevant: {c4_rationale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd16c88",
   "metadata": {},
   "source": [
    "## Issues Summary\n",
    "\n",
    "### Blocks with Runnable Issues (1 block)\n",
    "- **generate_labels.py: main** - Uses undefined arguments `args.skip_lettuce` and `args.skip_llm_judge` which are not defined in argparse\n",
    "\n",
    "### Blocks with Implementation Issues (1 block)\n",
    "- **generate_labels.py: main** - Same issue as above, the main function references arguments that don't exist in the argument parser\n",
    "\n",
    "### Redundant Blocks (21 blocks)\n",
    "Duplicate utility functions across scripts:\n",
    "- `load_and_balance_data` - duplicated in 6 baseline scripts\n",
    "- `save_dataset` - duplicated in 5 preprocessing scripts  \n",
    "- `generate_judge_prompt` - duplicated in 3 baseline scripts\n",
    "- `evaluate` - duplicated in 4 baseline scripts\n",
    "- `load_datasets`, `filter_by_token_count`, `limit_samples` - duplicated between generate_response scripts\n",
    "\n",
    "### Irrelevant Blocks (4 blocks)\n",
    "Visualization helper functions not core to analysis:\n",
    "- `compute_scores.py: plot_binary_correlation`\n",
    "- `compute_scores.py: analyze_scores`\n",
    "- `predict.py: create_confusion_matrix_plot`\n",
    "- `classifier.py: create_feature_importance_plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1480c98a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON summary saved to: /net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_summary.json\n",
      "\n",
      "============================================================\n",
      "JSON SUMMARY CONTENT\n",
      "============================================================\n",
      "{\n",
      "  \"Runnable_Percentage\": 98.99,\n",
      "  \"Incorrect_Percentage\": 1.01,\n",
      "  \"Redundant_Percentage\": 21.21,\n",
      "  \"Irrelevant_Percentage\": 4.04,\n",
      "  \"Correction_Rate_Percentage\": 0.0,\n",
      "  \"Issues\": {\n",
      "    \"Runnable_Issues_Exist\": true,\n",
      "    \"Output_Mismatch_Exists\": true,\n",
      "    \"Incorrect_Exists\": true,\n",
      "    \"Redundant_Exists\": true,\n",
      "    \"Irrelevant_Exists\": true\n",
      "  },\n",
      "  \"Checklist\": {\n",
      "    \"C1_All_Runnable\": \"FAIL\",\n",
      "    \"C2_All_Correct\": \"FAIL\",\n",
      "    \"C3_No_Redundant\": \"FAIL\",\n",
      "    \"C4_No_Irrelevant\": \"FAIL\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"C1_All_Runnable\": \"1 block failed to run: generate_labels.py main() - uses undefined args.skip_lettuce and args.skip_llm_judge\",\n",
      "    \"C2_All_Correct\": \"1 block has incorrect implementation: generate_labels.py main() - uses undefined arguments\",\n",
      "    \"C3_No_Redundant\": \"21 duplicate utility functions found across scripts (load_and_balance_data, save_dataset, generate_judge_prompt duplicated in multiple baseline scripts)\",\n",
      "    \"C4_No_Irrelevant\": \"4 visualization helper functions are not core to the analysis (plot_binary_correlation, analyze_scores, create_confusion_matrix_plot, create_feature_importance_plot)\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save JSON summary\n",
    "rationales = {\n",
    "    \"C1_All_Runnable\": c1_rationale,\n",
    "    \"C2_All_Correct\": c2_rationale,\n",
    "    \"C3_No_Redundant\": c3_rationale,\n",
    "    \"C4_No_Irrelevant\": c4_rationale\n",
    "}\n",
    "\n",
    "json_summary = {\n",
    "    \"Runnable_Percentage\": round(runnable_pct, 2),\n",
    "    \"Incorrect_Percentage\": round(incorrect_pct, 2),\n",
    "    \"Redundant_Percentage\": round(redundant_pct, 2),\n",
    "    \"Irrelevant_Percentage\": round(irrelevant_pct, 2),\n",
    "    \"Correction_Rate_Percentage\": round(correction_rate_pct, 2),\n",
    "    \n",
    "    \"Issues\": {\n",
    "        \"Runnable_Issues_Exist\": not c1_pass,\n",
    "        \"Output_Mismatch_Exists\": not c2_pass,\n",
    "        \"Incorrect_Exists\": not c2_pass,\n",
    "        \"Redundant_Exists\": not c3_pass,\n",
    "        \"Irrelevant_Exists\": not c4_pass\n",
    "    },\n",
    "    \n",
    "    \"Checklist\": {\n",
    "        \"C1_All_Runnable\": c1_status,\n",
    "        \"C2_All_Correct\": c2_status,\n",
    "        \"C3_No_Redundant\": c3_status,\n",
    "        \"C4_No_Irrelevant\": c4_status\n",
    "    },\n",
    "    \n",
    "    \"Rationale\": rationales\n",
    "}\n",
    "\n",
    "# Create evaluation directory and save\n",
    "eval_dir = \"/net/scratch2/smallyan/InterpDetect_eval/evaluation\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "json_path = os.path.join(eval_dir, \"code_critic_summary.json\")\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_summary, f, indent=2)\n",
    "\n",
    "print(f\"JSON summary saved to: {json_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"JSON SUMMARY CONTENT\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(json_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20420a9e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook copied to: /net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_evaluation.ipynb\n",
      "\n",
      "Verifying output files:\n",
      "  Notebook exists: True\n",
      "  JSON exists: True\n"
     ]
    }
   ],
   "source": [
    "# Copy the notebook to the required location\n",
    "import shutil\n",
    "\n",
    "source_notebook = \"/home/smallyan/eval_agent/notebooks/2025-12-24-20-53_CodeCriticEvaluation.ipynb\"\n",
    "target_notebook = \"/net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_evaluation.ipynb\"\n",
    "\n",
    "# Wait a moment for notebook to be saved\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Copy the notebook\n",
    "shutil.copy(source_notebook, target_notebook)\n",
    "print(f\"Notebook copied to: {target_notebook}\")\n",
    "\n",
    "# Verify both files exist\n",
    "print(\"\\nVerifying output files:\")\n",
    "print(f\"  Notebook exists: {os.path.exists(target_notebook)}\")\n",
    "print(f\"  JSON exists: {os.path.exists(json_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f5b68",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "### Evaluation Complete\n",
    "\n",
    "**Repository:** `/net/scratch2/smallyan/InterpDetect_eval`\n",
    "\n",
    "**Total Functions/Blocks Evaluated:** 99\n",
    "\n",
    "### Quantitative Metrics\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Runnable% | 98.99% |\n",
    "| Output-Matches-Expectation% | 98.99% |\n",
    "| Incorrect% | 1.01% |\n",
    "| Redundant% | 21.21% |\n",
    "| Irrelevant% | 4.04% |\n",
    "| Correction-Rate% | 0.00% |\n",
    "\n",
    "### Binary Checklist\n",
    "| Item | Status |\n",
    "|------|--------|\n",
    "| C1: All Runnable | FAIL |\n",
    "| C2: All Correct | FAIL |\n",
    "| C3: No Redundant | FAIL |\n",
    "| C4: No Irrelevant | FAIL |\n",
    "\n",
    "### Key Findings\n",
    "1. **Core analysis code is mostly correct** - The main ECS/PKS computation in `compute_scores.py`, classifier training in `classifier.py`, and prediction in `predict.py` all work correctly.\n",
    "\n",
    "2. **One implementation bug found** - `generate_labels.py` main() function references undefined arguments `args.skip_lettuce` and `args.skip_llm_judge`.\n",
    "\n",
    "3. **Significant code duplication** - 21 utility functions are duplicated across scripts. Recommend refactoring to use shared utility modules.\n",
    "\n",
    "4. **Visualization code is not core** - 4 plotting functions could be moved to a separate visualization module.\n",
    "\n",
    "### Output Files\n",
    "- **Notebook:** `/net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_evaluation.ipynb`\n",
    "- **JSON Summary:** `/net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_summary.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f7aba3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Output files created:\n",
      "  1. Notebook: /net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_evaluation.ipynb\n",
      "  2. JSON: /net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_summary.json\n",
      "\n",
      "All evaluation criteria have been assessed and documented.\n"
     ]
    }
   ],
   "source": [
    "# Final copy of notebook with all content\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "time.sleep(2)  # Wait for notebook to save\n",
    "\n",
    "source_notebook = \"/home/smallyan/eval_agent/notebooks/2025-12-24-20-53_CodeCriticEvaluation.ipynb\"\n",
    "target_notebook = \"/net/scratch2/smallyan/InterpDetect_eval/evaluation/code_critic_evaluation.ipynb\"\n",
    "\n",
    "shutil.copy(source_notebook, target_notebook)\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutput files created:\")\n",
    "print(f\"  1. Notebook: {target_notebook}\")\n",
    "print(f\"  2. JSON: {json_path}\")\n",
    "print(\"\\nAll evaluation criteria have been assessed and documented.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-20-53_CodeCriticEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
